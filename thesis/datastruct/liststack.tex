\section{Lists and stacks}
The first and least interesting data structure we will explore, is the stack.
The persistent stack in functional programming languages naturally comes from the \texttt{List} algebraic data structure.
In imperative programming languages a stack can be implemented via a linked list, which is similar to a \texttt{List}.
\begin{figure}
\begin{lstlisting}[language=ML,caption={Stack implementation},label={lst:stackimpl},mathescape=true]
type Maybe a =
  | Nothing
  | Just a
;
type Stack a =
  | Nil
  | Cons a (List a)
;
type Tuples a b = Tuple a b;

fun push xs x = 
  Cons x xs;

fun popt s =
  match s
    | Nil -> Tuple Nothing Nil;
    | Cons x xs -> Tuple (Just x) xs;
  ;

fun pop xs =
  match xs
    | Nil -> Nothing;
    | Cons x rest -> Just x;
  ;

fun tail xs = 
  match xs
    | Nil -> Nil;
    | Cons x xs -> xs;
  ;
\end{lstlisting}
\end{figure}
\autoref{lst:stackimpl} is an implementation of a stack similar to how stacks can be implemented in imperative languages.
The \texttt{pop} and \texttt{push} functions clearly have worst case constant time since neither of them perform any operations that depend on any input size.
A stack that can \texttt{pop} and \texttt{push} is all that is necessary in the previous usages of the stack data structure.
\begin{remark}
  The stack in \autoref{lst:stackimpl} can be refined further, allowing worst case constant time \texttt{multipush} operations by treating the internal \texttt{List a} as a \texttt{List (List a)}, but this is not necessary in this case.
\end{remark}
%\section{Lists and lazy evaluation}
%An instance of a data structure which has been thoroughly discussed throughout this thesis is \texttt{List} (\autoref{lst:listadt}).
%\texttt{List} is an excellent choice as an introductory data structure since it gives insight into some very universal problems regarding both immutable and mutable data structures.
%One is free to choose the operations for \texttt{List} but a common operation is \texttt{map} (\autoref{lst:map}).
%\begin{lstlisting}[language=ML,caption={Mapping from \texttt{List a} to \texttt{List b}},label={lst:map},mathescape=true]
%fun map f l = 
   %match l
    %| Cons x xs -> Cons (f x) (map f xs);
    %| Nil -> l;
   %;
%\end{lstlisting}

%There exists several analytical techniques to justify performance guarantees in call by value data structures, the most straight forward of which is the worst case analysis.
%Worst case analysis is usually the most straight forward, since it becomes a matter of finding the worst input for any possible state of the data structure.

%An interesting observation from map is that it runs differently in a call by need environment compared to a call by value environment.
%In a call by value environment \texttt{map} takes $\Theta(n)$ time since every \texttt{Cons}'ed value must be visited.
%In a call by need environment things become a bit more philosophical.
%When \texttt{map} is evaluated in a call by need environment it is technically suspended thus always requires one operation.
%When a value which depends on \textit{one} \texttt{map} invocation, is forced (from the addition operator for instance), then the computational complexity has the same bounds as if it were call by value.
%The computational complexity in a call by need (or name) environment for \textit{one} \texttt{map} invocation is thus $O(n)$ and $\Omega(1)$, in general \textit{all} call by need algorithms run in $\Omega(1)$.
%More interestingly, consider $n$ invocations of \texttt{map} (\autoref{lst:mulmap}) on some list.
%\begin{lstlisting}[language=ML,caption={$n$ invocation of map},label={lst:mulmap},mathescape=true]
%fun id x = x;
%let xs = Cons 1 (Cons 2 ($\dots$ (Cons $m$ Nil)));
%let m1 = map id xs;
%let m2 = map id m1;
%$\dots$
%let $\text{m}_n$ = map id $\text{m}_{n - 1}$;
%\end{lstlisting}
%Clearly \texttt{m$_n$} in \autoref{lst:mulmap} requires $n \cdot m$ time if it is forced.
%Moreover observe that we can ``enqueue'' an unbounded amount of \texttt{map} operations, or rather, the computational complexity is not a function of $n$ (a function of the input size), but rather a function of how much work has been performed on the data structure.

%With bounds such as $\Omega(1)$ and a worst case which is unbounded, traditional worst case analysis breaks down.
