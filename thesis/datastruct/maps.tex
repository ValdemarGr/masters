\section{Tables, hashes, colors and tries}
A mapping of variables to some other domain are a very common occurrence throughout the previous sections.
Hash tables are data structures that provide very good probabilistic bounds for such mappings.
Unfortunately, hash tables build upon mutability, thus do not provide the property of persistence that we require.
Fortunately, some data structures are better fitted for persistence than others.

In this section we will explore two types persistent types of associative data structures, the red-black tree and the hash array mapped trie.

\subsection{Red-black trees}
Trees are very common data-structures because of their inherit guarantees.
A \textit{nodes} is a containers of some key and value, as depicted in \autoref{fig:anode}.
The key of a node is it's identity, such that one can query the tree for the associated value.
Nodes occur in trees such as in \autoref{fig:atree}, where the \textit{root} of the tree is the topmost node (the root of \autoref{fig:atree} is $2$).
A node can have children, which are the nodes directly below it.
A node with no children, is called a leaf.
A sub-tree is the tree that is formed by letting a node that is otherwise a child, be a root.
A parent of a node $c$ is the node $p$ that has $c$ as a child.
A grandparent of a node is the node's parent's parent.
A path $p$ from the root $l$ to $o$ is a sequence of nodes $l \rightarrow \dots \rightarrow o$, such that the length of $p$, denoted $\#p$, is the number of nodes that occur in the path.
There are various orderings one can impose on a tree, but we will maintain the invariants that all sub-trees left of a node will have keys that are smaller than the key of that node and all sub-trees right of a node will have keys that are larger than the key of that node.
A balanced tree, is tree of size $n$ where no one node is further than $\ceil{\log_2 n}$ from the root.
Trees can be a very useful representation of stored data, since finding a value in a balanced tree requires one to visit at most $\ceil{\log_2 n}$ nodes since that is the furthest (the number of nodes between) a node can be from the root.
Searching for a key $k$ in a tree which maintains the previously established ordering invariant, is a matter of beginning at the root and determining if the root's key $r$ has the same key, if not then the left tree is visited if $r > k$ else the right tree is visited, this process is repeated until a node with a key equal to $k$ is found.
\begin{figure}
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
						level distance = 1.5cm}]

			\node [arn_n] {1};
		\end{tikzpicture}
		\caption{A node}
		\label{fig:anode}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1cm/#1,
						level distance = 1.5cm}]

			\node [arn_n] {2}
			child {
					node [arn_n] {1}
				}
			child {
					node [arn_n] {3}
				};
		\end{tikzpicture}
		\caption{A tree}
		\label{fig:atree}
	\end{subfigure}
	\caption{}
	\label{fig:nodeandtree}
\end{figure}

Trees by themselves are very open for interpretation, since they specify nothing of how keys are inserted and deleted.
An very popular variant, which we will interest ourselves in is the red-black tree~\cite{bayer1972symmetric}.
A variant of the red-black tree which we will interest ourselves in is the Okasaki variant~\cite{okasaki1999red}.
In a red-black tree, every leaf node is a node with no key or value, called an empty node.
Furthermore, in a red-black tree every node is enhanced with a color, which is either black or red, hence the name.
A red-black tree has two invariants that must be upheld.
\begin{itemize}
	\item No red node has a red parent.
	\item Every path from the root to an empty node contains the same number of black nodes.
\end{itemize}
Maintaining these two invariants, the worst case for searching becomes worse than a balanced tree, but only by a constant factor.
%The shortest possible path from an empty node to a root must be the path that only contains black nodes.
%The longest possible path from an empty node to the root must be the path with nodes alternating between black and red.
\begin{lemma}
	A red-black tree of size $n$ has height at most $2 \log_2 (n + 1)$.~\cite{cormen2009introduction}.
\end{lemma}
Since $O(2 \log_2 (n + 1)) = O(\log_2 n)$ then search can be performed in $O(\log_2 n)$ by the same algorithm as a balanced binary tree.
%If all other paths than one contain only black nodes $p_1 \equiv p_2 \dots \equiv p_k$ where $p_b = (\#p_1) = (\#p_2) \dots = (\#p_k)$, where the path $p_r$ contains an alternating sequence black and red, then the worst case must be searching for last node of of $p_r$.
%Visiting $p_b$ steps of $p_r$ must take us halfway since $\frac{\#p_r}{2} = p_b$ by invariant two; $\#(\textit{red}_1 \rightarrow \textit{black}_2 \dots \rightarrow \textit{red}_{p_b - 1} \rightarrow \textit{black}_{p_b})$, thus the whole path must be the double of $p_b$, $(\#p_r) = 2p_b$.
%To bound the worst case for $\#p_r$ consider that $p_b$ is the shortest possible path such that $p_b < \log_2 n$ and $2p_b = (\#p_r) < 2\log_2 n$ together with $\log_2 n < \#p_r$ asymptotically becomes $O(\log_2 n < \#p_r < 2\log_2 n) = O(\log_2 n)$.

\subsubsection{Insertion}
Insertion is the first practical problem we will tackle in the red-black tree.
In~\cite{okasaki1999red} presents an elegant algorithm for insertion.
Observe that by inserting a node naively, one might violate one of the two invariants.
When we insert a node, we perform an act of \textit{rebalancing}.
Rebalancing is an action that guarantees that the tree will maintain the two aforementioned invariants after action has occurred on the tree.
When inserting a node, the node must colored red and always be placed in the bottom of the tree by searching for a position, while maintaining the ordering of the tree.
When the node has been placed, rebalancing is performed on the way back up such that violations of the two invariants, are floated up throughout the tree until the root is encountered, and then the root is colored black.
Consider that inserting a node at the leaf, may only cause a violation if the parent node of the one inserted is also red; if the parent node is black, then the invariants hold.
When considering insertion (and rebalancing), the property that the red-black tree was in a state that upheld the invariants is always maintained.
There are two types of \textit{rotations} (and two reflections for each rotation) we must perform to balance the tree after a violation.
Rotations are the act of moving nodes around such that they maintain the invariants.
Rotations are considered only for nodes that have grandparents (or conversely, nodes that have grandchildren).
For a node that has grandchildren there are four cases that may violate the invariant (again, two reflections for each rotation), which is depicted in \autoref{fig:rbrot}.
These rotations satisfy the second invariant, since for all rotations the same number of black nodes are present above \texttt{a, b, c} and \texttt{d}.
Furthermore, notice that the red node in \autoref{fig:rbrot:end} may have a red parent, this violation is only temporary, since the tree is rebalanced in reverse order of the search path.
The insertion algorithm can elegantly be encoded into the programming language, given support for nested pattern matching (\autoref{sec:remscott}).
An implementation (give that the language supports nested pattern matching) has been presented in \autoref{lst:rbimpl}.

The number of nodes to visit when searching for a position to place the new node is at most $O(\log_2 n)$ time.
The time for one \texttt{balance} is at most $1$ invocation, the \texttt{balance} function is invoked for each travelled node thus it is invoked $O(\log_2 n)$ times, which implies that the combined insertion time is $O(\log_2 n)$.

\subsubsection{Deletion}
Deletion in red-black trees is a more complicated than insertion.
Deletion of a black node can imply a much larger range of rotations than insert otherwise would.
In~\cite{germane2014deletion}, an algorithm for deletion is presented.
The algorithm, inspired by the Okasaki red-black tree, solves deletion in a similar method as insertion.
When a deletion deletes a black node, then the tree will no longer satisfy the black node invariant.
The black node invariant is then restored by introducing a double black node.
After this double black node has been introduced, a recursive algorithm, similar to the \texttt{balance} algorithm, removes all double black nodes by performing rotations.
The rotations implied by deletion can be further investigated in~\cite{germane2014deletion}.

Instead of discussing the various rotations, we will move on to a another data structure that will supersede the red-black tree.

\begin{figure}[p]
	\begin{mdframed}
		\begin{subfigure}[b]{1\textwidth}
			\centering
			\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.6cm/#1,
							level distance = 1.5cm}]

				\node [arn_b] {z}
				child {
						node [arn_r] {x}
						child {
								node [arn_l] {a}
							}
						child {
								node [arn_r] {y}
								child {
										node [arn_l] {b}
									}
								child {
										node [arn_l] {c}
									}
							}
					}
				child {
						node [arn_l] {d}
					};
			\end{tikzpicture}
			\caption{}
			\label{fig:rbrot:c1}
		\end{subfigure}
		\begin{subfigure}[b]{1\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[implies-,double equal sign distance] (1,1) -- (1,2);
			\end{tikzpicture}
		\end{subfigure}

		\begin{subfigure}[b]{0.25\textwidth}
			\centering
			\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.6cm/#1,
							level distance = 1.5cm}]

				\node [arn_b] {z}
				child {
						node [arn_r] {y}
						child {
								node [arn_r] {x}
								child {
										node [arn_l] {a}
									}
								child {
										node [arn_l] {b}
									}
							}
						child {
								node [arn_l] {c}
							}
					}
				child {
						node [arn_l] {d}
					};
			\end{tikzpicture}
			\caption{}
			\label{fig:rbrot:c2}
		\end{subfigure}
		\begin{subfigure}[b]{0.1\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[-implies,double equal sign distance] (1,1) -- (2,1);
			\end{tikzpicture}
			\vspace*{2cm}
		\end{subfigure}
		\begin{subfigure}[b]{0.25\textwidth}
			\centering
			\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.6cm/#1,
							level distance = 1.5cm}]

				\node [arn_r] {y}
				child {
						node [arn_b] {x}
						child {
								node [arn_l] {a}
							}
						child {
								node [arn_l] {b}
							}
					}
				child {
						node [arn_b] {z}
						child {
								node [arn_l] {c}
							}
						child {
								node [arn_l] {d}
							}
					};
			\end{tikzpicture}
			\caption{}
			\label{fig:rbrot:end}
		\end{subfigure}
		\begin{subfigure}[b]{0.1\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[implies-,double equal sign distance] (1,1) -- (2,1);
			\end{tikzpicture}
			\vspace*{2cm}
		\end{subfigure}
		\begin{subfigure}[b]{0.25\textwidth}
			\centering
			\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.6cm/#1,
							level distance = 1.5cm}]

				\node [arn_b] {x}
				child {
						node [arn_l] {a}
					}
				child {
						node [arn_r] {y}
						child {
								node [arn_l] {b}
							}
						child {
								node [arn_r] {z}
								child {
										node [arn_l] {c}
									}
								child {
										node [arn_l] {d}
									}
							}
					};
			\end{tikzpicture}
		\end{subfigure}

		\begin{subfigure}[b]{1\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[implies-,double equal sign distance] (1,2) -- (1,1);
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}[b]{1\textwidth}
			\centering
			\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.6cm/#1,
							level distance = 1.5cm}]

				\node [arn_b] {x}
				child {
						node [arn_l] {a}
					}
				child {
						node [arn_r] {z}
						child {
								node [arn_r] {y}
								child {
										node [arn_l] {b}
									}
								child {
										node [arn_l] {c}
									}
							}
						child {
								node [arn_l] {d}
							}
					};
			\end{tikzpicture}
		\end{subfigure}
	\end{mdframed}
	\caption{Red-black tree rotations}
	\label{fig:rbrot}
\end{figure}

\subsection{Hash array mapped trie}
The red-black tree is a staple data structure in the competent programmers tool belt, because of their $O(\log_2 n)$ bound and simple implementation details.
In a persistent setting, Okasaki showed that red-black tree's can be implemented elegantly.
In a ephemeral setting, lookup oriented data structures can have search bounds of average case constant time (hash tables)~\cite{cormen2009introduction}.
A suitable candidate which lifts the ephemeral hash table to a persistent setting is Hash Array Mapped Trie (HAMT)~\cite{bagwell2001ideal}.

The HAMT is a tree, where all nodes have a constant amount of children, in the practical case, 32 is the number of children.
\begin{remark}
	32 seems like an odd number too pick, but has it's practicalities.
	Bitsets will shortly be introduced to the data structure.
	A bitset of size 32 is exactly a 32 bit word.
\end{remark}
A child can either be a terminal value (a key and associated value), or another node.

\subsubsection{An enhanced prefix tree}
In an introductory data structure to the HAMT, one can let all nodes have a \textit{child array} of size 32, where all entries are \texttt{nil} initially.
\begin{figure}
  \centering
	\begin{tikzpicture}
    \matrix[mtx,above] (t1) at (0,0) {4 & \texttt{nil} & 6 & \texttt{nil} & 2 & \dots\\};
    \matrix[mtx,below] (b1) at (0,0) {\bullet & \times & 142 & \times & \bullet & \dots\\};
    \matrix[mtx,above] (t21) at (-3,-4) {7 & 2 & \texttt{nil} & \texttt{nil} & \texttt{nil} & \dots\\};
    \matrix[mtx,below] (b21) at (-3,-4) {99 & 11 & \times & \times & \times & \dots\\};
    \matrix[mtx,above] (t23) at (3,-4) {1 & \texttt{nil} & 4 & \texttt{nil} & \texttt{nil} & \dots\\};
    \matrix[mtx,below] (b23) at (3,-4) {42 & \times & 24 & \times & \times & \dots\\};
		\draw[blue] (b1-1-1.center) -- (t21-1-3.north east)
		(b1-1-5.center) -- (t23-1-4.north west);
	\end{tikzpicture}
  \caption{Prefix tree}
  \label{fig:prefix}
\end{figure}
\autoref{fig:prefix} is a \textit{prefix tree} that is a prerequisite for the HAMT.
A prefix of a number is the first digit.
Searching for a number such as 42 will search the first node for the prefix of 42, 4, such that the left child is chosen and the new search number is 2.
In the left child the prefix of the remaining number, 2, is chosen such that the cell that contains 11 is found. 

Now we argument the prefix tree with hashes instead of digits, this change will become useful in the next iteration of the data structure.
A hash function is randomly chosen for the data structure $h \in \mathcal{H}$ that maps the universe of keys into a 32 bit word $\{0, 1 \dots , 2^{32}\}$, such that the probability of collision is $\frac{1}{2^{32}}$~\cite{cormen2009introduction}.
When a key occurs in the 32 sized array, the five most significant bits of the hash are converted to an integer and are to index directly into the array, since $2^5 = 32$.
\begin{exmp}
  The five most significant bits of \autoref{eq:significant} are \texttt{10010}, which is the index 18.
  \begin{align}
    \texttt{11 11110 01101 11100 10000 01101 10010}\label{eq:significant}
  \end{align}
\end{exmp}
Now we can efficiently index into the array.

Notice that the prefix tree contains many \textit{sparse arrays}, such that space may potentially be many fold of the actual size of the data.
To solve this, we augment the prefix tree with a bitset, such that it becomes a Hash Array Mapped Trie.
Each node is augmented with a bitset of size 32, such that each bit indicates whether a value is present or not.
Furthermore, a processor instruction exists (\texttt{CTPOP}) that counts the number of set (1's) bits in a bitset.
To enhance \texttt{CTPOP}, we count the number of set bits until an offset (this can be achieved by a very simple mask and OR operation).
When we have the ability to query for how many set bits come before an index in the bitset, we can represent the sparse array compactly.
The array will now only contain the set values and the number of set bits before an index in the bitset will be the offset in the array.

\subsubsection{Searching and inserting}
Now the structure of the HAMT has been completed.
Search is performed by combining the aforementioned enhancements:
\begin{enumerate}
  \item Compute the hash $h(k)$ of key $k$.
  \item If position $h(k)$ of the bitset is empty, return \texttt{nil}.\label{search:step2}
  \item Index into the array at the position given by the number of set bits before $h(k)$.
  \item If the element found is a value then compare $k$ and the stored key, if they match then return the value, if not the return \texttt{nil}.
  \item If the element found is a node then remove five bits from $k$ and goto \autoref{search:step2}.
\end{enumerate}
Worst case search is clearly bounded by the depth, with a branching factor of 32 the search time becomes $\log_{32} n$, or $O(\log n)$.

Insertion is performed by a simple set of steps:
\begin{enumerate}
  \item Search until either an unused slot reveals itself or a terminal value is found.
  \item If the slot is empty, insert the value and return.
  \item If the slot contains a value, create a new HAMT node and insert the old value into the node, and the recurse into the node.
  \item If the hash has been exhausted while solving collision, rehash the key $k$ by using a new hash function.
\end{enumerate}
\begin{remark}
  Collisions can occur, but they can be done away with traditionally.
  For instance, when collisions occur at the leaf level ($\floor{\frac{32}{5}}$) then one can pick a particular hash function for that leaf which local uniqueness, and then add a collision node.
\end{remark}

\subsubsection{Lazy root resizing}
Inevitably, when the tree grows, the probability of a hash not colliding decreases, which prompts for a resizing algorithm.
So far the root has been introduced as if it has the same properties as nodes.
The root will no longer have a bitset and initially be an array of \texttt{nil}.
Since the root is only the top node, then the extra space used by the array is but a constant factor of the total size.
When the HAMT reaches a point where it's size exceeds some threshold it must begin \texttt{lazy resizing}.
Lazy resizing is the act of increasing the size of the root and incrementally moving entries from the old root over.
Let $2^t$ be the number of bit's that the root can consume, where $t = 5$ initially, if we increase this number by five, such that $2^(t + 5)$, then the new root can contain all entries of all of the root's children since $(2^5)^2 = 2^{10}$.
Effectively, the children of the root are moved into the root once the HAMT's size becomes too large, such that the height is reduced by one.
The operation of moving the children into the new root can be amortized over searches and inserts.


%Every bit decreases the likelihood of collision by a factor two, such that the probability after 32 bits is $\frac{1}{2}^{32}$.
%If there are $n$ elements in the HAMT, when $\log_2 n$ bits have been checked then there is a probability of $1/2$.
%More formally, beginning at the root level the probability that two keys collided is $\frac{n}{32}$.
%If they collided, then the probability that they collided again is $\frac{\frac{n}{32}}{32}$
