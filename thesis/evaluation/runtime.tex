\documentclass[11pt,oneside,a4paper]{report}

\begin{document}

\section{Evaluation strategies}
When evaluating the untyped lambda calculus one has to choose an evaluation strategy.
The choice of evaluation strategy has a large impact on aspects such as complexity guarantees.
The names of such strategies are \textit{call by value}, \textit{call by name} and \textit{call by need}.
The call by value is most often the simplest and most natural way of assuming program execution.
\begin{lstlisting}[language=ML,caption={Program that doubles values},label={lst:callbyvalue},mathescape=true]
fun double x = x + x;
let a = double 10;
double (double 10);
\end{lstlisting}
By the call by value semantics, \autoref{lst:callbyvalue} eagerly evaluates every expression.
Clearly the variable \texttt{a} is never used but under the call by value semantics everything is eagerly evaluated.
Every expression is evaluated in logical order in the call by value evaluation strategy.
\begin{figure}[ht]
\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:callbyname},mathescape=true]
fun suspend x unit = x;
fun force x = x 0;
let value = suspend 10;
fun double x = 
    fun susExpensiveOp unit = 
        (force x) + (force x);
    susExpensiveOp;
let a = double value;
force (double value);
\end{lstlisting}
\end{figure}
The call by name semantics however does only evaluate expressions once they are needed (also commonly called \textit{lazy evaluation}).
By the call by name semantics \texttt{a} is never evaluated since it is never used.
In \autoref{lst:callbyname} call by name has been implemented by the use of various functions such as the two constant functions \texttt{suspend} and \texttt{force}.
\texttt{susExpensiveOp} ensures that the forcing (evaluation) of \texttt{x} never occurs until the caller of \texttt{double} forces the result.
By the aforementioned semantics of call by name in the context of the program in \autoref{lst:callbyname} \texttt{a} is never forced thus the computation is never performed.
The implementation of call by name can become quite troublesome and therefore in most cases is a part of the native execution environment which will be discussed in \autoref{tbd}.

The call by need semantics introduces the same lazy evaluation semantics as the call by name strategy but with one extra detail named \textit{sharing}.
In \autoref{lst:callbyname} \texttt{force x} is performed twice which may be an expensive operation.
Under call by need all non side-effectful operations' results are saved for later use similar to techniques such as dynamic programming.
\begin{figure}
    \begin{mdframed}[style=style2]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
    \begin{tikzpicture}
        \node[circle, draw=black] (force) {\texttt{force}};

        \node[circle, draw=black, below = of force] (double) {\texttt{double}};

        \node[circle, draw=black, below = of double] (value) {\texttt{value}};

        \path[->] (force) edge node[left] {} (double);
        \path[->] (double) edge node[left] {} (value);
    \end{tikzpicture}
        \caption{The last expression of the program.}
        \label{sub:eval:main}
    \end{subfigure}
    \begin{subfigure}[b]{0.66\textwidth}
        \centering
    \begin{tikzpicture}
        \node[circle, draw=black] (lamx) {$\lambda \texttt{x}$};

        \node[circle, draw=black, below = of lamx] (lamunit) {$\lambda \texttt{unit}$};

        \node[circle, draw=black, below = of lamunit] (addition) {$+$};

        \node[circle, draw=black, below left = of addition] (force1) {\texttt{force}};
        \node[circle, draw=black, below = of force1] (x1) {\texttt{x}};
        
        \node[circle, draw=black, below right = of addition] (force2) {\texttt{force}};
        \node[circle, draw=black, below = of force2] (x2) {\texttt{x}};

        \path[->] (lamx) edge node[left] {} (lamunit);
        \path[->] (lamunit) edge node[left] {} (addition);

        \path[->] (addition) edge node[left] {} (force1);
        \path[->] (addition) edge node[left] {} (force2);

        \path[->] (force1) edge node[left] {} (x1);
        \path[->] (force2) edge node[left] {} (x2);
    \end{tikzpicture}
        \caption{The expression tree for \texttt{double}}
        \label{sub:eval:double}
    \end{subfigure}
    \end{mdframed}
    \caption{}
    \label{fig:evalexpr}
\end{figure}
To understand this better observe the expression tree for \autoref{lst:callbyname} in \autoref{fig:evalexpr}.
Clearly the right and left branches in \autoref{sub:eval:double} are identical thus they may be memoized such that the forcing of \texttt{x} only occurs once.
More generally if the execution environment supports lazy evaluation, once an expression has been forced it is remembered if that branch is executed again.

\section{Runtime environments}
Now that the untyped lambda calculus has been introduced, implemented and validated efficiently the question of execution naturally follows.
There exists many different well understood strategies to implement an execution environment for the untyped lambda calculus.
Naively it may seem straightforward to evaluate the untyped lambda calculus mechanically by $\beta$-reductions some problems arises when implementing an interpreter.
%When applying the term $f x$ in $\lambda f . \lambda x . f x$ such that $f[? := x]$ where $?$ is the parameter name of $f$ it should become clear why a naive strategy is not enough since the parameters of $f$ are anonymous.

\subsection{Combinator reducers}
One of the most prominent techniques for evaluating functional programs is that of \textit{combinator graphs reductions}.
Formally a combinator is a function that has no free variables which is convenient since the problem of figuring out closures and parameter substitution in application never arises.
\begin{align}
    x \label{eq:comb:x}\\
    F \label{eq:comb:F}\\
    Y E \label{eq:comb:app}
\end{align}
There are three types of terms in combinator logic; the variable much like the lambda calculus (\autoref{eq:comb:x}), application (\autoref{eq:comb:app}) and the combinator (\autoref{eq:comb:F}).
The SKI calculus is a very simple set of combinators which are powerful enough to be turing complete and translate to and from the lambda calculus.
In SKI $F ::= S \,\,|\,\, K \,\,|\,\, I$ where the equivalent lambda calculus combinators for $S = \lambda x . \lambda y . \lambda z . x z (y z)$, $K = \lambda x . \lambda y . x$ and $I = \lambda x . x$.
Evaluating an SKI program is a straightforward reduction where $F'_F$ denotes combinator $F'$ has been partially applied with combinator $F$.
\begin{exmp}
    \begin{align}
        &SKSI\\
        = &KI(SI)\tag*{}\\
        = &K_I(SI)\tag*{}\\
        = &I\tag*{}
    \end{align}
\end{exmp}

The algorithm for converting a lambda calculus program into a SKI combinator program is a straightforward mechanical one.
The evaluation context is always an abstraction $\lambda x . E$.
\begin{pcases}
    \pcase{$E = x$ then rewrite $\lambda x . E$ to $I$.}
    \pcase{$E = y$ where $y$ is a variable then rewrite $\lambda x . y$ to $Ky$.}
    \pcase{$E = Y E$ then rewrite $\lambda x . Y E$ to $S(\lambda x . Y)(\lambda x . E)$ since applying some $y$ must lambda lift $y$ as a parameter named $x$ to both $Y$ and $E$ such that the lifted expression becomes $((\lambda x . Y)y)((\lambda x . E)y) = S(\lambda x . Y)(\lambda x . E)y$.
    Then recurse in both branches.
    }
    \pcase{$E = \lambda y . E'$ rewrite $E'$ and then rewrite the expression.}
\end{pcases}
When translating the untyped lambda calculus to SKI the ''magic`` variable names $\zeta, \kappa$ and $\iota$ are used as placeholder functions for the SKI combinators since the translation requires a lambda calculus form.
When the translation has been completed then replace $\zeta \mapsto S, \kappa \mapsto K, \iota \mapsto I$.
\subsection{Combinator translation growth}
The growth factor is in terms of SKI combinators to lambda calculus terms.
\begin{lemma}
    The SKI calculus increases the computational complexity to $O(n^2)$
\end{lemma}
\begin{proof}
\begin{pcases}
    \pcase{\label{eval:case:1} Rewriting $\lambda x . x$ to $I$ is a reduction of one.}
    \pcase{\label{eval:case:2} Rewriting $\lambda x . y$ to $Ky$ is equivalent in terms of size.}
    \pcase{Rewriting $\lambda x . YE$ to $S(\lambda x . Y)(\lambda x . E)$ is the interesting case.
        To induce the worst running time \autoref{eval:case:1} must be avoided.
        If $x \notin \textit{Free}(Y)$ and $x \notin \textit{Free}(E)$ then for every terminal term in $Y$ and $E$ \autoref{eval:case:2} is the only applicable rewrite rule which means that an at least equal size is guaranteed.
        Furthermore observe that by introducing unused parameters one can add one $K$ term to \textit{every} terminal case.
        Observe the instance $\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1)$ where the two unused parameters are used to add $K$ terms to all terminal cases in \autoref{eq:eval:red2} such that the amount of extra K terms minus the I becomes $\text{variablereferences} * (\text{unusedabstractions} - 1) = 3*(3-1)$.
        \begin{gather}
            S(S(KKI)(KKI))(KKI)\label{eq:eval:red2}
        \end{gather}
        Now let the amount of variable references be $n$ and the unused abstractions also be $n$ clearly $O(n * (n - 1)) = O(n^2)$
    }
    \pcase{Rewriting $\lambda x . E'$ is not a traslation rule so the cost is based on what $E'$ becomes.}
\end{pcases}
\end{proof}

In some cases computational complexity measures are very much purely academical and so is the analysis of the translated SKI calculus on a pathological input.
This is not to say that the running time is always linear is practice as a simple program that deliberately avoids the worst case still produces larger output.
In the case of SKI observe that the applications $f_1 f_1 \dots f_1$ can in fact be changed to $f_1 f_2 \dots f_n$ since for every $f_k$ where $0 < k \leq n$ there are $n - 1$ parameters that induce a K combinator.
\begin{exmp}
    Observe the size of \autoref{eq:eval:comp1} in comparison to \autoref{eq:evaltime}.
\begin{align}
    &\lambda f_1 . \lambda f_2 . f_1 f_2 \label{eq:eval:comp1}\\
    =&\lambda f_1 . \zeta(\lambda f_2 . f_1)(\lambda f_2 . f_2) \tag*{} \\
    =&\lambda f_1 . (\zeta(\kappa f_1))(\iota) \tag*{} \\
    =&\zeta (\lambda f_1 . \zeta (\kappa f_1)) (\lambda f_1 . \iota) \tag*{} \\
    =&\zeta (\zeta (\lambda f_1 . \zeta) (\lambda f_1 . \kappa f_1)) (\kappa \iota) \tag*{} \\
    =&\zeta (\zeta (\kappa \zeta) (\zeta (\lambda f_1 . \kappa) (\lambda f_1 . f_1))) (\kappa \iota) \tag*{} \\
    =&\zeta (\zeta (\kappa \zeta) (\zeta (\kappa \kappa) (\iota))) (\kappa \iota) \tag*{} \\
    =&S (S (K S) (S (K K) (I))) (K I) \tag*{}
\end{align}
\end{exmp}

\subsection{Reduction strategies}
Reductions in the context of the lambda calculus are a small set of well-defined rules for rewriting such that a program is evaluated.
The techniques required to correctly evaluate a program are a bit more complicated than the SKI calculus but are rewarding in the performance.
The substitution mapping set is a set of variable names to their value denoted $\{ x \mapsto \lambda y . y \}$ meaning ``the value of variable $x$ is $\lambda y . y$''.
%In $(\lambda f . \lambda x . \lambda y. \texttt{ let } a = f x \texttt{ in } a + y)(\lambda y . y) \,\, 10 \,\, 20$ when evaluating $f x$ where the substitution are $[f \mapsto \lambda y . y, x \mapsto 10, y \mapsto 20]$ there must be taken great care that $x$ is mapped to $y$ in $\lambda y . y$.
\begin{lstlisting}[language=ML,caption={Problematic program},label={lst:problemprog},mathescape=true]
($\lambda$g.
    ($\lambda$x.
        ($\lambda$g.
            x g
        )(0)
    )($\lambda$x.g x)
)($\lambda$x.x)
\end{lstlisting}
When evaluating \autoref{lst:problemprog} it should become clear why a naive evaluation strategy becomes insufficient.
If \autoref{lst:problemprog} was evaluated naively the expression $\lambda\texttt{x.}$ on line 2 would leak into the substitution mapping of $\lambda\texttt{x.x}$ on line 7.
Furthermore if no precautions are taken in $\lambda\texttt{x.g x}$ then $\lambda\texttt{g.}$ on line 4 will leak into the substitution mapping. 
More generally closures must be bound to abstractions and applications must substitute the variable name and restore the context when completed.
In the expression \texttt{x g} the substitutions from \texttt{x} may never persist outside of the scope of the body of \texttt{x}.

The first step to evaluate the lambda calculus is applying \textit{$\alpha$-conversions} which is the operation of renaming.
Let $\lambda x . ((\lambda x . x) x)$ be a term where a collision between the two $x$'s causes trouble, the $\alpha$-conversion can be $\lambda y . ((\lambda x . x) y)$ since $y$ and $x$ have been introduced by separate abstractions.
$\alpha$-conversions are solve some critical problems such as closures and recursion when evaluating the lambda calculus.
$\alpha$-conversions should also be non-destructive but still be context aware such that when leaving an abstraction the remaining substitution mapping is a strict superset of the substitution mapping when entering.
More formally $\textit{eval}: \lambda \times S \rightarrow \lambda \times S$ where $\lambda$ is a lambda term and $(\lambda',S') = eval(\lambda, S)$ where $S \subseteq S'$.
\begin{lstlisting}[language=ML,caption={Recursive addition function},label={lst:recalpha},mathescape=true]
let f = ($\lambda$f'.$\lambda$x.if (x = 0) x else f' f' (x - 1)) in
f f 10
\end{lstlisting}
A strong guarantee that can be made by tuning the evaluation strategy that is particularly useful for $\alpha$-conversion algorithms is that \textit{any} returned value has had \textit{every} term that it contains visited.
An algorithm can be implemented that just picks a new variable name from any infinite domain and replaces future encountered variables with the new one, such an algorithm only works if the guarantee of visiting every term is made.
\begin{remark}
The $\alpha$-conversion algorithm should drop the conversion mapping once it leaves the abstraction that introduced the conversion rule for that variable and restore the set introduced before it.
Such semantics ensure that expressions such as \texttt{f f x} are properly converted since \texttt{x}'s naming in ``this'' instantiation of \texttt{f} must be restored in a call by need environment.
\end{remark}
Let \texttt{($\lambda$g.$\lambda$y.if (y = 0) y else g g (y - 1))} be the $\alpha$-converted version of \texttt{f} in \autoref{lst:recalpha}.
It should become clear that an $\alpha$-conversion algorithm must also follow the reduction order or else one can force terrible runtime by creating useless terms which are never executed but are converted.

Evaluation strategies (\autoref{tbd}) are a core part of the reduction strategy since the choice of evaluation strategy changes the order in which terms are evaluated.
A \textit{redex} is a reducible expression in the context of some set of reduction rules.
The two interesting evaluation orders are \textit{applicative order} and \textit{normal order}~\cite{sestoft2002demonstrating}.
Applicative order specifies that the parameters of some application should always be evaluated first, e.g. call by value.
Normal order specifies that the leftmost outermost term should be evaluated first which yields the call by name strategy.
Call by need is a bit more tricky since it requires more than a particular evaluation order to implement~\cite{levy1988sharing}.


When evaluating a term that results in a change to the substitution mapping the previous substitution mapping version is paired with it.
Letting the substitution mapping be 
$\{ \texttt{x} \mapsto (\lambda \texttt{x.g x}, \{ \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)) \}, \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)\}$
the space complexity may seem to grow by $O(n^3)$ (\autoref{eq:evalruntimewrong}) but lifting the data structure to using pointers instead of copies reduces the size to $O(n)$.
\begin{align}
    \label{eq:evalruntimewrong}
    & O \left( \sum_{k = 1}^n \sum_{i = 1}^{k} i \right) \\
    = \,\,\, & O \left( \sum_{k = 1}^n \frac{k(k + 1)}{2}\right)\tag{Triangular}\\
    = \,\,\,& O \left( \frac{1}{2} \left( \sum_{k = 1}^n k^2 + \sum_{k = 1} k \right) \right)\tag*{}\\
    = \,\,\,& O \left( \sum_{k = 1}^n k^2 \right)\tag*{}\\
    = \,\,\,& O \left( \frac{1}{6}n + \frac{1}{2}n^2 + \frac{1}{3}n^3 \right)\tag{Faulhaber}\\
    = \,\,\,& O \left( n^3 \right)\tag*{}
\end{align}
Observe that the introduction of a context is based on free variables in closures and can in fact be avoided at the cost of compilation time by simply making all variables unique.
Let \autoref{eq:eval:comp} be a program translated from \autoref{eq:eval:nocomp}.
\begin{align}
    &(\lambda x . \texttt{ let } f = \lambda y . \lambda x . x \texttt{ in } x) \label{eq:eval:nocomp}\\
    &(\lambda x_1 . \texttt{ let } x_2 = \lambda x_3 . \lambda x_4 . x_4 \texttt{ in } x_1)\label{eq:eval:comp}
\end{align}
The method of storing substitutions warrants the question of evaluation strategy since eagerly replacing the mapped to expressions implies call by value (\autoref{eq:eval:eagereval}).
If one were to keep the initially proposed semantics the result would be call by name (\autoref{eq:eval:name}).
In the hybrid approach forcing a term rewrites the mapped to value (\autoref{eq:eval:need}).
\begin{align}
    &\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto 5, \texttt{y} \mapsto 5 \} \label{eq:eval:eagereval} \eqtag{Call by value}\\
    &\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto \texttt{y}, \texttt{y} \mapsto 5 \} \label{eq:eval:name} \eqtag{Call by name}\\
    &\textit{force}(\texttt{z}, \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto \texttt{f 0} \}) = \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto 0 \} \label{eq:eval:need} \eqtag{Call by need}
\end{align}
One may also have an interpreter that supports multiple strategies at once by simply specifying what type of strategy should be applied to every term.
Evaluating a program is a matter of entering through some expression and continually reading terms and applying substitutions until said expression has been evaluated regardless of evaluation strategy. 

The algorithm for evaluating the untyped lambda calculus is similar to Algorithm W (\autoref{fig:dmrules}) and can be seen in.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
        \begin{subfigure}[b]{0.30\textwidth}
            \begin{prooftree}
                \AxiomC{$e$}
                \RightLabel{Terminal}
                \UnaryInfC{$\emptyset ,e$}
            \end{prooftree}   
            \label{eq:prooftree:terminal}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{0.69\textwidth}
            \begin{prooftree}
                \AxiomC{$S,\lambda x . e_1$}
                \AxiomC{$S \cdot \{ x \mapsto e_2 \},e_1 $}
                \RightLabel{App}
                \BinaryInfC{$S,e_3$}
            \end{prooftree}   
            \label{eq:prooftree:ref}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S \cdot \{ x \mapsto e_1 \},e_2$}
                \RightLabel{Let}
                \UnaryInfC{$S,\texttt{ let } x = e_1 \texttt{ in } e_2$}
            \end{prooftree}   
            \label{eq:prooftree:ref}
            \caption{}
        \end{subfigure}

    \end{mdframed}
    
\end{figure}

Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
Beta reductions with De Bruijn indices~\cite{de1972lambda} is another more straightforward method of evaluating the untyped lambda calculus.
De Bruijn indices is a representation of lambda calculus which deals with variables based on the scope ''distance`` instead of variable names.
\begin{lstlisting}[language=ML,caption={Add as De Bruijn},label={lst:adddebru},mathescape=true]
let add = ($\lambda$($\lambda$ 2 1))
\end{lstlisting}
Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
More generally all variable occurrences are replaced with the distance to the abstraction which introduced them.
The use of De Bruijn indices allow anonymous naming of parameters thus imply a method of solving the application problem when directly interpreting the untyped lambda calculus.
A minor but important detail when 
\begin{align}
    &(\lambda x . ((\lambda f. \lambda x. f x)(\lambda z . z))x)''0`` \rightarrow\\
    &
\end{align}

\end{document}
