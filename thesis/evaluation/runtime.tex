\documentclass[11pt,oneside,a4paper]{report}

\begin{document}

\section{Evaluation strategies}
\label{sec:es}
When evaluating the untyped lambda calculus one has to choose an evaluation strategy.
The choice of evaluation strategy has a large impact on aspects such as complexity guarantees.
Such strategies are \textit{call by value}, \textit{call by name} and \textit{call by need}.
Call by value is most often the simplest and most natural way of assuming program execution.
\begin{lstlisting}[language=ML,caption={Program that doubles values},label={lst:callbyvalue},mathescape=true]
fun double x = x + x;
let a = double 10;
double 10;
\end{lstlisting}
By the call by value semantics, \autoref{lst:callbyvalue} eagerly evaluates every expression.
Clearly the variable \texttt{a} is never used but under the call by value semantics everything is eagerly evaluated.
Every expression is evaluated in logical order in the call by value evaluation strategy.
\newline
\begin{minipage}{\textwidth}
\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:callbyname},mathescape=true]
fun suspend x unit = x;
fun force x = x 0;
let value = suspend 10;
fun double x = 
    fun susExpensiveOp unit = 
        (force x) + (force x);
    susExpensiveOp;
let a = double value;
force (double value);
\end{lstlisting}
\end{minipage}
The call by name semantics however does only evaluate expressions once they are needed.
By the call by name semantics \texttt{a} is never evaluated since it is never used.
In \autoref{lst:callbyname} call by name has been implemented by the use of various functions such as the two constant functions \texttt{suspend} and \texttt{force}.
\texttt{susExpensiveOp} ensures that the forcing (evaluation) of \texttt{x} never occurs until the caller of \texttt{double} forces the result.
By the aforementioned semantics of call by name in the context of the program in \autoref{lst:callbyname} \texttt{a} is never forced thus the computation is never performed.
The implementation of call by name can become quite troublesome and therefore in most cases is a part of the native execution environment which will be discussed in \autoref{tbd}.

The call by need strategy introduces \textit{lazy evaluation} semantics which is the same as call by name with one extra detail named \textit{sharing}.
In \autoref{lst:callbyname} \texttt{force x} is computed twice which may be an expensive operation.
Under call by need all results are saved for later use similar to techniques such as dynamic programming.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.3cm and 0.3cm]
                \node[circle, draw=black] (app1) {\texttt{app}};
                    \node[circle, draw=black, below right = of app1] (force) {\texttt{force}};
                    \path[-] (app1) edge node[left] {} (force);

                    \node[circle, draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[circle, draw=black, below left = of app2] (value) {\texttt{value}};
                        \path[-] (app2) edge node[left] {} (value);
                        \node[circle, draw=black, below right = of app2] (double) {\texttt{double}};
                        \path[-] (app2) edge node[left] {} (double);

                \node[draw,scale=1.3,fill=none,rectangle,fit=(app1)(value)(force)](fstB){};
        \end{tikzpicture}
        \caption{The last expression of the program.}
        \label{sub:eval:main}
    \end{subfigure}
    \begin{subfigure}[b]{0.66\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.3cm and 0.3cm]
                \node[circle, draw=black] (lamx) {\texttt{$\lambda$x}};
                    \node[circle, draw=black, below = of lamx] (lamu) {\texttt{$\lambda$unit}};
                    \path[-] (lamx) edge node[left] {} (lamu);
                        \node[circle, draw=black, below = of lamu] (app1) {\texttt{app}};
                        \path[-] (lamu) edge node[left] {} (app1);
                            \node[circle, draw=red, below right = of app1] (app2) {\texttt{app}};
                            \path[-] (app1) edge node[left] {} (app2);
                                \node[circle, draw=red, below right = of app2] (x1) {\texttt{x}};
                                \path[-] (app2) edge node[left] {} (x1);
                                \node[circle, draw=red, below = of app2] (force1) {\texttt{force}};
                                \path[-] (app2) edge node[left] {} (force1);

                            \node[circle, draw=black, below left = of app1] (app4) {\texttt{app}};
                            \path[-] (app1) edge node[left] {} (app4);
                                \node[circle, draw=black, below left = of app4] (add) {\texttt{+}};
                                \path[-] (app4) edge node[left] {} (add);

                                \node[circle, draw=red, below right = of app4] (app3) {\texttt{app}};
                                \path[-] (app4) edge node[left] {} (app3);
                                    \node[circle, draw=red, below = of app3] (x2) {\texttt{x}};
                                    \path[-] (app3) edge node[left] {} (x2);
                                    \node[circle, draw=red, below left = of app3] (force2) {\texttt{force}};
                                    \path[-] (app3) edge node[left] {} (force2);
                \node[draw,scale=1.3,fill=none,rectangle,fit=(lamx)(force2)(x1)(add)](fstB){};
        \end{tikzpicture}
    %\begin{tikzpicture}
        %\node[circle, draw=black] (lamx) {$\lambda \texttt{x}$};

        %\node[circle, draw=black, below = of lamx] (lamunit) {$\lambda \texttt{unit}$};

        %\node[circle, draw=black, below = of lamunit] (addition) {$+$};

        %\node[circle, draw=black, below left = of addition] (force1) {\texttt{force}};
        %\node[circle, draw=black, below = of force1] (x1) {\texttt{x}};
        
        %\node[circle, draw=black, below right = of addition] (force2) {\texttt{force}};
        %\node[circle, draw=black, below = of force2] (x2) {\texttt{x}};

        %\path[->] (lamx) edge node[left] {} (lamunit);
        %\path[->] (lamunit) edge node[left] {} (addition);

        %\path[->] (addition) edge node[left] {} (force1);
        %\path[->] (addition) edge node[left] {} (force2);

        %\path[->] (force1) edge node[left] {} (x1);
        %\path[->] (force2) edge node[left] {} (x2);
    %\end{tikzpicture}
        \caption{The expression tree for \texttt{double}}
        \label{sub:eval:double}
    \end{subfigure}
    \caption{}
    \label{fig:evalexpr}
\end{figure}
To understand this better observe the expression tree for \autoref{lst:callbyname} in \autoref{fig:evalexpr}.
Clearly the two red subtrees in \autoref{sub:eval:double} are identical thus they may be memoized such that the forcing of \texttt{x} only occurs once.
More generally if the execution environment supports lazy evaluation, once an expression has been forced it is remembered.

\section{Runtime environments}
Now that the untyped lambda calculus has been introduced, implemented and validated efficiently the question of execution naturally follows.
There exists many different well understood strategies to implement an execution environment for the untyped lambda calculus.
Naively it may seem straightforward to evaluate the untyped lambda calculus mechanically by $\beta$-reductions, but doing so brings upon some problems when implementing an interpreter.
%When applying the term $f x$ in $\lambda f . \lambda x . f x$ such that $f[? := x]$ where $?$ is the parameter name of $f$ it should become clear why a naive strategy is not enough since the parameters of $f$ are anonymous.

\subsection{Combinator reducers}
\label{sec:comb}
One of the most prominent techniques for evaluating functional programs is that of \textit{combinator graphs reductions}.
Formally a combinator is a function that has no free variables which is convenient since the problem of figuring out closures and parameter substitutions in applications never arises.
\begin{align}
    x \label{eq:comb:x}\\
    F \label{eq:comb:F}\\
    Y E \label{eq:comb:app}
\end{align}
There are three types of terms in combinator logic; the variable much like the lambda calculus (\autoref{eq:comb:x}), application (\autoref{eq:comb:app}) and the combinator (\autoref{eq:comb:F}).
The SKI calculus is a very simple set of combinators which are powerful enough to be turing complete and translate to and from the lambda calculus.
In SKI $F ::= S \,\,|\,\, K \,\,|\,\, I$ where the equivalent lambda calculus combinators for $S = \lambda x . \lambda y . \lambda z . x z (y z)$, $K = \lambda x . \lambda y . x$ and $I = \lambda x . x$.
Evaluating an SKI program is a straightforward reduction where $F'_F$ denotes combinator $F'$ has been partially applied with combinator $F$.
\begin{exmp}
    \begin{align}
        &SKSI\\
        = &KI(SI)\tag*{}\\
        = &K_I(SI)\tag*{}\\
        = &I\tag*{}
    \end{align}
\end{exmp}

The algorithm for converting a lambda calculus program into a SKI combinator program is a straightforward mechanical one.
The evaluation context is always an abstraction $\lambda x . E$.
\begin{pcases}
    \pcase{$E = x$ then rewrite $\lambda x . E$ to $I$.}
    \pcase{$E = y$ where $y \neq x$ and $y$ is a variable then rewrite $\lambda x . y$ to $Ky$.}
    \pcase{$E = Y E'$ then rewrite $\lambda x . Y E'$ to $S(\lambda x . Y)(\lambda x . E')$ since applying some $y$ to $\lambda x . Y E'$ must lambda lift $y$ as a parameter named $x$ to both $Y$ and $E'$ such that the lifted expression becomes $((\lambda x . Y)y)((\lambda x . E')y) = S(\lambda x . Y)(\lambda x . E')y$.
    Then recurse in both branches.
    }
    \pcase{$E = \lambda x . E'$ then first rewrite $E'$ with the appropriate cases recursively such that $E'$ becomes either $x$, $y$ or $Y E$ such that Case 1, 2 or 3 can be applied.}
\end{pcases}
The termination of the rewriting to SKI is guaranteed since abstractions are always eliminated and the algorithm never introduce any additional abstractions.
When translating the untyped lambda calculus to SKI the ''magic`` variable names $\sigma, \kappa$ and $\iota$ are used as placeholder functions for the SKI combinators since the translation requires a lambda calculus form.
When the translation has been completed then replace $\sigma \mapsto S, \kappa \mapsto K, \iota \mapsto I$.
\subsection{Combinator translation growth}
Before proving that the SKI translation algorithm produces a program of larger size the notion of size must be established.
Size in terms of lambda calculus are the amount of lambda terms (\autoref{lc:lang:abs}, \autoref{lc:lang:var} and \autoref{lc:lang:app}) that make up a program.
For instance $\lambda x . x$ has a size of two since it is composed of an abstraction and a variable term.
The size of an SKI combinator program is in terms of the number of combinators.
\begin{lemma}
    There exists a family of lambda calculus programs of size $n$ which are translated into SKI-expressions of size $\Omega(n^2)$.
\end{lemma}
\begin{proof}
\begin{pcases}
    \pcase{\label{eval:case:1} Rewriting $\lambda x . x$ to $I$ is a reduction of one.}
    \pcase{\label{eval:case:2} Rewriting $\lambda x . y$ to $Ky$ is equivalent in terms of size.}
    \pcase{Rewriting $\lambda x . YE$ to $S(\lambda x . Y)(\lambda x . E)$ is the interesting case.
        To induce the worst case size \autoref{eval:case:1} must be avoided.
        If $x \notin \textit{Free}(Y)$ and $x \notin \textit{Free}(E)$ then for every non-recursive term in $Y$ and $E$ \autoref{eval:case:2} is the only applicable rewrite rule which means that an at least equal size is guaranteed.
        Furthermore observe that by introducing unused parameters one can add one $K$ term to \textit{every} non-recursive case.
        Observe the instance $\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1)$ where the two unused parameters are used to add $K$ terms to all non-recursive cases in \autoref{eq:eval:red2} such that the amount of extra K terms minus the I becomes $\text{variable\_references} * (\text{unused\_abstractions} - 1) = 3*(3-1)$:
        \begin{gather}
            S(S(KKI)(KKI))(KKI)\label{eq:eval:red2}
        \end{gather}
        Now let the number of variable references be $n$ and the unused abstractions also be $n$ clearly $\Omega(n * (n - 1)) = \Omega(n^2)$
    }
    \pcase{Rewriting $\lambda x . E'$ is not a translation rule so the cost is based on what $E'$ becomes.}
\end{pcases}
    \vspace*{0.5cm}
Notice that the applications $f_1 f_1 \dots f_1$ can in fact be changed to $f_1 f_2 \dots f_n$ since for every $f_k$ where $0 < k \leq n$ there are $n - 1$ parameters that induce a K combinator.
Let $P_n$ be family of programs with $n$ abstractions and $n$ applications.
$\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1) \in P_3$ and in fact for any $p$ where $\forall n \in \mathbb{Z}^+ \,\, p \in P_n$ $p$ translates into SKI-expressions of size $\Omega(n^2)$.
\end{proof}
\begin{exmp}
    Observe the size of \autoref{eq:eval:comp1} in comparison to \autoref{eq:evaltime}.
\begin{align}
    &\lambda f_1 . \lambda f_2 . f_1 f_2 \label{eq:eval:comp1}\\
    =&\lambda f_1 . \sigma(\lambda f_2 . f_1)(\lambda f_2 . f_2) \tag*{} \\
    =&\lambda f_1 . (\sigma(\kappa f_1))(\iota) \tag*{} \\
    =&\sigma (\lambda f_1 . \sigma (\kappa f_1)) (\lambda f_1 . \iota) \tag*{} \\
    =&\sigma (\sigma (\lambda f_1 . \sigma) (\lambda f_1 . \kappa f_1)) (\kappa \iota) \tag*{} \\
    =&\sigma (\sigma (\kappa \sigma) (\sigma (\lambda f_1 . \kappa) (\lambda f_1 . f_1))) (\kappa \iota) \tag*{} \\
    =&\sigma (\sigma (\kappa \sigma) (\sigma (\kappa \kappa) (\iota))) (\kappa \iota) \tag*{} \\
    =&S (S (K S) (S (K K) (I))) (K I) \tag*{}
\end{align}
\end{exmp}
It should become clear that many programs suffer from this consequence such as \texttt{let add = ($\lambda$x.$\lambda$y.(+ x) y)} $\in P_2$ where the program is written in prefix notation.
Translating the lambda calculus into the SKI-expressions does indeed increase the size significantly but does not warrant a write off entirely.
More advanced techniques exist to translate the lambda calculus to linearly sized SKI-expressions with the introduction of more complicated combinators~\cite{kiselyov2018lambda}.

\subsection{Reduction strategies}
Reductions in the context of the lambda calculus are a small set of well-defined rules for rewriting such that a program is evaluated.
The techniques required to correctly evaluate a program are a bit more complicated than the SKI calculus but are rewarding in the performance.
The substitution mapping set, denoted $S$, is a set of variable names to their value denoted $\{ x \mapsto \lambda y . y \}$ meaning ``the value of variable $x$ is $\lambda y . y$''.
Substitutions mappings can be combined $S \cup \Sigma$, if a variable name occurs in both $S$ and $\Sigma$ then the variable in the rightmost mapping is choose $\Sigma$.
\begin{align}
  \{ x \mapsto y \} x = y\label{eq:subsem}\\
  \{ x \mapsto y \} z = z \tag*{}
\end{align}
Substitution mappings are used as in \autoref{eq:subsem}; if $Sx = y$ if $(x \mapsto y) \in S$ else $Sx = x$.
%In $(\lambda f . \lambda x . \lambda y. \texttt{ let } a = f x \texttt{ in } a + y)(\lambda y . y) \,\, 10 \,\, 20$ when evaluating $f x$ where the substitution are $[f \mapsto \lambda y . y, x \mapsto 10, y \mapsto 20]$ there must be taken great care that $x$ is mapped to $y$ in $\lambda y . y$.

Evaluation strategies (\autoref{sec:es}) are a core part of the reduction strategy since the choice of evaluation strategy changes the order in which terms are evaluated.
A \textit{redex} is a reducible expression in the context of some set of reduction rules.
The two interesting evaluation orders are \textit{applicative order} and \textit{normal order}~\cite{sestoft2002demonstrating}.
Applicative order specifies that the parameters of some application should always be evaluated first, e.g. call by value.
Normal order specifies that the leftmost outermost term should be evaluated first which yields the call by name strategy.
Before delving into more complicated evaluation strategies such as call by need, call by name will be considered.

A naive reduction strategy would involve adding variables to the substitution set, once they are applied.
When evaluating a term such as \texttt{($\lambda$x.x + 5) 5}, \texttt{x} must be substituted by \texttt{5} such that the expression becomes \texttt{x + 5} with the substitution mapping \texttt{\{x $\mapsto$ 5\}}.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{S $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{A simple application rule}
    \label{fig:simpleapp}
\end{figure}
More generally, a simple rule for App could be defined as in \autoref{fig:simpleapp} which states ``Evaluate \texttt{($\lambda$x.e) z} with the substitution mapping \texttt{S} to \texttt{y} with the substitution mapping $\Sigma$ by first evaluating \texttt{e} with the substitution mapping \texttt{S} $\cup$ \texttt{\{x $\mapsto$ z\}} to \texttt{y} with the substitution mapping $\Sigma$''.

Let expressions are considered as if abstractions and applications; \texttt{let a = b in c $\rightarrow$ ($\lambda$a.c)b}.
\begin{lstlisting}[language=ML,caption={Problematic program},label={lst:problemprog},mathescape=true]
fun f a = 
  fun g x = 
    let a = 20;
    a + x;
  (g a) + a;
f 10;
\end{lstlisting}
Evaluating \autoref{lst:problemprog} using any reduction method (say $\beta$-reductions for simplicity), yields an interesting case of variable ambiguity.
At some point the evaluation machine will reach a state with the expression \texttt{(g a) + a} and substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ ($\lambda$x.let a = 20 in (a + x))\}}.
Evaluating the left side of \texttt{(g a) + a} first, yields \texttt{30 + a} with the substitution mapping \texttt{\{a $\mapsto$ 20, g $\mapsto$ $\dots$\}}, and finally \texttt{50}.
Evaluating the right side of \texttt{(g a) + a} first, yields \texttt{(g a) + 10} with the substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ $\dots$\}}, and finally \texttt{40}.
Clearly the order of evaluation is of importance for the result, which is often not the desired semantics.
If there exists multiple reduction techniques which ensure the same result, the system is called \textit{confluent}.
Reduction strategies are confluent, which leads to the question of what result is the correct one?
In $L$ variables only exist in the scope they are declared in, thus the expected result is \texttt{40}.
The different results are an effect of leaking the substitution mapping to the outside of the lexical scope in which is was used.
Confluence is an important property, since programs are difficult to reason with if the interpretation environment produces different results depending on the evaluation order, even if the program is pure.
A simple solution could involve saving the substitution mapping once the machine enters an abstraction, then restore the same substitution mapping when it leaves.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{S $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ S, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which does not leak}
    \label{fig:cleanapp}
\end{figure}
\noindent \autoref{fig:cleanapp} is a slightly modified version of \autoref{fig:simpleapp}, with the difference of requiring the resulting substitution mapping to be exactly the initial substitution mapping.

\autoref{fig:cleanapp} does not handle recursion that well, under normal order evaluations, which becomes apparent in expressions such as \texttt{let f = ($\lambda$f.$\lambda$x.if (x == 0) 0 (x + (f (x - 1))) in f f n)}.
Once the base case is reached (\texttt{x == 0}), then some expression has accumulated in the substitution set \texttt{\{x $\mapsto$ x + ((x - 1) + $\dots$ + ((x - 1) - 1 $\dots$))\}}, which is clearly not valid.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{S $\cup$ \{x $\mapsto$ Sz\}, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ S, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which works for recursion}
    \label{fig:recapp}
\end{figure}
\noindent The problem of recursion can be solved by never introducing variables (\autoref{fig:recapp}).
An unfortunate consideration which yields \autoref{fig:recapp} unsatisfactory, under the current semantics, is that \texttt{z} can also be bound to an expression which is not a variable, like an application for instance (parameters are never evaluated first in normal order).
One could recursively apply substitutions to \texttt{z}, but then the suspense would be broken.
For this method to work then substitutions must be applied lazily.
The notion of substitutions should now be changed such that substitutions are performed recursively and lazily (\autoref{eq:subsemlaz}).
\begin{align}
  &\{ x \mapsto y \} x = y &\label{eq:subsemlaz}\\
  &\{ x \mapsto y \} z = z  &\tag*{}\\
  &S f z = (Sf)(Sz) &\tag*{}\\
  &S (\lambda x.e) = (\lambda x.Se) & (x \mapsto y) \notin S \tag*{}\\
  &S (\lambda x.e) = (\lambda x.(S \backslash \{x \mapsto y\})e) & (x \mapsto y) \in S \label{eq:subsemlaz2}
\end{align}
Expressions can be modelled by a pair \texttt{S,e}, where \texttt{S} is a substation mapping which should be considered once \texttt{e} is evaluated. 
If a substitution mapping \texttt{S} is applied and then a substitution mapping $\Sigma$ is applied, then the resulting set for an expression should become \texttt{S $\cup$ $\Sigma$}.
\autoref{eq:subsemlaz2} has been introduced for completeness, but can safely be ignored since it cannot occur because of the substitution union semantics.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{\{x $\mapsto$ z\} e $\rightarrow$ y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{($\lambda$x.e) z $\rightarrow$ y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which has abstracted away substitution mappings}
    \label{fig:recappsimple}
\end{figure}
Notice that any variables introduced must be so through applications.
All the substitution work can now be moved into the introduced variable (\autoref{fig:recappsimple}).
\begin{remark}
  Since substitutions are now local to variables they are can be omitted when written out.
\end{remark}
Surely a system which uses \autoref{fig:recappsimple} is confluent, but, alas this is not yet the case.
Consider the following valid state \texttt{\{x $\mapsto$ z, y $\mapsto$ x\} ($\lambda$x.y)g}.
When visiting \texttt{($\lambda$x.y)} then the following substitution mapping is applied \texttt{\{x $\mapsto$ g\}}.
Once \texttt{y} is visited the substitution mapping will be \texttt{\{x $\mapsto$ g, y $\mapsto$ x\}}, thus the expression is equivalent to \texttt{($\lambda$x.x)g}, which is not necessarily correct in the case that \texttt{z $\neq$ g}.

The shortcomings of \autoref{fig:simpleapp} become more apparent once one begins to consider more exotic evaluation strategies, namely call by need.
Looking at call by need from a philosophical perspective, indicates that reduction rules must support returning information about newly computed variable values ``up'' through the evaluation tree.
Furthermore when returning these modified variables up through the program tree, all variables which point to the same expression must also be updated.
\\

The first step in correctly evaluating the lambda calculus is applying \textit{$\alpha$-conversions} which is the operation of renaming.
The $\alpha$-conversion mapping written $[x_1 \mapsto \gamma_1, x_2 \mapsto \gamma_2 \dots , x_n \mapsto \gamma_n]$ states that for some $x_k$ where $1 \leq k \leq n$ all variables in the program tree $v$ where $v = x_k$ should be renamed to $\gamma_k$.
Let \texttt{$\lambda$f.$\lambda$x.x + (f (x - 1))} be a term where a collision between the two $x$'s causes trouble, the $\alpha$-conversion can be $\lambda y . ((\lambda x . x) y)$ since $y$ and $x$ have been introduced by separate abstractions.
$\alpha$-conversions solve some critical problems such as closures and recursion when evaluating the lambda calculus.
$\alpha$-conversions should also be non-destructive but still be context aware such that when leaving an abstraction the remaining substitution mapping is a superset of the substitution mapping when entering.
More formally $\textit{eval}: \Lambda \times S \rightarrow \Lambda \times S$ where $S$ is the domain of substitution mappings and $\Lambda$ is in the domain of lambda expressions such that  $(\lambda',s') = eval(\lambda, s)$ always implies $s \subseteq s'$.
\begin{lstlisting}[language=ML,caption={Recursive addition function},label={lst:recalpha},mathescape=true]
let f = ($\lambda$f'.$\lambda$x.
    if (x = 0) x else f' f' ((x - 1) + (x - 1))) in
f f 10
\end{lstlisting}
A strong guarantee that can be made by tuning the evaluation strategy which is particularly useful for $\alpha$-conversion algorithms is that \textit{any} returned value has had \textit{every} term that it contains visited.
An algorithm can be implemented that picks a new variable name from any infinite domain when an abstraction has had a value applied to it and replaces future encountered variables with the new one, such an algorithm only works if the guarantee of visiting every term is made.
The algorithm should also introduce the applied value to the substitution set through the alpha converted name.
Let \texttt{($\lambda\gamma_1$.$\lambda\gamma_2$.if ($\gamma_2$ = 0) $\gamma_2$ else $\gamma_1$ $\gamma_1$ (($\gamma_2$ - 1) + ($\gamma_2$ - 1)))} be the $\alpha$-converted version of \texttt{f} in \autoref{lst:recalpha}.
It should become clear that an $\alpha$-conversion algorithm must also follow the reduction order or else one can force terrible runtime in a call by name (or need) environment by creating purposeless terms which are never executed but are converted.
In a call by need or call by name environment one must suspend the conversion until it is needed; let $\alpha E$ denote the $\alpha$-conversion for some conversion mapping $\alpha$ on a lambda expression $E$ which either eagerly or lazily $\alpha$-converts $E$ dependent on the evaluation strategy.
In \autoref{subfig:red4} the application which contains \texttt{$\lambda$f'} and \texttt{f'} as children (the purple node) must $\alpha$-convert \texttt{f'} in ``this scope''.
If \texttt{f'} is not $\alpha$-converted a circular dependency will arise $S = \{ \gamma \mapsto \texttt{f'} \}, \alpha = [ \texttt{f'} \mapsto \gamma ]$.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=black] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=red, below left = of app2] (f1) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f1);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f2);

                \node[draw,scale=1.3,fill=none,rectangle,fit=(app1)(f1)(var1)](fstB){};
                \path (fstB.north east) -- (fstB.south east) coordinate[midway] (fstP);
        \end{tikzpicture}
        \caption{
            \\
            \texttt{$S_1 = \{ \gamma_1 \mapsto $($\lambda$f'.$\lambda$x.if (x = 0) x else f' f' (x - 1))}\\
            \texttt{$\alpha_1 = [ \texttt{f} \mapsto \gamma_1 ]$}
        }
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=black] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=red, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        \path[-] (app2) edge node[left] {} (lam1);
                            \node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            \path[-] (lam1) edge node[left] {} (lam2);
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                \path[-] (lam2) edge node[left] {} (app3);
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ((x - 1) + (x - 1))}};
                                    \path[-] (app3) edge node[left] {} (iff);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f2);

                \node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](sndB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \\
                \texttt{$S_2 = S_1$}\\
                \texttt{$\alpha_2 = \alpha_1$}
            }
    \end{subfigure}
    %\tikz[overlay,remember picture]{\draw [->] (fstP) -- (fstP-|sndP);}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    \path[-] (app3) edge node[left] {} (iff);
                \node[draw,fill=none,scale=1.3,rectangle,fit=(app3)(if)(ifc)(iff)](fouB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \\
                \texttt{$S_3 = S_2 \cup \{ \gamma_2 \mapsto S_2\gamma_1, \gamma_3 \mapsto 10 \}$}\\
                \texttt{$\alpha_3 = \alpha_2 \cup [\texttt{f'} \mapsto \gamma_2, \texttt{x} \mapsto \gamma_3]$}
            }
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=blue] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{x - 1}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=purple, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=black, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        \path[-] (app2) edge node[left] {} (lam1);
                            \node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            \path[-] (lam1) edge node[left] {} (lam2);
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                \path[-] (lam2) edge node[left] {} (app3);
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=black, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    \path[-] (app3) edge node[left] {} (iff);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f'}};
                        \path[-] (app2) edge node[left] {} (f2);
                \node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](fouB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \\
                \texttt{$S_4 = S_3$}\\
                \texttt{$\alpha_4 = \alpha_3$}
            }
            \label{subfig:red4}
    \end{subfigure}
\end{figure}
%\tikz[overlay,remember picture]{\draw[-latex,thick] (fstB) -- (fstB-|sndB) node[midway,below,text width=0.5cm]{};} 

Call by need is a bit more tricky since it requires more than a particular evaluation order to implement~\cite{levy1988sharing}.
Substitutions can be rewritten when a variable \texttt{x} maps to an application such that for some substitution mapping of the form $\{ \dots, \texttt{x} \mapsto Y E \}$ then the substituted to value is reduced.
When two substitutions are combined one should always choose the most reduced of the two, which is trivial since it becomes a matter of picking the ''newest`` version of the substituted value.
An important observation which is discussed in~\cite{levy1988sharing} is that of \textit{object} duplication.
Objects are the substituted to values which are also lambda terms.
The term object is used to emphasize the uniqueness of applications.
A problem when sharing evaluation in the substitution set is that of transitive substitutions.
Transitive substitutions are not expressible since that causes non-linear evaluation times on some inputs.
Naturally when new mappings are introduced the mapped to values can be duplicated instead such that instead of $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto \gamma_1 \}$ then $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ which violates call by need.
Instead $E$ can be labelled with some name $a$ when found in the evaluation process such that when some value with the label $a$ is evaluated then all duplications of $E$ can share the computed result, effectively adding another type of mapping.
\begin{remark}
    The system which implements call by need is not of much importance.
    Sharing can also be implemented in a way which promotes more imperative constructs, such as variables.
    When some object $E$ is discovered it is declared as a mutable pointer by the interpreter (or natively if implemented in a lazy language) such that all duplications effectively point to the same object.
\end{remark}

Let $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ be the system without labels, then let \autoref{eq:labelsystem} be the labelled call by need system.
\begin{align}
    \label{eq:labelsystem}
    S &= \{ \gamma_1 \mapsto a, \gamma_2 \mapsto a \}\\
    \Omega &= \{ a \mapsto E \} \tag*{}
\end{align}
By \autoref{eq:labelsystem} it should becomes clear that object duplication is mitigated by ``merging'' references.

$\alpha$-conversion loses some of it's purpose when a labelling approach is considered.
\begin{lstlisting}[language=ML,caption={Program},label={lst:labeltest},mathescape=true]
fun id a = a;
fun f g a =
  id (g a a);
\end{lstlisting}




%The labelled system can evaluate expressions in parallel by implementing locking (or cooperative yielding) on the label level (in \autoref{eq:labelsystem} evaluating $\gamma_1$ and $\gamma_2$ in paralell would lock $a$).
%The only scenario where mutual evaluation of an expression is possible, is when two other expressions can access (either through abstraction or closure) the same value.
%\\
An evaluation written $S, \Omega, \alpha, x \rightarrow \Theta, e$ means $x$ evaluates to $e$ with the resulting label mapping $\Theta$, under some substitution mapping $S$, label mapping $\Omega$ and $\alpha$-conversion mapping $\alpha$.
$Sx$ means that variable $x$ is replaced with whatever the substitution map maps $x$ to if $x \in S$ otherwise $Sx = x$, the same operation counts for $\alpha$.
$\Omega l \rightarrow k$ should be read as, the expression which $l$ points to should be evaluated and be bound to $k$.
In a call by value environment $\alpha x$ can eagerly convert since the value must be evaluated eagerly.
In a call by name (or need) environment $\alpha x$ should remain suspended until needed like discussed in this section.
%An implementation (\autoref{lst:cbn}) of the call by name reduction algorithm \autoref{fig:cbn} can easily be extended with call by need by the aforementioned modifications.
%\begin{remark}
    %In the implementation the \texttt{Map} constructor is the usual implementation of the Map data structure.
    %Also the implementation is generalized in that it does not describe operations such as addition or whether it is a call by need or name environment.
%\end{remark}
%\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:cbn},mathescape=true]
%type Three a b c =
    %| Triple a b c;
%type Term =
    %| Abstraction Int Term
    %| Var Int 
    %| Application Term Term
%;

%fun reduceApp S alpha e1 e2 =
    %match e1 ->
        %| Abstraction x e' ->
            %let newvar = newvar in
            %let newS = 
                %Map newvar (apply alpha e2) in
            %let S2 = union S newS in
            %let newA =
                %Map x newvar in
            %let alpha2 = union alpha newA in
            %reduce newS newA e1
        %| e' -> match (reduce S alpha e')
            %| Triple S' alpha' e'' ->
                %let Sn = union S S' in
                %let alphan = union alpha alpha' in
                %let en = Application e'' e2
                %reduce Sn alphan en

%fun reduce S alpha e =
    %match e 
        %| Abstraction x e' -> 
            %reduce S alpha e'
        %| Var x -> match (find x S) 
            %| Just e' -> Triple S alpha e'
            %| Nothing -> Triple S alpha x
        %| Application e1 e2 -> 
            %reduceApp S alpha e1 e2
%\end{lstlisting}
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.4cm}
        %\begin{subfigure}[b]{0.30\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S, \alpha, S \alpha x \rightarrow \Sigma, e$}
                %\RightLabel{Var}
                %\UnaryInfC{$S, \alpha, x \rightarrow \Sigma, e$}
            %\end{prooftree}   
            %\label{eq:prooftree:ident}
            %\caption{}
        %\end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{$S,\alpha,(\lambda x.e) \rightarrow S,\alpha,(\lambda x.e)$}
            \end{prooftree}   
            \label{eq:prooftree:abs}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S \cup \{ \gamma \mapsto e \},\alpha \cup \{ x \mapsto \gamma \} , e \rightarrow \Sigma, w$}
                %\AxiomC{$S \alpha x \not\equiv \alpha x$}
                \RightLabel{Var share (pointer)}
                \UnaryInfC{$S \cup \{  \gamma \mapsto e \}, \alpha \cup [ x \mapsto \gamma ], x \rightarrow \Sigma, e \hspace*{1cm} e := w$}
            \end{prooftree}   
            \label{eq:prooftree:point}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S,\alpha, y \rightarrow \Sigma, (\lambda x . e)$}
                \AxiomC{$\Sigma \cup \{ \gamma \mapsto \Sigma \alpha p \},\alpha \cup [ x \mapsto \gamma ],e \rightarrow \Delta, w \,\,\, \gamma = \texttt{newvar}$}
                \RightLabel{App}
                \BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Delta, w$}
            \end{prooftree}   
            \label{eq:prooftree:app}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S,\alpha,y \rightarrow \Sigma, f$}
                \AxiomC{$f \not\equiv (\lambda x . e)$}
                \RightLabel{App (Reduce left first)}
                \BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Sigma, (f \,\, p)$}
            \end{prooftree}   
            \label{eq:prooftree:appleft}
            \caption{}
        \end{subfigure}
    \end{mdframed}
    \caption{Reduction rules for the call by need lambda calculus}
    \label{fig:cbn}
\end{figure}
\clearpage
\begin{exmp}
Consider the following program which duplicates an object.
\begin{lstlisting}[language=ML,caption={Object duplication},label={lst:objdup},mathescape=true]
let d = ($\lambda$f.$\lambda$x.f x x) in
d ($\lambda$z.$\lambda$y.z + y) (($\lambda$i.i) 0)
\end{lstlisting}
The evaluation of \autoref{lst:objdup} can be seen in \autoref{eq:objdup} (the underlying call by need implementation is not of interest).
\begin{align}
    \label{eq:objdup}
    &\texttt{g s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\\
    \rightarrow &(\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y})\texttt{ s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\tag*{}\\
    \rightarrow &(\lambda \texttt{y}.\texttt{z + y})\texttt{ s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto s \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}]\tag*{}\\
    \rightarrow &\texttt{c + q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    \rightarrow &\texttt{+ s q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    \rightarrow &\texttt{+ force(s) q }\{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ force(}(\lambda\texttt{i}.\texttt{i})0\texttt{) q } \{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ force(0) q }\{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 q }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 s }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 0 }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{0 }\{ \dots, s \mapsto 0 \}\tag*{}
\end{align}
\end{exmp}

\subsubsection{Garbage collection}
Garbage collection in purely function languages is a bit of a controversial topic since there exists no resources to release.
But adding mappings from lambda terms to variables without any subtractions will lead to an ever increasing substitution mapping size.
When performing reductions one might desire to garbage collect the substitution mapping once it contains useless substitutions.
Fortunately the task of collecting garbage is quite easy since the ``scope'' of a substitution is only relevant where the substituted variables has been introduced, which may only happen in the abstraction term.
When some algorithm with garbage collection evaluates some abstraction term $S,[\texttt{x} \mapsto \gamma]$,\texttt{$\lambda$x.E}, the algorithm should remove the introduced variable \texttt{x} by inspecting the $\alpha$-conversion mapping such that the substitution mapping becomes $S\backslash \{ \gamma \}$ and the $\alpha$-conversion alike.

\subsection{An invariant on infinite programs}
An important problem still remains which is that of infinite programs.
Imperative programming languages often solve this by introducing loops, whereas functional programming languages use recursion.
Recursion may be equally powerful in terms of expressiveness, but becomes a bit more tricky when cosidering interpreter details.
A prerequisite for an infinitely running program to exist in practice is that the program must not grow it's resource needs as it runs.

The distinction between recursive functions and loops in imperative programming languages is often what makes infinite programs expressible.
In a traditional imperative language, a function allocates a \textit{stack frame} and is explicitly parameterized, whereas a loop acts more like an anonymous closure which is always parameterized with itself (a function which is wrapped in a fixed point combinator, like the Y-combinator).
\begin{remark}
    A call stack is a stack of stack frames.
    A stack frame is a pointer to a function pointer.
    Stack frames are used to return execution to the previous function (the calling function).
    Every time a new function is called, the called-from function places a ``resume execution from here'' pointer onto the call stack.
\end{remark}
\noindent Imperative languages are also often evaluated under call by value which further simplifies implementation details.
Imperative loops (more interestingly, infinite loops) can safely release all static resources (variables bindings), which were allocated in the iteration, once an iteration has completed.
In traditional imperative languages recursive functions can only iterate a finite number of times, more specifically until the call stack is full.
\begin{figure}
\begin{lstlisting}[language=ML,caption={Program that implements two functions that fold a \texttt{List a} to a \texttt{b}},label={lst:listfoldboth},mathescape=true]
type List a = 
    | Nil
    | Cons a (List a)
;
fun add a b = a + b;

fun foldl f z l =
    match l
        | Nil -> z;
        | Cons x xs -> 
            foldl f (f x z) xs;
    ;

fun foldr f z l =
    match l
        | Nil -> z;
        | Cons x xs ->
            f x (foldr f z xs);
    ;
\end{lstlisting}
\end{figure}
%fun mapTail r f =
%//The type of mapTail
%//List a $\rightarrow$ (a $\rightarrow$ b) $\rightarrow$ (List b $\rightarrow$ List b) $\rightarrow$ List b
    %fun mapImpl r f b =
        %match r
            %| Nil -> b Nil;
            %| Cons x xs -> 
                %mapTail xs f (b (Cons (f x)));
        %;
    %match r
        %| Nil -> Nil;
        %| Cons x xs -> mapImpl xs f (Cons (f x));
    %;
%fun mapInf f l = 
    %match l
        %| Nil -> Nil;
        %| Cons x xs -> Cons (f x) (mapGrowing f xs);
    %;
%\end{lstlisting}
%\end{figure}

To really understand what happens in a lambda calculus interpreter, we must understand what happens in \autoref{lst:listfoldboth}.
\autoref{lst:listfoldboth} implements two variants of a \texttt{fold} function which accumulates a list of type \texttt{List a} to a \texttt{b}.
The two variants differ when considering evaluation strategy and \textit{tail call optimization}.
\begin{remark}
    Tail call optimization is an optimization which can be performed on programs with a particular structure.
    If the last expression is a function invocation, then the rewritten program does not grow.
    For instance the expression \texttt{let f = ($\lambda$g.$\lambda$x.g x) in $\dots$ f g' 0} is eventually rewritten to \texttt{g' 0}.
    If for instance the expression awaited a result like in \texttt{let f = ($\lambda$g.$\lambda$x.x + (g x)) in $\dots$ f g' 0}, then it would be rewritten to \texttt{x + (g' 0)}, increases the size of the program by \texttt{x +}, since the \texttt{+} operator requires both expressions to be evaluated.
    It should become clear that reduction strategies always imply tail call optimization, whenever possible.
\end{remark}
The first flavor of \texttt{fold}; \texttt{foldl}, implements \texttt{fold} such that the program expression tree does not grow throughout program interpretation, under a call by value environment.
The constraint on evaluation strategy is important for \texttt{foldl}, for reasons which will become clear once other evaluation strategies are discussed.
\begin{align}
    &\texttt{ foldl 0 add (Cons 1 (Cons 2 $\dots$ (Cons n Nil)))}  \\
    =&\texttt{ l z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ foldl f (f x z) xs $\{$ xs $\mapsto$ (Cons 2 (Cons 3 $\dots$ (Cons n Nil))), x $\mapsto$ 1, $\dots$ $\}$}  \tag*{} \\
    =&\texttt{ foldl add (add 1 0) (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    =&\texttt{ foldl add 1 (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    & \dots \tag*{}
\end{align}
Evaluating \texttt{foldl} on a list of size \texttt{n} with the addition function showcases how the program only grows by a constant number of terms.
\begin{remark}
    Note again that the list is always refereed to by reference; the list is not copied.
\end{remark}
%\begin{remark}
    %If the program is to remain within a constant amount of space throughout the evaluation of \texttt{foldl}, then garbage collection must support this choice.
    %Clearly a simple strategy 
%\end{remark}

%\\
%\begin{minipage}{\textwidth}
%\begin{lemma}
    %\texttt{mapTail} is tail recursive and requires an additional $O(n \cdot m)$ space when evaluated on a list of size $n$ with a function \texttt{f} produces values of size $m$.
%\end{lemma}
%\begin{proof}
    %Unpacking \texttt{Cons} and \texttt{Nil} as their underlying scott encoded functions is omitted for readability.
    %In this proof the list is of \texttt{Int} and \texttt{f} is the identity function.
    %For some $1 \leq k \leq n$ where $n$ is the size of the list, at the recursive invocation of \texttt{mapImpl}, the list is partitioned into the lower $1, \dots k - 1$ elements (\texttt{b (Cons x)}) and the higher $k, \dots n$ elements (\texttt{xs}).
    %\begin{align}
        %&\texttt{ mapTail (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) ($\lambda$x.x)}  \\
        %=&\texttt{ r Nil ($\lambda$x.$\lambda$xs.mapImpl xs ($\lambda$x.x) (Cons (f x)))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1)}  \eqtag{2$\dots$n} \label{eq:map:2n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (Cons 1 (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 3 (Cons 4 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2))}  \eqtag{3$\dots$n} \label{eq:map:3n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 (Cons 2 xs)) (Cons 3))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 4 (Cons 5 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2 (Cons 3)))}  \eqtag{4$\dots$n} \\
        %=&\dots\tag*{}\\
        %=&\texttt{ mapImpl (Cons k (Cons (k + 1) $\dots$ (Cons n Nil)))} \tag*{} \\
        %&\texttt{ ($\lambda$x.x) (Cons 1 ($\dots$ (Cons (k - 1))))}  \tag*{} 
    %\end{align}
    %The base case for the proof is once \texttt{mapImpl} is recursively invoked (\autoref{eq:map:2n}), size of the newly created list is $m \cdot 1$ (\texttt{f (Cons 1)}).
    %The first recursive iteration for \texttt{mapImpl} is the step between \autoref{eq:map:2n} and \autoref{eq:map:3n} such that the size becomes $m \cdot 2$ (\texttt{Cons (f 1) (Cons (f 2))}).
    %If we assume that the first $k$ recursive iterations have size $m \cdot k$, then the $k+1$'th iteration has size $m \cdot (k + 1)$ by \texttt{Cons (f 1) (Cons (f 2) $\dots$ (Cons (f (k + 1))))}.
    %Clearly whet $k = n$ then the newly created list is of size $m \cdot n$ and \texttt{xs} is empty, thus the function is.
%\end{proof}
%\end{minipage}

%When evaluating a term that results in a change to the substitution mapping the previous substitution mapping version is paired with it.
%Letting the substitution mapping be 
%$\{ \texttt{x} \mapsto (\lambda \texttt{x.g x}, \{ \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)) \}, \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)\}$
%the space complexity may seem to grow by $O(n^3)$ (\autoref{eq:evalruntimewrong}) but lifting the data structure to using pointers instead of copies reduces the size to $O(n)$.
%\begin{align}
    %\label{eq:evalruntimewrong}
    %& O \left( \sum_{k = 1}^n \sum_{i = 1}^{k} i \right) \\
    %= \,\,\, & O \left( \sum_{k = 1}^n \frac{k(k + 1)}{2}\right)\tag{Triangular}\\
    %= \,\,\,& O \left( \frac{1}{2} \left( \sum_{k = 1}^n k^2 + \sum_{k = 1} k \right) \right)\tag*{}\\
    %= \,\,\,& O \left( \sum_{k = 1}^n k^2 \right)\tag*{}\\
    %= \,\,\,& O \left( \frac{1}{6}n + \frac{1}{2}n^2 + \frac{1}{3}n^3 \right)\tag{Faulhaber}\\
    %= \,\,\,& O \left( n^3 \right)\tag*{}
%\end{align}
%Observe that the introduction of a context is based on free variables in closures and can in fact be avoided at the cost of compilation time by simply making all variables unique.
%Let \autoref{eq:eval:comp} be a program translated from \autoref{eq:eval:nocomp}.
%\begin{align}
    %&(\lambda x . \texttt{ let } f = \lambda y . \lambda x . x \texttt{ in } x) \label{eq:eval:nocomp}\\
    %&(\lambda x_1 . \texttt{ let } x_2 = \lambda x_3 . \lambda x_4 . x_4 \texttt{ in } x_1)\label{eq:eval:comp}
%\end{align}
%The method of storing substitutions warrants the question of evaluation strategy since eagerly replacing the mapped to expressions implies call by value (\autoref{eq:eval:eagereval}).
%If one were to keep the initially proposed semantics the result would be call by name (\autoref{eq:eval:name}).
%In the hybrid approach forcing a term rewrites the mapped to value (\autoref{eq:eval:need}).
%\begin{align}
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto 5, \texttt{y} \mapsto 5 \} \label{eq:eval:eagereval} \eqtag{Call by value}\\
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto \texttt{y}, \texttt{y} \mapsto 5 \} \label{eq:eval:name} \eqtag{Call by name}\\
    %&\textit{force}(\texttt{z}, \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto \texttt{f 0} \}) = \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto 0 \} \label{eq:eval:need} \eqtag{Call by need}
%\end{align}
%One may also have an interpreter that supports multiple strategies at once by simply specifying what type of strategy should be applied to every term.
%Evaluating a program is a matter of entering through some expression and continually reading terms and applying substitutions until said expression has been evaluated regardless of evaluation strategy. 

%The algorithm for evaluating the untyped lambda calculus is similar to Algorithm W (\autoref{fig:dmrules}) and can be seen in.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
        %\begin{subfigure}[b]{0.30\textwidth}
            %\begin{prooftree}
                %\AxiomC{$e$}
                %\RightLabel{Terminal}
                %\UnaryInfC{$\emptyset ,e$}
            %\end{prooftree}   
            %\label{eq:prooftree:terminal}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{0.69\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S,\lambda x . e_1$}
                %\AxiomC{$S \cdot \{ x \mapsto e_2 \},e_1 $}
                %\RightLabel{App}
                %\BinaryInfC{$S,e_3$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S \cdot \{ x \mapsto e_1 \},e_2$}
                %\RightLabel{Let}
                %\UnaryInfC{$S,\texttt{ let } x = e_1 \texttt{ in } e_2$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}

    %\end{mdframed}
    
%\end{figure}

%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%Beta reductions with De Bruijn indices~\cite{de1972lambda} is another more straightforward method of evaluating the untyped lambda calculus.
%De Bruijn indices is a representation of lambda calculus which deals with variables based on the scope ''distance`` instead of variable names.
%\begin{lstlisting}[language=ML,caption={Add as De Bruijn},label={lst:adddebru},mathescape=true]
%let add = ($\lambda$($\lambda$ 2 1))
%\end{lstlisting}
%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%More generally all variable occurrences are replaced with the distance to the abstraction which introduced them.
%The use of De Bruijn indices allow anonymous naming of parameters thus imply a method of solving the application problem when directly interpreting the untyped lambda calculus.
%A minor but important detail when 
%\begin{align}
    %&(\lambda x . ((\lambda f. \lambda x. f x)(\lambda z . z))x)''0`` \rightarrow\\
    %&
%\end{align}

\end{document}
