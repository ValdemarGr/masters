\documentclass[11pt,oneside,a4paper]{report}

\begin{document}

\section{Evaluation strategies}
\label{sec:es}
When evaluating the untyped lambda calculus one has to choose an evaluation strategy.
The choice of evaluation strategy has a large impact on aspects such as complexity guarantees.
Such strategies are \textit{call by value}, \textit{call by name} and \textit{call by need}.
Call by value is most often the simplest and most natural way of assuming program execution.
\begin{lstlisting}[language=ML,caption={Program that doubles values},label={lst:callbyvalue},mathescape=true]
fun double x = x + x;
let a = double 10;
double 10;
\end{lstlisting}
By the call by value semantics, \autoref{lst:callbyvalue} eagerly evaluates every expression.
Clearly the variable \texttt{a} is never used but under the call by value semantics everything is eagerly evaluated.
Every expression is evaluated in logical order in the call by value evaluation strategy.
\newline
\begin{minipage}{\textwidth}
\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:callbyname},mathescape=true]
fun suspend x unit = x;
fun force x = x 0;
let value = suspend 10;
fun double x = 
    fun susExpensiveOp unit = 
        (force x) + (force x);
    susExpensiveOp;
let a = double value;
force (double value);
\end{lstlisting}
\end{minipage}
The call by name semantics however does only evaluate expressions once they are needed.
By the call by name semantics \texttt{a} is never evaluated since it is never used.
In \autoref{lst:callbyname} call by name has been implemented by the use of various functions such as the two constant functions \texttt{suspend} and \texttt{force}.
\texttt{susExpensiveOp} ensures that the forcing (evaluation) of \texttt{x} never occurs until the caller of \texttt{double} forces the result.
By the aforementioned semantics of call by name in the context of the program in \autoref{lst:callbyname} \texttt{a} is never forced thus the computation is never performed.
The implementation of call by name can become quite troublesome and therefore in most cases is a part of the native execution environment which will be discussed in \autoref{tbd}.

The call by need strategy introduces \textit{lazy evaluation} semantics which is the same as call by name with one extra detail named \textit{sharing}.
In \autoref{lst:callbyname} \texttt{force x} is computed twice which may be an expensive operation.
Under call by need all results are saved for later use similar to techniques such as dynamic programming.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.3cm and 0.3cm]
                \node[circle, draw=black] (app1) {\texttt{app}};
                    \node[circle, draw=black, below right = of app1] (force) {\texttt{force}};
                    \path[-] (app1) edge node[left] {} (force);

                    \node[circle, draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[circle, draw=black, below left = of app2] (value) {\texttt{value}};
                        \path[-] (app2) edge node[left] {} (value);
                        \node[circle, draw=black, below right = of app2] (double) {\texttt{double}};
                        \path[-] (app2) edge node[left] {} (double);

                \node[draw,scale=1.3,fill=none,rectangle,fit=(app1)(value)(force)](fstB){};
        \end{tikzpicture}
        \caption{The last expression of the program.}
        \label{sub:eval:main}
    \end{subfigure}
    \begin{subfigure}[b]{0.66\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.3cm and 0.3cm]
                \node[circle, draw=black] (lamx) {\texttt{$\lambda$x}};
                    \node[circle, draw=black, below = of lamx] (lamu) {\texttt{$\lambda$unit}};
                    \path[-] (lamx) edge node[left] {} (lamu);
                        \node[circle, draw=black, below = of lamu] (app1) {\texttt{app}};
                        \path[-] (lamu) edge node[left] {} (app1);
                            \node[circle, draw=red, below right = of app1] (app2) {\texttt{app}};
                            \path[-] (app1) edge node[left] {} (app2);
                                \node[circle, draw=red, below right = of app2] (x1) {\texttt{x}};
                                \path[-] (app2) edge node[left] {} (x1);
                                \node[circle, draw=red, below = of app2] (force1) {\texttt{force}};
                                \path[-] (app2) edge node[left] {} (force1);

                            \node[circle, draw=black, below left = of app1] (app4) {\texttt{app}};
                            \path[-] (app1) edge node[left] {} (app4);
                                \node[circle, draw=black, below left = of app4] (add) {\texttt{+}};
                                \path[-] (app4) edge node[left] {} (add);

                                \node[circle, draw=red, below right = of app4] (app3) {\texttt{app}};
                                \path[-] (app4) edge node[left] {} (app3);
                                    \node[circle, draw=red, below = of app3] (x2) {\texttt{x}};
                                    \path[-] (app3) edge node[left] {} (x2);
                                    \node[circle, draw=red, below left = of app3] (force2) {\texttt{force}};
                                    \path[-] (app3) edge node[left] {} (force2);
                \node[draw,scale=1.3,fill=none,rectangle,fit=(lamx)(force2)(x1)(add)](fstB){};
        \end{tikzpicture}
    %\begin{tikzpicture}
        %\node[circle, draw=black] (lamx) {$\lambda \texttt{x}$};

        %\node[circle, draw=black, below = of lamx] (lamunit) {$\lambda \texttt{unit}$};

        %\node[circle, draw=black, below = of lamunit] (addition) {$+$};

        %\node[circle, draw=black, below left = of addition] (force1) {\texttt{force}};
        %\node[circle, draw=black, below = of force1] (x1) {\texttt{x}};
        
        %\node[circle, draw=black, below right = of addition] (force2) {\texttt{force}};
        %\node[circle, draw=black, below = of force2] (x2) {\texttt{x}};

        %\path[->] (lamx) edge node[left] {} (lamunit);
        %\path[->] (lamunit) edge node[left] {} (addition);

        %\path[->] (addition) edge node[left] {} (force1);
        %\path[->] (addition) edge node[left] {} (force2);

        %\path[->] (force1) edge node[left] {} (x1);
        %\path[->] (force2) edge node[left] {} (x2);
    %\end{tikzpicture}
        \caption{The expression tree for \texttt{double}}
        \label{sub:eval:double}
    \end{subfigure}
    \caption{}
    \label{fig:evalexpr}
\end{figure}
To understand this better observe the expression tree for \autoref{lst:callbyname} in \autoref{fig:evalexpr}.
Clearly the two red subtrees in \autoref{sub:eval:double} are identical thus they may be memoized such that the forcing of \texttt{x} only occurs once.
More generally if the execution environment supports lazy evaluation, once an expression has been forced it is remembered.

\section{Runtime environments}
Now that the untyped lambda calculus has been introduced, implemented and validated efficiently the question of execution naturally follows.
There exists many different well understood strategies to implement an execution environment for the untyped lambda calculus.
Naively it may seem straightforward to evaluate the untyped lambda calculus mechanically by $\beta$-reductions, but doing so brings upon some problems when implementing an interpreter.
%When applying the term $f x$ in $\lambda f . \lambda x . f x$ such that $f[? := x]$ where $?$ is the parameter name of $f$ it should become clear why a naive strategy is not enough since the parameters of $f$ are anonymous.

\subsection{Combinator reducers}
\label{sec:comb}
One of the most prominent techniques for evaluating functional programs is that of \textit{combinator graphs reductions}.
Formally a combinator is a function that has no free variables which is convenient since the problem of figuring out closures and parameter substitutions in applications never arises.
\begin{align}
    x \label{eq:comb:x}\\
    F \label{eq:comb:F}\\
    Y E \label{eq:comb:app}
\end{align}
There are three types of terms in combinator logic; the variable much like the lambda calculus (\autoref{eq:comb:x}), application (\autoref{eq:comb:app}) and the combinator (\autoref{eq:comb:F}).
The SKI calculus is a very simple set of combinators which are powerful enough to be turing complete and translate to and from the lambda calculus.
In SKI $F ::= S \,\,|\,\, K \,\,|\,\, I$ where the equivalent lambda calculus combinators for $S = \lambda x . \lambda y . \lambda z . x z (y z)$, $K = \lambda x . \lambda y . x$ and $I = \lambda x . x$.
Evaluating an SKI program is a straightforward reduction where $F'_F$ denotes combinator $F'$ has been partially applied with combinator $F$.
\begin{exmp}
    \begin{align}
        &SKSI\\
        = &KI(SI)\tag*{}\\
        = &K_I(SI)\tag*{}\\
        = &I\tag*{}
    \end{align}
\end{exmp}

The algorithm for converting a lambda calculus program into a SKI combinator program is a straightforward mechanical one.
The evaluation context is always an abstraction $\lambda x . E$.
\begin{pcases}
    \pcase{$E = x$ then rewrite $\lambda x . E$ to $I$.}
    \pcase{$E = y$ where $y \neq x$ and $y$ is a variable then rewrite $\lambda x . y$ to $Ky$.}
    \pcase{$E = Y E'$ then rewrite $\lambda x . Y E'$ to $S(\lambda x . Y)(\lambda x . E')$ since applying some $y$ to $\lambda x . Y E'$ must lambda lift $y$ as a parameter named $x$ to both $Y$ and $E'$ such that the lifted expression becomes $((\lambda x . Y)y)((\lambda x . E')y) = S(\lambda x . Y)(\lambda x . E')y$.
    Then recurse in both branches.
    }
    \pcase{$E = \lambda x . E'$ then first rewrite $E'$ with the appropriate cases recursively such that $E'$ becomes either $x$, $y$ or $Y E$ such that Case 1, 2 or 3 can be applied.}
\end{pcases}
The termination of the rewriting to SKI is guaranteed since abstractions are always eliminated and the algorithm never introduce any additional abstractions.
When translating the untyped lambda calculus to SKI the ''magic`` variable names $\sigma, \kappa$ and $\iota$ are used as placeholder functions for the SKI combinators since the translation requires a lambda calculus form.
When the translation has been completed then replace $\sigma \mapsto S, \kappa \mapsto K, \iota \mapsto I$.
\subsection{Combinator translation growth}
Before proving that the SKI translation algorithm produces a program of larger size the notion of size must be established.
Size in terms of lambda calculus are the amount of lambda terms (\autoref{lc:lang:abs}, \autoref{lc:lang:var} and \autoref{lc:lang:app}) that make up a program.
For instance $\lambda x . x$ has a size of two since it is composed of an abstraction and a variable term.
The size of an SKI combinator program is in terms of the number of combinators.
\begin{lemma}
    There exists a family of lambda calculus programs of size $n$ which are translated into SKI-expressions of size $\Omega(n^2)$.
\end{lemma}
\begin{proof}
\begin{pcases}
    \pcase{\label{eval:case:1} Rewriting $\lambda x . x$ to $I$ is a reduction of one.}
    \pcase{\label{eval:case:2} Rewriting $\lambda x . y$ to $Ky$ is equivalent in terms of size.}
    \pcase{Rewriting $\lambda x . YE$ to $S(\lambda x . Y)(\lambda x . E)$ is the interesting case.
        To induce the worst case size \autoref{eval:case:1} must be avoided.
        If $x \notin \textit{Free}(Y)$ and $x \notin \textit{Free}(E)$ then for every non-recursive term in $Y$ and $E$ \autoref{eval:case:2} is the only applicable rewrite rule which means that an at least equal size is guaranteed.
        Furthermore observe that by introducing unused parameters one can add one $K$ term to \textit{every} non-recursive case.
        Observe the instance $\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1)$ where the two unused parameters are used to add $K$ terms to all non-recursive cases in \autoref{eq:eval:red2} such that the amount of extra K terms minus the I becomes $\text{variable\_references} * (\text{unused\_abstractions} - 1) = 3*(3-1)$:
        \begin{gather}
            S(S(KKI)(KKI))(KKI)\label{eq:eval:red2}
        \end{gather}
        Now let the number of variable references be $n$ and the unused abstractions also be $n$ clearly $\Omega(n * (n - 1)) = \Omega(n^2)$
    }
    \pcase{Rewriting $\lambda x . E'$ is not a translation rule so the cost is based on what $E'$ becomes.}
\end{pcases}
    \vspace*{0.5cm}
Notice that the applications $f_1 f_1 \dots f_1$ can in fact be changed to $f_1 f_2 \dots f_n$ since for every $f_k$ where $0 < k \leq n$ there are $n - 1$ parameters that induce a K combinator.
Let $P_n$ be family of programs with $n$ abstractions and $n$ applications.
$\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1) \in P_3$ and in fact for any $p$ where $\forall n \in \mathbb{Z}^+$ and $p \in P_n$, $p$ translates into SKI-expressions of size $\Omega(n^2)$.
\end{proof}
\begin{exmp}
    Observe the size of \autoref{eq:eval:comp1} in comparison to \autoref{eq:evaltime}.
\begin{align}
    &\lambda f_1 . \lambda f_2 . f_1 f_2 \label{eq:eval:comp1}\\
    =&\lambda f_1 . \sigma(\lambda f_2 . f_1)(\lambda f_2 . f_2) \tag*{} \\
    =&\lambda f_1 . (\sigma(\kappa f_1))(\iota) \tag*{} \\
    =&\sigma (\lambda f_1 . \sigma (\kappa f_1)) (\lambda f_1 . \iota) \tag*{} \\
    =&\sigma (\sigma (\lambda f_1 . \sigma) (\lambda f_1 . \kappa f_1)) (\kappa \iota) \tag*{} \\
    =&\sigma (\sigma (\kappa \sigma) (\sigma (\lambda f_1 . \kappa) (\lambda f_1 . f_1))) (\kappa \iota) \tag*{} \\
    =&\sigma (\sigma (\kappa \sigma) (\sigma (\kappa \kappa) (\iota))) (\kappa \iota) \tag*{} \\
    =&S (S (K S) (S (K K) (I))) (K I) \tag*{}
\end{align}
\end{exmp}
It should become clear that many programs suffer from this consequence such as \texttt{let add = ($\lambda$x.$\lambda$y.(+ x) y)} $\in P_2$ where the program is written in prefix notation.
Translating the lambda calculus into the SKI-expressions does indeed increase the size significantly but does not warrant a write off entirely.
More advanced techniques exist to translate the lambda calculus to linearly sized SKI-expressions with the introduction of more complicated combinators~\cite{kiselyov2018lambda}.

\subsection{Reduction strategies}
Reductions in the context of the lambda calculus are a small set of well-defined rules for rewriting such that a program is evaluated.
The techniques required to correctly evaluate a program are a bit more complicated than the SKI calculus but are rewarding in flexibility and performance.
The substitution mapping set, denoted $S$, is a set of variable names to their value denoted $\{ x \mapsto \lambda y . y \}$ meaning ``the value of variable $x$ is $\lambda y . y$''.
%Substitutions mappings can be combined $S \cup \Sigma$, if a variable name occurs in both $S$ and $\Sigma$ then the variable in the rightmost mapping is chosen ($\Sigma$).
\begin{align}
  \{ x \mapsto y \} x = y\label{eq:subsem}\\
  \{ x \mapsto y \} z = z \tag*{}
\end{align}
Substitution mappings are used as in \autoref{eq:subsem}; if $Sx = y$ if $(x \mapsto y) \in S$ else $Sx = x$.
%In $(\lambda f . \lambda x . \lambda y. \texttt{ let } a = f x \texttt{ in } a + y)(\lambda y . y) \,\, 10 \,\, 20$ when evaluating $f x$ where the substitution are $[f \mapsto \lambda y . y, x \mapsto 10, y \mapsto 20]$ there must be taken great care that $x$ is mapped to $y$ in $\lambda y . y$.

\begin{remark}
Let expressions are considered as if abstractions and applications; \texttt{let a = b in c $\rightarrow$ ($\lambda$a.c)b}.
\end{remark}

Evaluation strategies (\autoref{sec:es}) are a core part of the reduction strategy since the choice of evaluation strategy changes the order in which terms are evaluated.
A \textit{redex} is a reducible expression in the context of some set of reduction rules.
The two interesting evaluation orders are \textit{applicative order} and \textit{normal order}~\cite{sestoft2002demonstrating}.
Applicative order specifies that the parameters of some application should always be evaluated first, e.g. call by value.
Normal order specifies that the leftmost outermost term should be evaluated first which yields the call by name strategy.
Before delving into more complicated evaluation strategies such as call by need, call by name will be considered.

A naive reduction strategy would involve adding variables to the substitution set, once they are applied.
When evaluating a term such as \texttt{($\lambda$x.x + 5) 5}, \texttt{x} must be substituted by \texttt{5} such that the expression becomes \texttt{x + 5} with the substitution mapping \texttt{\{x $\mapsto$ 5\}}.
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.49cm}
        \begin{subfigure}[b]{0.32\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{\texttt{($\lambda$x.e) $\rightarrow$ ($\lambda$x.e)}}
            \end{prooftree}   
            \caption{}
            \label{eq:simpleabs}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
            %\vspace*{0.4cm}
              \begin{prooftree}
                \AxiomC{\texttt{\{x $\mapsto$ z\} e $\rightarrow$ y}}
                  \RightLabel{App}
                  \UnaryInfC{\texttt{($\lambda$x.e) z $\rightarrow$ y}}
              \end{prooftree}   
          \caption{A simple application rule}
          \label{fig:simpleapp}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
            \vspace*{0.4cm}
            \begin{prooftree}
                \AxiomC{}
                %\AxiomC{$S \alpha x \not\equiv \alpha x$}
                \RightLabel{Var}
                \UnaryInfC{\texttt{x $\rightarrow$ y}}
            \end{prooftree}   
            \caption{}
            \label{eq:simplevar}
        \end{subfigure}
    \end{mdframed}
    \caption{Simple call by name lambda calculus}
    \label{fig:scbn}
\end{figure}
The rules in \autoref{fig:scbn} display a simple set of rules for evaluating the call by name lambda calculus.
The Abs and Var rules (\autoref{eq:simpleabs} and \autoref{eq:simplevar}) are rules which act as leaves within the evaluation of a program.
Abs and Var both state that if either of them occur then simply return the expression.
The App rule (\autoref{fig:simpleapp}) states that ``Evaluate \texttt{e} to \texttt{y}, where \texttt{x} has been replaced with \texttt{z} in \texttt{e}''.
We must introduce rules for how substitutions should act upon encountering lambda calculus term types (\autoref{eq:subsem}).
\begin{align}
  &\texttt{\{x $\mapsto$ y\} x = y} \label{eq:subsem}\\
  &\texttt{\{x $\mapsto$ y\} z = z}  \tag*{}\\
  &\texttt{S f z = (Sf)(Sz)} \tag*{}\\
  &\texttt{S ($\lambda$ x.e) = ($\lambda$ x.Se) } \label{eq:subsemrem}%\\
\end{align}

\begin{lstlisting}[language=ML,caption={Program with variable ambiguity},label={lst:varambg},mathescape=true]
fun f x =
  fun g x = x;
  g (x + x);
f 2;
\end{lstlisting}
Evaluating \autoref{lst:varambg} under the rules in \autoref{eq:subsem} yields a case for more thorough rules.
Using the App rule (\autoref{fig:simpleapp}) and applying substitutions as defined in (\autoref{eq:subsem}), yields the program in \autoref{lst:varambg2}.
\begin{lstlisting}[language=ML,caption={Program with variable ambiguity after one reduction pass},label={lst:varambg2},mathescape=true]
fun g x = 2;
g (2 + 2);
\end{lstlisting}
Which eventually evaluates to \texttt{2}, which clearly is not the intended result.
Removing the rule \autoref{eq:subsemrem} and adding the two rules in \autoref{eq:subsemfixed} solves variable ambiguity.
\begin{align}
  &\texttt{S($\lambda$x.e) = ($\lambda$x.Se)} & \texttt{(x $\mapsto$ y) $\notin$ S} \label{eq:subsemfixed}\\
  &\texttt{S($\lambda$x.e) = ($\lambda$x.(S$\backslash$\{x $\mapsto$ y\})e)} & \texttt{(x $\mapsto$ y) $\in$ S} \tag*{}
\end{align}
This evaluation model is indeed powerful enough to evaluate the call by name lambda calculus naively.

To further refine the call by name evaluation semantics observe that substitutions are performed eagerly, that is, they \textit{break suspension}.
To explore solutions to avoid eager substitutions, environments may be used.
\begin{remark}
  When considering an arbitrarily large expression under call by name (or need), there is no guarantee that the expression will ever be evaluated, that is, a suspended expression being forced.
  Breaking suspension violates the philosophy behind call by name (and need).
  A solution could involve composing a function in compile time, which replaces all instances of a variable (by \autoref{eq:subsem} and \autoref{eq:subsemfixed}) with some parameterized value.
  Substituting expressions eagerly might compromise performance guarantees, even if substitution is an asymptotically constant number of operations.
  Consider substituting an expression which contains conditional branches, one branch might be a single expression and the other might be a very large expression, consisting of many instances of the same variable.
  If the program never picks the large expression, why should it ever have any impact on performance.
  For instance, in \autoref{lst:fastslow} under call by name one would expect \texttt{fastMap} to evaluate in roughly the same time as \texttt{slowMap}.
\begin{lstlisting}[language=ML,caption={Performance considerations in substitution},label={lst:fastslow},mathescape=true]
fun fastMap f l =
  match l
    | Nil -> Nil;
    | Cons x xs -> Cons (f x) (map f xs);
  ;
fun slowMap f l =
  match l
    | Nil -> Nil;
    | Cons x xs -> 
      if (x == 0)
        let v = x + x + x + x + x + x; 
        Cons (f x) (map f xs);
      else
        Cons (f x) (map f xs);
      ;
  ;
\end{lstlisting}
\end{remark}
\subsubsection{Environments}
Environments, like environments in typing (\autoref{eq:env}), define what ``state'' is required to evaluate some expression.
In the case of reduction strategies, the environment is always a substitution mapping.
Environments are a requirement for some call by need semantics and lazy substitution.
A simple modification to the rules in \autoref{fig:rules:env}, implies an interpreter which brings an environment along when interpreting. 
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.49cm}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{\texttt{S, ($\lambda$x.e) $\rightarrow$ S, ($\lambda$x.e)}}
            \end{prooftree}   
            \caption{}
          \label{fig:rules:env:abs}
        \end{subfigure}
        \begin{subfigure}[b]{0.58\textwidth}
          \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{S $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          \end{prooftree}   
          \caption{}
          \label{fig:rules:env:app}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Var}
                \UnaryInfC{\texttt{S, x $\rightarrow$ S, Sx}}
            \end{prooftree}   
            \caption{}
          \label{fig:rules:env:var}
        \end{subfigure}
    \end{mdframed}
    \caption{Call by name lambda calculus with environments}
    \label{fig:rules:env}
\end{figure}
The $\cup$ operator merges two substitution mappings, choosing the rightmost mapping on duplicates, such that \texttt{\{x $\mapsto$ y, z $\mapsto$ h\} $\cup$ \{x $\mapsto$ m\} $\rightarrow$ \{x $\mapsto$ m, z $\mapsto$ h\}}, such that the semantics of $\cup$ satisfy \autoref{eq:subsemfixed}.
%More generally, a simple rule for App could be defined as in \autoref{fig:simpleapp} which states ``Evaluate \texttt{($\lambda$x.e) z} with the substitution mapping \texttt{S} to \texttt{y} with the substitution mapping $\Sigma$ by first evaluating \texttt{e} with the substitution mapping \texttt{S} $\cup$ \texttt{\{x $\mapsto$ z\}} to \texttt{y} with the substitution mapping $\Sigma$''.

\begin{figure}[ht]
\begin{lstlisting}[language=ML,caption={Problematic program},label={lst:problemprog},mathescape=true]
fun f a = 
  fun g x = 
    let a = 20;
    a + x;
  (g a) + a;
f 10;
\end{lstlisting}
\end{figure}
Evaluating \autoref{lst:problemprog}, using the rules proposed in \autoref{fig:rules:env}, yields an interesting case of variable ambiguity.
At some point the evaluation machine will reach a state with the expression \texttt{(g a) + a} and substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ ($\lambda$x.let a = 20 in (a + x))\}}.
By the laws of addition, that is, commutativity, it should be possible pick either of the two expressions to evaluate first.
If any one of the branches in the addition operator are side effectful, it may become a problem, but this language only deals with pure programs.
Evaluating the left side of \texttt{(g a) + a} first, yields \texttt{30 + a} with the substitution mapping \texttt{\{a $\mapsto$ 20, g $\mapsto$ $\dots$\}}, and finally \texttt{50}.
Evaluating the right side of \texttt{(g a) + a} first, yields \texttt{(g a) + 10} with the substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ $\dots$\}}, and finally \texttt{40}.
Clearly the order of evaluation is of importance for the result, which is often not the desired semantics.
If there exists multiple reduction techniques which ensure the same result, the system is called \textit{confluent}.
The reduction rules for the lambda calculus are confluent~\cite{church1936some}, which implies that the current set of rules are not valid evaluation rules for the lambda calculus.
%If reduction strategies for the lambda calculus are not confluent, programs become difficult to reason with.
If the rules were to be confluent, any reduction order would result in \texttt{40}.
The different results are an effect of leaking the substitution mapping to the outside of the lexical scope in which it was used.
A simple solution could involve saving the substitution mapping once the machine enters an abstraction, then restore the same substitution mapping when it leaves.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{S $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ S, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which does not leak}
    \label{fig:cleanapp}
\end{figure}
\noindent \autoref{fig:cleanapp} is a slightly modified version of \autoref{fig:rules:env:app}, with the difference of requiring the resulting substitution mapping to be exactly the initial substitution mapping.

\autoref{fig:cleanapp} does not handle closure correctly.
Evaluating \autoref{lst:closprob} under the rules defined in \autoref{fig:cleanapp} yields yet another problem.
\begin{figure}[ht]
\begin{lstlisting}[language=ML,caption={Program with closure},label={lst:closprob},mathescape=true]
fun g x =
  fun f z = x + z; // closes x
  fun run y =
    let x = 20; $\label{closov}$
    f y;
  run 10;
g 3;
\end{lstlisting}
\end{figure}
When line \autoref{closov} is reached in \autoref{lst:closprob}, then \texttt{x} in the substitution mapping is overwritten by the rules in \autoref{fig:cleanapp}.
Even if renaming is performed such that all variables become unique, recursive functions still remain a problem.
Taking a step back and considering what the evaluation strategy which involves eager substitution implies.
The \textit{first} substitution mapping which reaches an expression, is the substitution mapping which is relevant for that expression.
By these semantics substitution mappings must be paired with expressions and should now be read as ``\texttt{S, e} are the pair of the environment \texttt{S} necessary to evaluate the expression \textit{e}''.
Substitutions bound to expressions should remain immutable; whenever an expression is discovered during interpretation, the substitution at that time is the only important substitution mapping.
The $\cup$ operator for two substitutions should now be inverted, that is, \texttt{S $\cup$ $\Sigma$} should now pick substitutions from \texttt{S} if duplicates occur.
Furthermore, notice that binding substitutions to expressions also allows substitutions to float up throughout interpretation.
Now the application rule must implement \autoref{eq:subsemfixed}, seen in \autoref{fig:subsapp}.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{\{x $\mapsto$ S, z\} $\cup$ S, e $\rightarrow$ $\Sigma$, y}}
              \RightLabel{App}
              \UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which implements \autoref{eq:subsemfixed} and paired substitutions.}
    \label{fig:subsapp}
\end{figure}
\begin{remark}
  An interesting observation is that these semantics introduce expressions with closures as a construct similar to classes in object oriented languages.
  \texttt{e} is an abstract representation of something, while the environment \texttt{S} is the necessary values to instantiate \texttt{e}, together they can produce some output.
  Curried functions can also be seen this way, yet it remains an interesting observation.
\end{remark}

Surely \autoref{fig:subsapp} solves the problem of lazy substitutions, but alas this is not yet the case.
To reach a set of rules which will guarantee lazy substitutions and allow call by need, an additional ingredient is needed.
To understand the need for this ingredient, consider the following valid state \texttt{\{x $\mapsto$ z, y $\mapsto$ x\} ($\lambda$x.y) g} in the context of \autoref{fig:subsapp}.
When visiting \texttt{($\lambda$x.y)} then the following substitution mapping is applied \texttt{\{x $\mapsto$ g\}}.
Once \texttt{y} is visited the substitution mapping will be \texttt{\{x $\mapsto$ g, y $\mapsto$ x\}}, substituting again yields \texttt{x} which becomes \texttt{g} which does not necessarily satisfy \texttt{g $\equiv$ z}.
One could rename all variables in some phase in compilation, but the case for recursion is not handled since a duplicate variable name can occur.
Consider the program in \autoref{lst:amgprog} which yields an invalid state during interpretation.
\begin{figure}[ht]
\begin{lstlisting}[language=ML,caption={Recursive program with ambiguity},label={lst:amgprog},mathescape=true]
fun f x = 
  f (x + x); $\label{lst:amgprog:l2}$
f 10;
\end{lstlisting}
\end{figure}
Once Line \autoref{lst:amgprog:l2} is reached the first time, the program will have the state \texttt{\{$\dots$, x $\mapsto$ 10\}}.
On the second iteration, the program will have the state \texttt{\{$\dots$, x $\mapsto$ (x + x)\}}, which does not make sense.

%, thus the expression is equivalent to \texttt{($\lambda$x.x) g}, which is not necessarily correct in the case that \texttt{z $\neq$ g}.

%\autoref{fig:cleanapp} does not handle recursion that well, under normal order evaluations, which becomes apparent in expressions such as \texttt{let f = ($\lambda$f.$\lambda$x.if (x == 0) 0 (x + (f (x - 1))) in f f n)}.
%Once the base case is reached (\texttt{x == 0}), then some expression has accumulated in the substitution set \texttt{\{x $\mapsto$ x + ((x - 1) + $\dots$ + ((x - 1) - 1 $\dots$))\}}, which is clearly not valid.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{S $\cup$ \{x $\mapsto$ Sz\}, e $\rightarrow$ $\Sigma$, y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ S, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which works for recursion}
    %\label{fig:recapp}
%\end{figure}
%\noindent The problem of recursion can be solved by never introducing variables, but instead always substituting them thus mapping to the value (\autoref{fig:recapp}).
%An unfortunate consideration which yields \autoref{fig:recapp} unsatisfactory, under the current semantics, is that \texttt{z} can also be bound to an expression which is not a variable, like an application for instance (parameters are never evaluated first in normal order).
%One could recursively apply substitutions to \texttt{z}, but then the \textit{suspense} would be broken.
%\begin{remark}
  %When considering an arbitrarily large expression under call by name (or need), there is no guarantee that the expression will ever be evaluated, that is, a suspended expression being evaluated.
  %Therefore substituting expressions eagerly might compromise performance guarantees.
  %Furthermore consider substituting an expression which contains conditional branches, one branch might be a single expression and the other might be a very large expression.
  %If the program never picks the large expression, it should never have any impact on performance.
%\end{remark}
%\noindent For this method to work then substitutions must be applied lazily.
%The notion of substitutions should now be changed such that substitutions are performed recursively and lazily (\autoref{eq:subsemlaz}).
%\begin{align}
  %&\{ x \mapsto y \} x = y \label{eq:subsemlaz}\\
  %&\{ x \mapsto y \} z = z  \tag*{}\\
  %&S f z = (Sf)(Sz) \tag*{}\\
  %&S (\lambda x.e) = (\lambda x.Se) \label{eq:subsemlaz2} %& (x \mapsto y) \notin S \tag*{}%\\
  %%&S (\lambda x.e) = (\lambda x.(S \backslash \{x \mapsto y\})e) & (x \mapsto y) \in S \tag*{}%\label{eq:subsemlaz2}
%\end{align}
%Note that previously the substitution mappings travelled along with the interpreter while interpreting, but now substitution mappings should travel along variables.
%Expressions should now be modelled by a pair \texttt{S,e}, where \texttt{S} is a substitution mapping which should be considered once \texttt{e} is evaluated. 
%If a substitution mapping \texttt{S} is applied and then a substitution mapping $\Sigma$ is applied, then the resulting set for an expression should become \texttt{S $\cup$ $\Sigma$}.
%The order in which substitution mappings are combined is of importance since the ``most recent'' substitution mapping should contain the lexically ``closest'' variables.
%\begin{remark}
  %The union of two substitution mappings is important since applying them sequentially could damage the asymptotic complexity of substitution.
  %Clearly the variables which are lexically ``closer'' must also be respected. % but this is not of significant importance since $\alpha$-conversion will become necessary later.
  %If $n$ substitutions \texttt{S$_1 \dots$ S$_n$} are applied efficiently ($O(1)$ lookup), then the total number of operations will be $n$, whereas if the substitutions were unioned as they were created the work would be constant.
  %An interpreter would not be of much value if a recursive function which recurses $n$ times takes $O(n^2)$ time, since $O(1 + 2  \dots + n) = O(n \frac{n + 1}{2}) = O(n^2)$.
%\end{remark}
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{\{x $\mapsto$ $\Sigma$,z\} e $\rightarrow$ $\Theta$,y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S,($\lambda$x.e) z $\rightarrow$ $\Theta$,y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which has abstracted away substitution mappings}
    %\label{fig:recappsimple}
%\end{figure}
%%\autoref{eq:subsemlaz2} has been introduced for completeness, but can be omitted since it cannot occur in the lazy substitution model, since the only method which carries the substitution mapping into an abstraction is an application.
%Notice that any variables introduced must be so through applications.
%All the substitution work can now be moved into the introduced variable (\autoref{fig:recappsimple}).

%Functions can store intermediate values in two ways, either by partial application or closure.
%When partially applying functions, the intermediate values (and their respective variable bindings) must be saved in a safe and isolated way.
%Surely a system which uses \autoref{fig:recappsimple} is confluent, but, alas this is not the case.
%\texttt{let f = ($\lambda$x.$\lambda$y.x + y) 0 in E} shows a partially applied function \texttt{f} which should have \texttt{x} binded to the expression \texttt{0} in such a way that applying another value to \texttt{f} will finalize the value.
%Letting \texttt{E} satisfy \texttt{x $\notin$ Free(E)} while \texttt{x} occurs in \texttt{E}, that is, \texttt{x} is bound in \texttt{E}, provides an interesting problem for interpreters under call by value.
%If we let \texttt{f} be eager, that is, the binding of \texttt{x} to the expression \texttt{0} is not suspended, then \texttt{f} must associate itself with the substitution \texttt{\{x $\mapsto$ 0\}} and \textit{never} let any other substitutions override the variable \texttt{x}.
%\begin{remark}
  %Ad-hoc eager evaluation, that is, forcing suspended evaluations by use of some symbol is often allowed in functional programming languages.
  %Explicit forcing of suspended computations is a technique often used when creating data structures with good worst case performance.
%\end{remark}
%\noindent Now let \texttt{E} be \texttt{($\lambda$x.let g = f in g) 10} such that an evaluation yields \autoref{eq:red}.
%\begin{align}
  %&\texttt{S = \{x $\mapsto$ 10, f $\mapsto$ \{x $\mapsto$ 0\}($\lambda$y.x + y)\}} \tag*{}\\
  %&\texttt{S(let g = f in g)} \label{eq:red}\\
  %&\texttt{S($\lambda$g.g) S(f)} \tag*{}\\
  %&\texttt{($\lambda$g.S(g)) \{x $\mapsto$ 0\}($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{(\{x $\mapsto$ 0\} $\cup$ S)($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{S($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{\{$\dots$, x $\mapsto$ 10\}($\lambda$y.x + y)} \tag*{}
%\end{align}
%\autoref{eq:red} is a textbook closure and partial application problem in disguise.
%Consider the following valid state \texttt{\{x $\mapsto$ z, y $\mapsto$ x\} ($\lambda$x.y) g} in the context of \autoref{fig:cleanapp}.
%When visiting \texttt{($\lambda$x.y)} then the following substitution mapping is applied \texttt{\{x $\mapsto$ g\}}.
%Once \texttt{y} is visited the substitution mapping will be \texttt{\{x $\mapsto$ g, y $\mapsto$ x\}}, thus the expression is equivalent to \texttt{($\lambda$x.x) g}, which is not necessarily correct in the case that \texttt{z $\neq$ g}.
%Both problems are an effect of variable ambiguity.

%We almost have all the building blocks in place now.
%Temporarily assuming that substitutions occur eagerly yields interesting results which will help finalize the model.
%Let \texttt{((($\lambda$x.$\lambda$y.$\lambda$x.x) 1) 2) 3} be an expression which is to be evaluated.
%If we follow the rules in \autoref{eq:subsemlaz} then the first substitution will yield.
%\[
  %\texttt{\{x $\mapsto$ 3\}(($\lambda$y.$\lambda$x.x) 1) 2} \rightarrow \texttt{(($\lambda$y.$\lambda$x.3) 1) 2}
%\]
%Adding a rule which allows only the innermost substitution to be captured yields the two following rules (\autoref{eq:subsemfixed2}) instead of \autoref{eq:subsemlaz2}.
%\begin{align}
  %&S (\lambda x.e) = (\lambda x.Se) \label{eq:subsemfixed2} & (x \mapsto y) \notin S \tag*{}\\
  %&S (\lambda x.e) = (\lambda x.(S \backslash \{x \mapsto y\})e) & (x \mapsto y) \in S \tag*{}%\label{eq:subsemlaz2}
%\end{align}


%\autoref{fig:recappsimple} solves the problem of not letting substituted to values have their meaning changed, by pairing expressions with their entire substitution mapping.

%\autoref{fig:recappsimple} induces rather complicated programs to reason with since substitution mapping can occur within substitution mappings.
%Attempting to extend \autoref{fig:recappsimple} further, to solve variable ambiguity, sacrifices algorithmic elegance and simplicity further.
Taking a step back and considering what closures and partial application need to behave deterministically is rooted in solving variable ambiguity.
Instead, if all variables are unique, ambiguity cannot occur.
By renaming all variables to a unique name when they are introduced, the evaluation model can be simplified significantly and allows \textit{any} of the aforementioned application rules (some of which imply eager substitution).
The operation of renaming is called an $\alpha$-conversion.
%The shortcomings of \autoref{fig:simpleapp} become more apparent once one begins to consider more exotic evaluation strategies, namely call by need.
%Looking at call by need from a philosophical perspective, indicates that reduction rules must support returning information about newly computed variable values ``up'' through the evaluation tree.
%Furthermore when returning these modified variables up through the program tree, all variables which point to the same expression must also be updated.
%\\
%The next step in correctly evaluating the lambda calculus is applying \textit{$\alpha$-conversions} which is the operation of renaming.
\subsubsection{$\alpha$-conversions}
The $\alpha$-conversion mapping can occur as the substitution mapping, that is \texttt{\{x $\mapsto$ $\gamma_1$, y $\mapsto$ $\gamma_2$\}}; $\alpha$-conversions are philosophical constructs more than materialized mappings.
$\alpha$-conversions guarantee what is called \textit{$\alpha$-equivalence} which is the notion of semantic equivalence.
For instance \texttt{$\lambda$x.x} is $\alpha$-equivalent with \texttt{$\lambda\gamma.\gamma$} since both expressions are semantically equivalent.
%$\alpha$-conversions solve some critical problems such as closures and recursion when evaluating the lambda calculus.
%$\alpha$-conversions should also be non-destructive but still be context aware such that when leaving an abstraction the remaining substitution mapping is a superset of the substitution mapping when entering.
%More formally if \texttt{S, e $\rightarrow$ $\Sigma$, e'} then $\{x \,\,|\,\, (x \mapsto y) \in S\} \subseteq \{x \,\,|\,\, (x \mapsto y) \in \Sigma\}$.
\begin{lstlisting}[language=ML,caption={Recursive addition function},label={lst:recalpha},mathescape=true]
let f = ($\lambda$f'.$\lambda$x.
    if (x = 0) x else f' f' ((x - 1) + (x - 1))) in
f f 10
\end{lstlisting}
%A strong guarantee that can be made by tuning the evaluation strategy which is particularly useful for $\alpha$-conversion algorithms is that \textit{any} returned value has had \textit{every} term that it contains visited.
%An algorithm for performing $\alpha$-conversions can be implemented that picks a new variable name from any infinite domain when an abstraction has had a value applied to it and replaces future encountered variables with the new one, such an algorithm only works if the guarantee of visiting every term is made.
%The algorithm should also introduce the applied value to the substitution set through the alpha converted name.
\noindent An $\alpha$-conversion algorithm can be implemented such that when a new variable is introduced through an abstraction, a new name for the variable is given.
More formally; Let $V_1$ be the domain of variables in the program and $V_2$ be the infinite domain for variable names that satisfies $V_1 \cap V_2 = \emptyset$, then when a new variable \texttt{x} is discovered, replace it with some $\gamma \in V_2$ and let $V_2 = V_2 \backslash \{\gamma\}$. 
For instance, let \texttt{($\lambda\gamma_1$.$\lambda\gamma_2$.if ($\gamma_2$ = 0) $\gamma_2$ else $\gamma_1$ $\gamma_1$ (($\gamma_2$ - 1) + ($\gamma_2$ - 1)))} be the $\alpha$-converted version of \texttt{f} in \autoref{lst:recalpha}.
%It should become clear that an $\alpha$-conversion algorithm must also follow the reduction order or else one can force terrible runtime in a call by name (or need) environment by creating purposeless terms which are never executed but are converted.
%$\alpha$-conversions must be suspended until it is needed; let $\alpha E$ denote the $\alpha$-conversion for some conversion mapping $\alpha$ on a lambda expression $E$ which lazily $\alpha$-converts $E$.
%The replacement rules for $\alpha$-conversions are the same as for substitutions.
%\begin{align}
  %&\texttt{[x $\mapsto$ y]x = y} &\label{eq:alpharep}\\
  %&\texttt{[x $\mapsto$ y]z = z} &\tag*{}\\
  %&\texttt{$\alpha$(f z) = ($\alpha$f)($\alpha$z)} &\tag*{}\\
  %&\texttt{$\alpha$($\lambda$x.e) = ($\lambda$x.$\alpha$e)} & \texttt{(x $\mapsto$ y) $\notin$ $\alpha$} \tag*{}\\
  %&\texttt{$\alpha$($\lambda$x.e) = ($\lambda$x.($\alpha\backslash$[x $\mapsto$ y])e)} & \texttt{(x $\mapsto$ y) $\in$ $\alpha$} \label{eq:alprepsub}
%\end{align}

We almost have all the building blocks in place now.
The problem of ambiguity between \textit{different} variables, that is, variables different places in the program with the same name and in recursion, can now be solved.
Once again, not breaking suspension remains as an interesting value.
In \autoref{fig:subrenam} the \texttt{fresh} function picks a new variable from $V_2$ and updates $V_2$ such that picked variable no longer exists in $V_2$.
\begin{figure}[ht]
    \begin{mdframed}[style=style1]
        \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{\{$\gamma$ $\mapsto$ (S, z), x $\mapsto$ $\gamma$\} $\cup$ S, e $\rightarrow$ $\Sigma$, y}}
            \AxiomC{$\gamma$ = \texttt{fresh}}
              \RightLabel{App}
              \BinaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          \end{prooftree}   
    \end{mdframed}
    \caption{An application rule which renames variables}
    \label{fig:subrenam}
\end{figure}

%First consider the implications of an eager $\alpha$-conversion strategy in \autoref{eq:alpharep}.
%Applying $\alpha$-conversions in applications eagerly ensures that all references to the applied to variable is are replaced (\autoref{fig:simpleappalpha}).
%Also notice that \autoref{eq:alprepsub} ensures that only variables that reference the replaced variable are renamed.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\gamma$ = fresh \,\,\,\,\, S $\cup$ \{$\gamma$ $\mapsto$ z\}, [x $\mapsto$ $\gamma$]e $\rightarrow$ $\Sigma$, y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{The simple application rule with $\alpha$-conversions}
    %\label{fig:simpleappalpha}
%\end{figure}
%The semantics of eager $\alpha$-conversion implies that once an $\alpha$-conversion is converting an expression, all free variables which the $\alpha$-conversion addresses should be renamed eagerly.
%Moreover, this implies that if some $\alpha$-conversion $\alpha_1$ suspends conversion of an expression and then some other conversion $\alpha_2$ suspends a conversion on the same expression, $\alpha_1$'s conversions should take precedence.
%That is, suspended conversions cannot have their conversions overwritten by other conversions such that \texttt{[x $\mapsto$ y] $\cup$ [x $\mapsto$ z, $\gamma$ $\mapsto$ k] = [x $\mapsto$ y, $\gamma$ $\mapsto$ k]}.

%\begin{lstlisting}[language=ML,caption={$\alpha$-conversion of a program with variable name collisions},label={lst:ambg},mathescape=true]
%($\lambda$x.$\lambda$y.$\lambda$x.x + y) $\rightarrow$ ($\lambda\gamma$.$\lambda\theta$.$\lambda\sigma$.$\sigma$ + $\theta$)
%\end{lstlisting}

%In \autoref{subfig:red4} the application which contains \texttt{$\lambda$f'} and \texttt{f'} as children (the red node) must $\alpha$-convert \texttt{f'} in ``this scope''.
%If \texttt{f'} is not $\alpha$-converted a circular dependency will arise $S = \{ \gamma \mapsto \texttt{f'} \}, \alpha = [ \texttt{f'} \mapsto \gamma ]$.

\begin{exmp}
  \autoref{fig:red:exmp} is an example of an iteration of \autoref{lst:recalpha}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=black] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=red, below left = of app2] (f1) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f1);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f2);

                \node[draw,scale=1.3,fill=none,rectangle,fit=(app1)(f1)(var1)](fstB){};
                \path (fstB.north east) -- (fstB.south east) coordinate[midway] (fstP);
        \end{tikzpicture}
        \caption{
          \texttt{S = \{$\gamma_1 \mapsto$ ($\lambda$f'.$\lambda$x.if (x = 0) x else f' f' (x - 1)), f $\mapsto \gamma_1$\}}
        }
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=black] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=red, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        \path[-] (app2) edge node[left] {} (lam1);
                            \node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            \path[-] (lam1) edge node[left] {} (lam2);
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                \path[-] (lam2) edge node[left] {} (app3);
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ((x - 1) + (x - 1))}};
                                    \path[-] (app3) edge node[left] {} (iff);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        \path[-] (app2) edge node[left] {} (f2);

                \node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](sndB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \texttt{$\Sigma$ = S}
            }
    \end{subfigure}
    %\tikz[overlay,remember picture]{\draw [->] (fstP) -- (fstP-|sndP);}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    \path[-] (app3) edge node[left] {} (iff);
                \node[draw,fill=none,scale=1.3,rectangle,fit=(app3)(if)(ifc)(iff)](fouB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \\
                \texttt{$\Theta_0$ = \{$\gamma_2 \mapsto$ ($\Sigma$, f), f' $\mapsto \gamma_2$\} $\cup$ $\Sigma$}\\
                \texttt{$\Theta$ = \{$\gamma_3 \mapsto$ ($\Theta_0$, 10), x $\mapsto \gamma_3$\} $\cup$ $\Theta_0$}
            }
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                \node[draw=blue] (app1) {\texttt{app}};
                    \node[draw=black, below right = of app1] (var1) {\texttt{x - 1}};
                    \path[-] (app1) edge node[left] {} (var1);

                    \node[draw=purple, below left = of app1] (app2) {\texttt{app}};
                    \path[-] (app1) edge node[left] {} (app2);
                        \node[draw=black, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        \path[-] (app2) edge node[left] {} (lam1);
                            \node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            \path[-] (lam1) edge node[left] {} (lam2);
                                \node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                \path[-] (lam2) edge node[left] {} (app3);
                                    \node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    \path[-] (app3) edge node[left] {} (app4);
                                        \node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        \path[-] (app4) edge node[left] {} (app5);
                                            \node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            \path[-] (app5) edge node[left] {} (if);
                                            \node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            \path[-] (app5) edge node[left] {} (ifc);
                                        \node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        \path[-] (app4) edge node[left] {} (ift);
                                    \node[draw=black, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    \path[-] (app3) edge node[left] {} (iff);
                        \node[draw=black, below right = of app2] (f2) {\texttt{f'}};
                        \path[-] (app2) edge node[left] {} (f2);
                \node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](fouB){};
                %\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            \end{tikzpicture}
            \caption{
                \texttt{$\Delta = \Theta$}\\
            }
            \label{subfig:red4}
    \end{subfigure}
    \caption{Example of substitutions in recursive function.}
    \label{fig:red:exmp}
\end{figure}
\end{exmp}
%\tikz[overlay,remember picture]{\draw[-latex,thick] (fstB) -- (fstB-|sndB) node[midway,below,text width=0.5cm]{};} 

Call by need is a bit more tricky since it requires more than a particular evaluation order to implement~\cite{levy1988sharing}.
Substitutions can be rewritten when a variable \texttt{x} maps to an application such that for some substitution mapping of the form $\{ \dots, \texttt{x} \mapsto Y E \}$ then the substituted to value is reduced.
When two substitutions are combined one should always choose the most reduced of the two, which is trivial since it becomes a matter of picking the ''newest`` version of the substituted value.
An important observation which is discussed in~\cite{levy1988sharing} is that of \textit{object} duplication.
Objects are the substituted to values which are lambda calculus terms.
The term object is used to emphasize the uniqueness of lambda calculus terms.
%A problem when sharing evaluation in the substitution mapping is that of transitive substitutions.
%Transitive substitutions are not expressible since that causes non-linear evaluation times on some inputs.
%Naturally when new mappings are introduced the mapped to values can be duplicated instead such that instead of $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto \gamma_1 \}$ then $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ which violates call by need.
A lambda calculus expression $E$ can be labelled with some name $a$ when found in the evaluation process such that when some value with the label $a$ is evaluated then all duplications of $E$ can share the computed result, effectively adding another type of mapping.
\begin{lstlisting}[language=ML,caption={Program which benifits from lazy evaluation},label={lst:lazyevalprog},mathescape=true]
fun double x = x + x;
let y = double 10;
y + y;
\end{lstlisting}
\autoref{lst:lazyevalprog} is a program which under call by name computes \texttt{y} two times.
By assigning a unique name to the expression of \texttt{y} to \texttt{a} for instance, the unique expression can be tracked.
When \texttt{y + y} is computed, the first evaluation of \texttt{y} changes \texttt{a} in the substitution mapping such that \texttt{\{$\dots$, a $\mapsto$ 20\}}.
Furthermore, notice that the phrase ``assigning a unique name'' is exactly what $\alpha$-conversion does.
By these rules sharing can be implemented very elegantly by also noticing that variable references are transitive.
Let a valid substitution mapping be \texttt{\{x $\mapsto$ 10 + 2, y $\mapsto$ (x + 1), z $\mapsto$ (y + 2)\}} with the originating program in \autoref{eq:lazyexmp}.
\begin{align}
  \texttt{($\lambda$x.($\lambda$y.($\lambda$z.E) (y + 2)) (x + 1)) (10 + 20)} \label{eq:lazyexmp}
\end{align}
If the lambda calculus sub-program in \texttt{E} forces \texttt{y}, then the substitution mapping should be updated with \texttt{\{y $\mapsto$ 13\}} which implies that x is also forced \texttt{\{x $\mapsto$ 12\}}.
\begin{lemma}
  Unique objects cannot be duplicated in the $\alpha$-conversion labelling semantics.
\end{lemma}
\begin{proof}
  
\end{proof}
\begin{remark}
    The system which implements call by need is not of much importance.
    Sharing can also be implemented in a way which promotes more imperative constructs, such as variables.
    When some object $E$ is discovered it is declared as a mutable pointer by the interpreter (or natively if implemented in a lazy language) such that all duplications effectively point to the same object.
\end{remark}

Let $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ be the system without labels, then let \autoref{eq:labelsystem} be the labelled call by need system.
\begin{align}
    \label{eq:labelsystem}
    S &= \{ \gamma_1 \mapsto a, \gamma_2 \mapsto a \}\\
    \Omega &= \{ a \mapsto E \} \tag*{}
\end{align}
By \autoref{eq:labelsystem} it should becomes clear that object duplication is mitigated by ``merging'' references.

$\alpha$-conversion loses some of it's purpose when a labelling approach is considered.
\begin{lstlisting}[language=ML,caption={Program},label={lst:labeltest},mathescape=true]
fun id a = a;
fun f g a =
  id (g a a);
\end{lstlisting}




%The labelled system can evaluate expressions in parallel by implementing locking (or cooperative yielding) on the label level (in \autoref{eq:labelsystem} evaluating $\gamma_1$ and $\gamma_2$ in paralell would lock $a$).
%The only scenario where mutual evaluation of an expression is possible, is when two other expressions can access (either through abstraction or closure) the same value.
%\\
An evaluation written $S, \Omega, \alpha, x \rightarrow \Theta, e$ means $x$ evaluates to $e$ with the resulting label mapping $\Theta$, under some substitution mapping $S$, label mapping $\Omega$ and $\alpha$-conversion mapping $\alpha$.
$Sx$ means that variable $x$ is replaced with whatever the substitution map maps $x$ to if $x \in S$ otherwise $Sx = x$, the same operation counts for $\alpha$.
$\Omega l \rightarrow k$ should be read as, the expression which $l$ points to should be evaluated and be bound to $k$.
In a call by value environment $\alpha x$ can eagerly convert since the value must be evaluated eagerly.
In a call by name (or need) environment $\alpha x$ should remain suspended until needed like discussed in this section.
%An implementation (\autoref{lst:cbn}) of the call by name reduction algorithm \autoref{fig:cbn} can easily be extended with call by need by the aforementioned modifications.
%\begin{remark}
    %In the implementation the \texttt{Map} constructor is the usual implementation of the Map data structure.
    %Also the implementation is generalized in that it does not describe operations such as addition or whether it is a call by need or name environment.
%\end{remark}
%\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:cbn},mathescape=true]
%type Three a b c =
    %| Triple a b c;
%type Term =
    %| Abstraction Int Term
    %| Var Int 
    %| Application Term Term
%;

%fun reduceApp S alpha e1 e2 =
    %match e1 ->
        %| Abstraction x e' ->
            %let newvar = newvar in
            %let newS = 
                %Map newvar (apply alpha e2) in
            %let S2 = union S newS in
            %let newA =
                %Map x newvar in
            %let alpha2 = union alpha newA in
            %reduce newS newA e1
        %| e' -> match (reduce S alpha e')
            %| Triple S' alpha' e'' ->
                %let Sn = union S S' in
                %let alphan = union alpha alpha' in
                %let en = Application e'' e2
                %reduce Sn alphan en

%fun reduce S alpha e =
    %match e 
        %| Abstraction x e' -> 
            %reduce S alpha e'
        %| Var x -> match (find x S) 
            %| Just e' -> Triple S alpha e'
            %| Nothing -> Triple S alpha x
        %| Application e1 e2 -> 
            %reduceApp S alpha e1 e2
%\end{lstlisting}
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.4cm}
        %\begin{subfigure}[b]{0.30\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S, \alpha, S \alpha x \rightarrow \Sigma, e$}
                %\RightLabel{Var}
                %\UnaryInfC{$S, \alpha, x \rightarrow \Sigma, e$}
            %\end{prooftree}   
            %\label{eq:prooftree:ident}
            %\caption{}
        %\end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{$S,\alpha,(\lambda x.e) \rightarrow S,\alpha,(\lambda x.e)$}
            \end{prooftree}   
            \label{eq:prooftree:abs}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S \cup \{ \gamma \mapsto e \},\alpha \cup \{ x \mapsto \gamma \} , e \rightarrow \Sigma, w$}
                %\AxiomC{$S \alpha x \not\equiv \alpha x$}
                \RightLabel{Var share (pointer)}
                \UnaryInfC{$S \cup \{  \gamma \mapsto e \}, \alpha \cup [ x \mapsto \gamma ], x \rightarrow \Sigma, e \hspace*{1cm} e := w$}
            \end{prooftree}   
            \label{eq:prooftree:point}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S,\alpha, y \rightarrow \Sigma, (\lambda x . e)$}
                \AxiomC{$\Sigma \cup \{ \gamma \mapsto \Sigma \alpha p \},\alpha \cup [ x \mapsto \gamma ],e \rightarrow \Delta, w \,\,\, \gamma = \texttt{newvar}$}
                \RightLabel{App}
                \BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Delta, w$}
            \end{prooftree}   
            \label{eq:prooftree:app}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \begin{prooftree}
                \AxiomC{$S,\alpha,y \rightarrow \Sigma, f$}
                \AxiomC{$f \not\equiv (\lambda x . e)$}
                \RightLabel{App (Reduce left first)}
                \BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Sigma, (f \,\, p)$}
            \end{prooftree}   
            \label{eq:prooftree:appleft}
            \caption{}
        \end{subfigure}
    \end{mdframed}
    \caption{Reduction rules for the call by need lambda calculus}
    \label{fig:cbn}
\end{figure}
\clearpage
\begin{exmp}
Consider the following program which duplicates an object.
\begin{lstlisting}[language=ML,caption={Object duplication},label={lst:objdup},mathescape=true]
let d = ($\lambda$f.$\lambda$x.f x x) in
d ($\lambda$z.$\lambda$y.z + y) (($\lambda$i.i) 0)
\end{lstlisting}
The evaluation of \autoref{lst:objdup} can be seen in \autoref{eq:objdup} (the underlying call by need implementation is not of interest).
\begin{align}
    \label{eq:objdup}
    &\texttt{g s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\\
    \rightarrow &(\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y})\texttt{ s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\tag*{}\\
    \rightarrow &(\lambda \texttt{y}.\texttt{z + y})\texttt{ s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto s \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}]\tag*{}\\
    \rightarrow &\texttt{c + q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    \rightarrow &\texttt{+ s q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    \rightarrow &\texttt{+ force(s) q }\{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ force(}(\lambda\texttt{i}.\texttt{i})0\texttt{) q } \{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ force(0) q }\{ \dots \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 q }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 s }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{+ 0 0 }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    \rightarrow &\texttt{0 }\{ \dots, s \mapsto 0 \}\tag*{}
\end{align}
\end{exmp}

\subsubsection{Garbage collection}
Garbage collection in purely function languages is a bit of a controversial topic since there exists no resources to release.
But adding mappings from lambda terms to variables without any subtractions will lead to an ever increasing substitution mapping size.
When performing reductions one might desire to garbage collect the substitution mapping once it contains useless substitutions.
Fortunately the task of collecting garbage is quite easy since the ``scope'' of a substitution is only relevant where the substituted variables has been introduced, which may only happen in the abstraction term.
When some algorithm with garbage collection evaluates some abstraction term $S,[\texttt{x} \mapsto \gamma]$,\texttt{$\lambda$x.E}, the algorithm should remove the introduced variable \texttt{x} by inspecting the $\alpha$-conversion mapping such that the substitution mapping becomes $S\backslash \{ \gamma \}$ and the $\alpha$-conversion alike.

\subsection{An invariant on infinite programs}
An important problem still remains which is that of infinite programs.
Imperative programming languages often solve this by introducing loops, whereas functional programming languages use recursion.
Recursion may be equally powerful in terms of expressiveness, but becomes a bit more tricky when cosidering interpreter details.
A prerequisite for an infinitely running program to exist in practice is that the program must not grow it's resource needs as it runs.

The distinction between recursive functions and loops in imperative programming languages is often what makes infinite programs expressible.
In a traditional imperative language, a function allocates a \textit{stack frame} and is explicitly parameterized, whereas a loop acts more like an anonymous closure which is always parameterized with itself (a function which is wrapped in a fixed point combinator, like the Y-combinator).
\begin{remark}
    A call stack is a stack of stack frames.
    A stack frame is a pointer to a function pointer.
    Stack frames are used to return execution to the previous function (the calling function).
    Every time a new function is called, the called-from function places a ``resume execution from here'' pointer onto the call stack.
\end{remark}
\noindent Imperative languages are also often evaluated under call by value which further simplifies implementation details.
Imperative loops (more interestingly, infinite loops) can safely release all static resources (variables bindings), which were allocated in the iteration, once an iteration has completed.
In traditional imperative languages recursive functions can only iterate a finite number of times, more specifically until the call stack is full.
\begin{figure}
\begin{lstlisting}[language=ML,caption={Program that implements two functions that fold a \texttt{List a} to a \texttt{b}},label={lst:listfoldboth},mathescape=true]
type List a = 
    | Nil
    | Cons a (List a)
;
fun add a b = a + b;

fun foldl f z l =
    match l
        | Nil -> z;
        | Cons x xs -> 
            foldl f (f x z) xs;
    ;

fun foldr f z l =
    match l
        | Nil -> z;
        | Cons x xs ->
            f x (foldr f z xs);
    ;
\end{lstlisting}
\end{figure}
%fun mapTail r f =
%//The type of mapTail
%//List a $\rightarrow$ (a $\rightarrow$ b) $\rightarrow$ (List b $\rightarrow$ List b) $\rightarrow$ List b
    %fun mapImpl r f b =
        %match r
            %| Nil -> b Nil;
            %| Cons x xs -> 
                %mapTail xs f (b (Cons (f x)));
        %;
    %match r
        %| Nil -> Nil;
        %| Cons x xs -> mapImpl xs f (Cons (f x));
    %;
%fun mapInf f l = 
    %match l
        %| Nil -> Nil;
        %| Cons x xs -> Cons (f x) (mapGrowing f xs);
    %;
%\end{lstlisting}
%\end{figure}

To really understand what happens in a lambda calculus interpreter, we must understand what happens in \autoref{lst:listfoldboth}.
\autoref{lst:listfoldboth} implements two variants of a \texttt{fold} function which accumulates a list of type \texttt{List a} to a \texttt{b}.
The two variants differ when considering evaluation strategy and \textit{tail call optimization}.
\begin{remark}
    Tail call optimization is an optimization which can be performed on programs with a particular structure.
    If the last expression is a function invocation, then the rewritten program does not grow.
    For instance the expression \texttt{let f = ($\lambda$g.$\lambda$x.g x) in $\dots$ f g' 0} is eventually rewritten to \texttt{g' 0}.
    If for instance the expression awaited a result like in \texttt{let f = ($\lambda$g.$\lambda$x.x + (g x)) in $\dots$ f g' 0}, then it would be rewritten to \texttt{x + (g' 0)}, increases the size of the program by \texttt{x +}, since the \texttt{+} operator requires both expressions to be evaluated.
    It should become clear that reduction strategies always imply tail call optimization, whenever possible.
\end{remark}
The first flavor of \texttt{fold}; \texttt{foldl}, implements \texttt{fold} such that the program expression tree does not grow throughout program interpretation, under a call by value environment.
The constraint on evaluation strategy is important for \texttt{foldl}, for reasons which will become clear once other evaluation strategies are discussed.
\begin{align}
    &\texttt{ foldl 0 add (Cons 1 (Cons 2 $\dots$ (Cons n Nil)))}  \\
    =&\texttt{ l z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ foldl f (f x z) xs $\{$ xs $\mapsto$ (Cons 2 (Cons 3 $\dots$ (Cons n Nil))), x $\mapsto$ 1, $\dots$ $\}$}  \tag*{} \\
    =&\texttt{ foldl add (add 1 0) (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    =&\texttt{ foldl add 1 (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    & \dots \tag*{}
\end{align}
Evaluating \texttt{foldl} on a list of size \texttt{n} with the addition function showcases how the program only grows by a constant number of terms.
\begin{remark}
    Note again that the list is always refereed to by reference; the list is not copied.
\end{remark}
%\begin{remark}
    %If the program is to remain within a constant amount of space throughout the evaluation of \texttt{foldl}, then garbage collection must support this choice.
    %Clearly a simple strategy 
%\end{remark}

%\\
%\begin{minipage}{\textwidth}
%\begin{lemma}
    %\texttt{mapTail} is tail recursive and requires an additional $O(n \cdot m)$ space when evaluated on a list of size $n$ with a function \texttt{f} produces values of size $m$.
%\end{lemma}
%\begin{proof}
    %Unpacking \texttt{Cons} and \texttt{Nil} as their underlying scott encoded functions is omitted for readability.
    %In this proof the list is of \texttt{Int} and \texttt{f} is the identity function.
    %For some $1 \leq k \leq n$ where $n$ is the size of the list, at the recursive invocation of \texttt{mapImpl}, the list is partitioned into the lower $1, \dots k - 1$ elements (\texttt{b (Cons x)}) and the higher $k, \dots n$ elements (\texttt{xs}).
    %\begin{align}
        %&\texttt{ mapTail (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) ($\lambda$x.x)}  \\
        %=&\texttt{ r Nil ($\lambda$x.$\lambda$xs.mapImpl xs ($\lambda$x.x) (Cons (f x)))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1)}  \eqtag{2$\dots$n} \label{eq:map:2n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (Cons 1 (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 3 (Cons 4 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2))}  \eqtag{3$\dots$n} \label{eq:map:3n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 (Cons 2 xs)) (Cons 3))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 4 (Cons 5 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2 (Cons 3)))}  \eqtag{4$\dots$n} \\
        %=&\dots\tag*{}\\
        %=&\texttt{ mapImpl (Cons k (Cons (k + 1) $\dots$ (Cons n Nil)))} \tag*{} \\
        %&\texttt{ ($\lambda$x.x) (Cons 1 ($\dots$ (Cons (k - 1))))}  \tag*{} 
    %\end{align}
    %The base case for the proof is once \texttt{mapImpl} is recursively invoked (\autoref{eq:map:2n}), size of the newly created list is $m \cdot 1$ (\texttt{f (Cons 1)}).
    %The first recursive iteration for \texttt{mapImpl} is the step between \autoref{eq:map:2n} and \autoref{eq:map:3n} such that the size becomes $m \cdot 2$ (\texttt{Cons (f 1) (Cons (f 2))}).
    %If we assume that the first $k$ recursive iterations have size $m \cdot k$, then the $k+1$'th iteration has size $m \cdot (k + 1)$ by \texttt{Cons (f 1) (Cons (f 2) $\dots$ (Cons (f (k + 1))))}.
    %Clearly whet $k = n$ then the newly created list is of size $m \cdot n$ and \texttt{xs} is empty, thus the function is.
%\end{proof}
%\end{minipage}

%When evaluating a term that results in a change to the substitution mapping the previous substitution mapping version is paired with it.
%Letting the substitution mapping be 
%$\{ \texttt{x} \mapsto (\lambda \texttt{x.g x}, \{ \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)) \}, \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)\}$
%the space complexity may seem to grow by $O(n^3)$ (\autoref{eq:evalruntimewrong}) but lifting the data structure to using pointers instead of copies reduces the size to $O(n)$.
%\begin{align}
    %\label{eq:evalruntimewrong}
    %& O \left( \sum_{k = 1}^n \sum_{i = 1}^{k} i \right) \\
    %= \,\,\, & O \left( \sum_{k = 1}^n \frac{k(k + 1)}{2}\right)\tag{Triangular}\\
    %= \,\,\,& O \left( \frac{1}{2} \left( \sum_{k = 1}^n k^2 + \sum_{k = 1} k \right) \right)\tag*{}\\
    %= \,\,\,& O \left( \sum_{k = 1}^n k^2 \right)\tag*{}\\
    %= \,\,\,& O \left( \frac{1}{6}n + \frac{1}{2}n^2 + \frac{1}{3}n^3 \right)\tag{Faulhaber}\\
    %= \,\,\,& O \left( n^3 \right)\tag*{}
%\end{align}
%Observe that the introduction of a context is based on free variables in closures and can in fact be avoided at the cost of compilation time by simply making all variables unique.
%Let \autoref{eq:eval:comp} be a program translated from \autoref{eq:eval:nocomp}.
%\begin{align}
    %&(\lambda x . \texttt{ let } f = \lambda y . \lambda x . x \texttt{ in } x) \label{eq:eval:nocomp}\\
    %&(\lambda x_1 . \texttt{ let } x_2 = \lambda x_3 . \lambda x_4 . x_4 \texttt{ in } x_1)\label{eq:eval:comp}
%\end{align}
%The method of storing substitutions warrants the question of evaluation strategy since eagerly replacing the mapped to expressions implies call by value (\autoref{eq:eval:eagereval}).
%If one were to keep the initially proposed semantics the result would be call by name (\autoref{eq:eval:name}).
%In the hybrid approach forcing a term rewrites the mapped to value (\autoref{eq:eval:need}).
%\begin{align}
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto 5, \texttt{y} \mapsto 5 \} \label{eq:eval:eagereval} \eqtag{Call by value}\\
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto \texttt{y}, \texttt{y} \mapsto 5 \} \label{eq:eval:name} \eqtag{Call by name}\\
    %&\textit{force}(\texttt{z}, \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto \texttt{f 0} \}) = \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto 0 \} \label{eq:eval:need} \eqtag{Call by need}
%\end{align}
%One may also have an interpreter that supports multiple strategies at once by simply specifying what type of strategy should be applied to every term.
%Evaluating a program is a matter of entering through some expression and continually reading terms and applying substitutions until said expression has been evaluated regardless of evaluation strategy. 

%The algorithm for evaluating the untyped lambda calculus is similar to Algorithm W (\autoref{fig:dmrules}) and can be seen in.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
        %\begin{subfigure}[b]{0.30\textwidth}
            %\begin{prooftree}
                %\AxiomC{$e$}
                %\RightLabel{Terminal}
                %\UnaryInfC{$\emptyset ,e$}
            %\end{prooftree}   
            %\label{eq:prooftree:terminal}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{0.69\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S,\lambda x . e_1$}
                %\AxiomC{$S \cdot \{ x \mapsto e_2 \},e_1 $}
                %\RightLabel{App}
                %\BinaryInfC{$S,e_3$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S \cdot \{ x \mapsto e_1 \},e_2$}
                %\RightLabel{Let}
                %\UnaryInfC{$S,\texttt{ let } x = e_1 \texttt{ in } e_2$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}

    %\end{mdframed}
    
%\end{figure}

%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%Beta reductions with De Bruijn indices~\cite{de1972lambda} is another more straightforward method of evaluating the untyped lambda calculus.
%De Bruijn indices is a representation of lambda calculus which deals with variables based on the scope ''distance`` instead of variable names.
%\begin{lstlisting}[language=ML,caption={Add as De Bruijn},label={lst:adddebru},mathescape=true]
%let add = ($\lambda$($\lambda$ 2 1))
%\end{lstlisting}
%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%More generally all variable occurrences are replaced with the distance to the abstraction which introduced them.
%The use of De Bruijn indices allow anonymous naming of parameters thus imply a method of solving the application problem when directly interpreting the untyped lambda calculus.
%A minor but important detail when 
%\begin{align}
    %&(\lambda x . ((\lambda f. \lambda x. f x)(\lambda z . z))x)''0`` \rightarrow\\
    %&
%\end{align}

\end{document}
