\documentclass[11pt,oneside,a4paper]{report}

\begin{document}

\section{Evaluation strategies}
When evaluating the untyped lambda calculus one has to choose an evaluation strategy.
The choice of evaluation strategy has a large impact on aspects such as complexity guarantees.
The names of such strategies are \textit{call by value}, \textit{call by name} and \textit{call by need}.
The call by value is most often the simplest and most natural way of assuming program execution.
\begin{lstlisting}[language=ML,caption={Program that doubles values},label={lst:callbyvalue},mathescape=true]
fun double x = x + x;
let a = double 10;
double (double 10);
\end{lstlisting}
By the call by value semantics, \autoref{lst:callbyvalue} eagerly evaluates every expression.
Clearly the variable \texttt{a} is never used but under the call by value semantics everything is eagerly evaluated.
Every expression is evaluated in logical order in the call by value evaluation strategy.
\begin{figure}[ht]
\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:callbyname},mathescape=true]
fun suspend x unit = x;
fun force x = x 0;
let value = suspend 10;
fun double x = 
    fun susExpensiveOp unit = 
        (force x) + (force x);
    susExpensiveOp;
let a = double value;
force (double value);
\end{lstlisting}
\end{figure}
The call by name semantics however does only evaluate expressions once they are needed (also commonly called \textit{lazy evaluation}).
By the call by name semantics \texttt{a} is never evaluated since it is never used.
In \autoref{lst:callbyname} call by name has been implemented by the use of various functions such as the two constant functions \texttt{suspend} and \texttt{force}.
\texttt{susExpensiveOp} ensures that the forcing (evaluation) of \texttt{x} never occurs until the caller of \texttt{double} forces the result.
By the aforementioned semantics of call by name in the context of the program in \autoref{lst:callbyname} \texttt{a} is never forced thus the computation is never performed.
The implementation of call by name can become quite troublesome and therefore in most cases is a part of the native execution environment which will be discussed in \autoref{tbd}.

The call by need semantics introduces the same lazy evaluation semantics as the call by name strategy but with one extra detail named \textit{sharing}.
In \autoref{lst:callbyname} \texttt{force x} is performed twice which may be an expensive operation.
Under call by need all non side-effectful operations' results are saved for later use similar to techniques such as dynamic programming.
\begin{figure}
    \begin{mdframed}[style=style2]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
    \begin{tikzpicture}
        \node[circle, draw=black] (force) {\texttt{force}};

        \node[circle, draw=black, below = of force] (double) {\texttt{double}};

        \node[circle, draw=black, below = of double] (value) {\texttt{value}};

        \path[->] (force) edge node[left] {} (double);
        \path[->] (double) edge node[left] {} (value);
    \end{tikzpicture}
        \caption{The last expression of the program.}
        \label{sub:eval:main}
    \end{subfigure}
    \begin{subfigure}[b]{0.66\textwidth}
        \centering
    \begin{tikzpicture}
        \node[circle, draw=black] (lamx) {$\lambda \texttt{x}$};

        \node[circle, draw=black, below = of lamx] (lamunit) {$\lambda \texttt{unit}$};

        \node[circle, draw=black, below = of lamunit] (addition) {$+$};

        \node[circle, draw=black, below left = of addition] (force1) {\texttt{force}};
        \node[circle, draw=black, below = of force1] (x1) {\texttt{x}};
        
        \node[circle, draw=black, below right = of addition] (force2) {\texttt{force}};
        \node[circle, draw=black, below = of force2] (x2) {\texttt{x}};

        \path[->] (lamx) edge node[left] {} (lamunit);
        \path[->] (lamunit) edge node[left] {} (addition);

        \path[->] (addition) edge node[left] {} (force1);
        \path[->] (addition) edge node[left] {} (force2);

        \path[->] (force1) edge node[left] {} (x1);
        \path[->] (force2) edge node[left] {} (x2);
    \end{tikzpicture}
        \caption{The expression tree for \texttt{double}}
        \label{sub:eval:double}
    \end{subfigure}
    \end{mdframed}
    \caption{}
    \label{fig:evalexpr}
\end{figure}
To understand this better observe the expression tree for \autoref{lst:callbyname} in \autoref{fig:evalexpr}.
Clearly the right and left branches in \autoref{sub:eval:double} are identical thus they may be memoized such that the forcing of \texttt{x} only occurs once.
More generally if the execution environment supports lazy evaluation, once an expression has been forced it is remembered if that branch is executed again.

\section{Runtime environments}
Now that the untyped lambda calculus has been introduced, implemented and validated efficiently the question of execution naturally follows.
There exists many different well understood strategies to implement an execution environment for the untyped lambda calculus.
Naively it may seem straightforward to evaluate the untyped lambda calculus mechanically by $\beta$-reductions but a problem arises when implementing an interpreter for application.
When applying the term $f x$ in $\lambda f . \lambda x . f x$ such that $f[? := x]$ where $?$ is the parameter name of $f$ it should become clear why a naive strategy is not enough since the parameters of $f$ are anonymous.

\subsection{Combinator reducers}
One of the most prominent techniques for evaluating functional programs is that of \textit{combinator graphs reductions}.
Formally a combinator is a function that has no free variables which is convenient since the problem of figuring out what a parameter name is never arises.
\begin{align}
    x \label{eq:comb:x}\\
    F \label{eq:comb:F}\\
    Y E \label{eq:comb:app}
\end{align}
There are three types of terms in combinator logic; the variable much like the lambda calculus (\autoref{eq:comb:x}), application (\autoref{eq:comb:app}) and the combinator (\autoref{eq:comb:F}).
The SKI calculus is a very simple set of combinators which are powerful enough to be turing complete and translate to and from the lambda calculus.
In SKI $F ::= S \,\,|\,\, K \,\,|\,\, I$ where the equivalent lambda calculus combinators for $S = \lambda x . \lambda y . \lambda z . x z (y z)$, $K = \lambda x . \lambda y . x$ and $I = \lambda x . x$.
Evaluating an SKI program is a straightforward reduction where $F'_F$ denotes combinator $F'$ has been partially applied with combinator $F$.
\begin{exmp}
    \begin{align}
        &SKSI\\
        = &KI(SI)\tag*{}\\
        = &K_I(SI)\tag*{}\\
        = &I\tag*{}
    \end{align}
\end{exmp}

The algorithm for converting a lambda calculus program into a SKI combinator program is a straightforward mechanical one.
The evaluation context is always an abstraction $\lambda x . E$ thus translation of $E$ will also always be an abstraction.
\begin{pcases}
    \pcase{$E = x$ then rewrite $\lambda x . E$ to $I$.}
    \pcase{$E = y$ where $y$ is a variable then rewrite $\lambda x . y$ to $Ky$.}
    \pcase{$E = Y E$ then rewrite $\lambda x . Y E$ to $S(\lambda x . Y)(\lambda x . E)$ since applying some $y$ must lambda lift $y$ as a parameter named $x$ to both $Y$ and $E$ such that the lifted expression becomes $((\lambda x . Y)y)((\lambda x . E)y) = S(\lambda x . Y)(\lambda x . E)y$.
    Then recurse in both branches.
    }
    \pcase{$E = \lambda y . E'$ rewrite $E'$ and then rewrite the expression.}
\end{pcases}
When translating the untyped lambda calculus to SKI the ''magic`` variable names $\zeta, \kappa$ and $\iota$ are used as placeholder functions for the SKI combinators since the translation requires a lambda calculus form.
When the translation has been completed then replace $\zeta \mapsto S, \kappa \mapsto K, \iota \mapsto I$.
\subsection{Combinator translation growth}
The growth factor is in terms of SKI combinators to lambda calculus terms.
\begin{lemma}
    The SKI calculus increases the computational complexity to $O(n^2)$
\end{lemma}
\begin{proof}
\begin{pcases}
    \pcase{\label{eval:case:1} Rewriting $\lambda x . x$ to $I$ is a reduction of one.}
    \pcase{\label{eval:case:2} Rewriting $\lambda x . y$ to $Ky$ is equivalent in terms of size.}
    \pcase{Rewriting $\lambda x . YE$ to $S(\lambda x . Y)(\lambda x . E)$ is the interesting case.
        To induce the worst running time \autoref{eval:case:1} must be avoided.
        If $x \notin \textit{Free}(Y)$ and $x \notin \textit{Free}(E)$ then for every terminal term in $Y$ and $E$ \autoref{eval:case:2} is the only applicable rewrite rule which means that an at least equal size is guaranteed.
        Furthermore observe that by introducing unused parameters one can add one $K$ term to \textit{every} terminal case.
        Observe the instance $\lambda f_1 . \lambda f_2 . \lambda f_3 . (f_1 f_1 f_1)$ where the two unused parameters are used to add $K$ terms to all terminal cases in \autoref{eq:eval:red2} such that the amount of extra K terms minus the I becomes $\text{variablereferences} * (\text{unusedabstractions} - 1) = 3*(3-1)$.
        \begin{gather}
            S(S(KKI)(KKI))(KKI)\label{eq:eval:red2}
        \end{gather}
        Now let the amount of variable references be $n$ and the unused abstractions also be $n$ clearly $O(n * (n - 1)) = O(n^2)$
    }
    \pcase{Rewriting $\lambda x . E'$ is not a traslation rule so the cost is based on what $E'$ becomes.}
\end{pcases}
\end{proof}

In some cases computational complexity measures are very much purely academical and so is the analysis of the translated SKI calculus on a pathological input.
This is not to say that the running time is always linear is practice as a simple program that deliberately avoids the worst case still produces larger output.
In the case of SKI observe that the applications $f_1 f_1 \dots f_1$ can in fact be changed to $f_1 f_2 \dots f_n$ since for every $f_k$ where $0 < k \leq n$ there are $n - 1$ parameters that induce a K combinator.
\begin{exmp}
    Observe the size of \autoref{eq:eval:comp1} in comparison to \autoref{eq:evaltime}.
\begin{align}
    &\lambda f_1 . \lambda f_2 . f_1 f_2 \label{eq:eval:comp1}\\
    =&\lambda f_1 . \zeta(\lambda f_2 . f_1)(\lambda f_2 . f_2) \tag*{} \\
    =&\lambda f_1 . (\zeta(\kappa f_1))(\iota) \tag*{} \\
    =&\zeta (\lambda f_1 . \zeta (\kappa f_1)) (\lambda f_1 . \iota) \tag*{} \\
    =&\zeta (\zeta (\lambda f_1 . \zeta) (\lambda f_1 . \kappa f_1)) (\kappa \iota) \tag*{} \\
    =&\zeta (\zeta (\kappa \zeta) (\zeta (\lambda f_1 . \kappa) (\lambda f_1 . f_1))) (\kappa \iota) \tag*{} \\
    =&\zeta (\zeta (\kappa \zeta) (\zeta (\kappa \kappa) (\iota))) (\kappa \iota) \tag*{} \\
    =&S (S (K S) (S (K K) (I))) (K I) \tag*{}
\end{align}
\end{exmp}

\subsection{Beta reductions with De Bruijn indices}
$\beta$-reductions on the untyped lambda calculus is another method of evaluating a program.
The techniques required to correctly evaluate a program are a bit more complicated than the SKI calculus but are rewarding in the performance.
In $(\lambda f . \lambda x . \lambda y. \texttt{ let } a = f x \texttt{ in } a + y)(\lambda y . y) \,\, 10 \,\, 20$ when evaluating $f x$ where the substitution is $[f \mapsto \lambda y . y, x \mapsto 10, y \mapsto 20]$ there must be taken great care that $x$ is mapped to $y$ in $\lambda y . y$.

Beta reductions with De Bruijn indices~\cite{de1972lambda} is another more straightforward method of evaluating the untyped lambda calculus.
De Bruijn indices is a representation of lambda calculus which deals with variables based on the scope ''distance`` instead of variable names.
\begin{lstlisting}[language=ML,caption={Add as De Bruijn},label={lst:adddebru},mathescape=true]
let add = ($\lambda$($\lambda$ 2 1))
\end{lstlisting}
Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
More generally all variable occurrences are replaced with the distance to the abstraction which introduced them.
The use of De Bruijn indices allow anonymous naming of parameters thus imply a method of solving the application problem when directly interpreting the untyped lambda calculus.
A minor but important detail when 
\begin{align}
    &(\lambda x . ((\lambda f. \lambda x. f x)(\lambda z . z))x)''0`` \rightarrow\\
    &
\end{align}

\end{document}
