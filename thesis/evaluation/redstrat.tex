\section{Reduction strategies}
Reductions in the context of the lambda calculus are a small set of well-defined rules for rewriting such that a program is proved or evaluated.
The techniques required to correctly prove and evaluate a program are a bit more complicated than the SKI calculus but are rewarding in flexibility and performance.
Throughout this section we will explore what difficulties lie within proving and evaluating the untyped lambda calculus via reduction strategies.
The first section will interest itself with the semantics of proving the untyped lambda calculus, whilst the second will implement a machine capable of evaluating a result.

\subsection{Symbols and notation}
In this section, expressions will be written in a different typesetting since some expressions and symbols mean something different than in other sections.
For instance \texttt{x $\rightarrow$ y} means the evaluation of \texttt{x} results in \texttt{y}, while $x \rightarrow y$ means the type of a function that takes a value of type $x$ and returns a value of type $y$.

The following sections will have many variables with different meaning, therefore symbols are constrained to certain types of values as described in \autoref{eq:redstrat:sym}.
\begin{align}
  \texttt{x, y, z, f, v, $\gamma$ }&\texttt{:= Var}\label{eq:redstrat:sym}\\
  \texttt{e, p, l, o }&\texttt{:= Exp}\tag*{}\\
  \texttt{$\Gamma$, $\Sigma$, $\Theta$, S }&\texttt{:= Heap}\tag*{}\\
  \texttt{E, $\mathcal{E}$ }&\texttt{:= Environment}\tag*{}
\end{align}
\texttt{Exp} is any expression, expressible in both untyped lambda calculus and future extensions.


\subsection{The abstract evaluation model}
The environment is a set of substitutions, that is, a set of variable names to their value denoted \texttt{\{x $\mapsto$ $\lambda$y.y\}} meaning ``the value of variable \texttt{x} is \texttt{$\lambda$y.y}''.
\begin{remark}
  In the section regarding the semantics of reduction strategies, environments will exist as singleton sets, named \textit{substitutions}.
\end{remark}
%Substitutions mappings can be combined $S \cup \Sigma$, if a variable name occurs in both $S$ and $\Sigma$ then the variable in the rightmost mapping is chosen ($\Sigma$).
\begin{align}
  \texttt{\{x $\mapsto$ y\}x = y}\label{eq:subsem}
\end{align}
Substitutions are performed like shown in \autoref{eq:subsem}, which states ``x is substituted by y''.
%In $(\lambda f . \lambda x . \lambda y. \texttt{ let } a = f x \texttt{ in } a + y)(\lambda y . y) \,\, 10 \,\, 20$ when evaluating $f x$ where the substitution are $[f \mapsto \lambda y . y, x \mapsto 10, y \mapsto 20]$ there must be taken great care that $x$ is mapped to $y$ in $\lambda y . y$.

Evaluation strategies (\autoref{sec:es}) are a core part of the reduction strategy since the choice of evaluation strategy determines the order in which terms are evaluated.
%A \textit{redex} is a reducible expression in the context of some set of reduction rules.
%The two interesting evaluation orders are \textit{applicative order} and \textit{normal order}~\cite{sestoft2002demonstrating}.
%The most interesting evaluation order is 
%Applicative order specifies that the parameters of some application should always be evaluated first, e.g. call by value.
%Normal order specifies that the leftmost outermost term should be evaluated first which yields the call by name strategy.
The order of evaluation decides the evaluation strategy and also the final form of expressions~\cite{sestoft2002demonstrating}.
Before delving into more complicated evaluation strategies such as call by need, call by name will be considered.

A reduction strategy would involve substituting variables, once they are applied.
When evaluating a term such as \texttt{($\lambda$x.x) y}, \texttt{x} must be substituted by \texttt{y} such that the expression then becomes \texttt{x} with the substitution \texttt{\{x $\mapsto$ y\}} and finally becomes \texttt{y} after the substitution has occurred.
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.49cm}
        \begin{subfigure}[b]{0.48\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{\texttt{($\lambda$x.e) $\rightarrow$ ($\lambda$x.e)}}
            \end{prooftree}   
            \caption{}
            \label{eq:simpleabs}
        \end{subfigure}
        \begin{subfigure}[b]{0.48\textwidth}
            \vspace*{0.4cm}
            \begin{prooftree}
                \AxiomC{}
                %\AxiomC{$S \alpha x \not\equiv \alpha x$}
                \RightLabel{Var}
                \UnaryInfC{\texttt{x $\rightarrow$ x}}
            \end{prooftree}   
            \caption{}
            \label{eq:simplevar}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \vspace*{0.4cm}
              \begin{prooftree}
                \AxiomC{\texttt{\{x $\mapsto$ e\} p $\rightarrow$ l}}
                  \RightLabel{Let}
                  \UnaryInfC{\texttt{let x = e in p $\rightarrow$ l}}
              \end{prooftree}   
          \caption{}
          \label{fig:simplelet}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
            \vspace*{0.4cm}
              \begin{prooftree}
                \AxiomC{\texttt{l $\rightarrow$ ($\lambda$x.e)}}
                \AxiomC{\texttt{\{x $\mapsto$ p\} e $\rightarrow$ o}}
                  \RightLabel{App}
                  \BinaryInfC{\texttt{l p $\rightarrow$ o}}
              \end{prooftree}   
          \caption{A simple application rule}
          \label{fig:simpleapp}
        \end{subfigure}
    \end{mdframed}
    \caption{Simple call by name lambda calculus}
    \label{fig:scbn}
\end{figure}
The rules in \autoref{fig:scbn} display a simple set of rules for proving call by name lambda calculus programs.
\begin{itemize}
\item The Abs and Var rules (\autoref{eq:simpleabs} and \autoref{eq:simplevar}) are rules which act as terminal cases of a proof.
      Abs and Var both state that if either of them occur then the expression must be an axiom by their identity.
\item The App rule (\autoref{fig:simpleapp}) states that ``\texttt{l p} can be proved to evaluate to \texttt{o} if \texttt{l} can be proved to be \texttt{($\lambda$x.e)} and \texttt{e} can be proved to evaluate to \texttt{o}, where \texttt{x} has been replaced by \texttt{p} in \texttt{e}''.
\item The Let rule has the same function as the App rule, but will have an important role in a more refined version of the semantics.
\end{itemize}
We must introduce rules for how substitutions should act upon encountering lambda calculus terms (\autoref{eq:subsem}).
\begin{align}
  &\texttt{\{x $\mapsto$ e\}x = e} \label{eq:subsem}\\
  &\texttt{\{x $\mapsto$ e\}p = p}  \tag*{}\\
  &\texttt{E(l p) = (El)(Ep)} \tag*{}\\
  &\texttt{E($\lambda$x.e) = ($\lambda$x.Ee) } \label{eq:subsemrem}%\\
\end{align}

\subsubsection{Ambiguous programs}
\begin{align}
  \texttt{($\lambda$x.($\lambda$x.x) 0) 1} \label{eq:ambgprog}
\end{align}
Proving \autoref{eq:ambgprog} under the rules in \autoref{eq:subsem} yields a case for more thorough substitution rules.
By inspection one can determine that a simple program like \autoref{eq:ambgprog} yields the symbol \texttt{0}, but alas this is not the case.
The first step to prove \autoref{eq:ambgprog} is to apply through the App rule, which prompts the application of the Abs rule on the left side for \texttt{f}, such that the expression to prove now becomes the lambda abstraction with \texttt{x} replaced by \texttt{1} (\autoref{eq:ambgprog2}).
\begin{align}
  \texttt{($\lambda$x.1) 0} \label{eq:ambgprog2}
\end{align}
Clearly \autoref{eq:ambgprog2} changed the meaning of the program.
If we continue the proof which states that the program in \autoref{eq:ambgprog2} should evaluate to the symbol \texttt{0}, we would not be in luck.
Clearly this system is not sound, thus requires some further refinement.
%\begin{figure}
%\begin{lstlisting}[language=ML,caption={Program with variable ambiguity},label={lst:varambg},mathescape=true]
%fun f x =
  %fun g x = x;
  %g (x + x);
%f 2;
%\end{lstlisting}
%\end{figure}
%Evaluating \autoref{lst:varambg} under the rules in \autoref{eq:subsem} yields a case for more thorough substitution rules.
%Using the App rule (\autoref{fig:simpleapp}) and applying substitutions as defined in (\autoref{eq:subsem}), yields the program in \autoref{lst:varambg2}.
%\begin{lstlisting}[language=ML,caption={Program with variable ambiguity after one reduction pass},label={lst:varambg2},mathescape=true]
%fun g x = 2;
%g (2 + 2);
%\end{lstlisting}
%Which eventually evaluates to \texttt{2}, which clearly is not the intended result.
Removing the rule \autoref{eq:subsemrem} and adding the two rules in \autoref{eq:subsemfixed} solves this type.
\begin{align}
  &\texttt{E($\lambda$x.e) = ($\lambda$x.Ee)} & \texttt{(x $\mapsto$ p) $\notin$ S} \label{eq:subsemfixed}\\
  &\texttt{E($\lambda$x.e) = ($\lambda$x.(E$\backslash$\{x $\mapsto$ p\})e)} & \texttt{(x $\mapsto$ p) $\in$ S} \tag*{}
\end{align}
This is a simple instance of variable ambiguity, a more problematic variant exists which goes by the name of variable capture.
This evaluation model is indeed powerful enough to evaluate \textbf{most} call by name lambda calculus programs.
\begin{exmp}
  With the aforementioned rules, programs can now be proved.
  Note that the Substitution rule in \autoref{fig:rules:exmp:sol} is simply the substitution semantics from \autoref{eq:subsemfixed} made clearer.
  Let the program in \autoref{eq:exmp:subrules}, where \texttt{0} is a symbol of any type, be subject to the rules in \autoref{fig:scbn}, which solves to \autoref{fig:rules:exmp:sol}.
  \begin{align}
    \texttt{(($\lambda$f.$\lambda$x.f x) ($\lambda$x.x)) 0}\label{eq:exmp:subrules}
  \end{align}
  \begin{figure}[ht]
    \begin{mdframed}
        \vspace*{0.49cm}
      \begin{subfigure}[b]{1\textwidth}
        \begin{prooftree}
    \AxiomC{}
    \LeftLabel{Abs}
  \UnaryInfC{\texttt{($\lambda$x.($\lambda$x.x) x) $\rightarrow$ ($\lambda$x.($\lambda$x.x) x)}}
\LeftLabel{Substitution}
\UnaryInfC{\texttt{\{f $\mapsto$ ($\lambda$x.x)\}($\lambda$x.f x) $\rightarrow$ ($\lambda$x.($\lambda$x.x) x)}}
        \end{prooftree}
        \caption{}
        \label{fig:rules:exmp:left:right}
      \end{subfigure}
        \vspace*{0.49cm}
      \begin{subfigure}[b]{1\textwidth}
        \begin{prooftree}
    \AxiomC{}
  \LeftLabel{Abs}
  \UnaryInfC{\texttt{($\lambda$f.$\lambda$x.f x) $\rightarrow$ ($\lambda$f.$\lambda$x.f x)}}
  \AxiomC{\autoref{fig:rules:exmp:left:right}}
\LeftLabel{App}
\BinaryInfC{\texttt{($\lambda$f.$\lambda$x.f x) ($\lambda$x.x) $\rightarrow$ ($\lambda$x.($\lambda$x.x) x)}}
        \end{prooftree}
        \caption{}
        \label{fig:rules:exmp:left}
      \end{subfigure}
        \vspace*{0.49cm}
      \begin{subfigure}[b]{1\textwidth}
        \begin{prooftree}
      \AxiomC{}
    \LeftLabel{Abs}
    \UnaryInfC{\texttt{($\lambda$x.x) $\rightarrow$ ($\lambda$x.x)}}
        \AxiomC{}
      \RightLabel{Var}
      \UnaryInfC{\texttt{0 $\rightarrow$ 0}}
    \RightLabel{Substitution}
    \UnaryInfC{\texttt{\{x $\mapsto$ 0\}x $\rightarrow$ 0}}
  \RightLabel{App}
  \BinaryInfC{\texttt{($\lambda$x.x) 0 $\rightarrow$ 0}}
\RightLabel{Substitution}
\UnaryInfC{\texttt{\{x $\mapsto$ 0\}($\lambda$x.x) x $\rightarrow$ 0}}
        \end{prooftree}
        \caption{}
        \label{fig:rules:exmp:right}
      \end{subfigure}
        \vspace*{0.49cm}
      \begin{subfigure}[b]{1\textwidth}
        \begin{prooftree}
  \AxiomC{\autoref{fig:rules:exmp:left}}
  \AxiomC{\autoref{fig:rules:exmp:right}}
\LeftLabel{App}
\BinaryInfC{\texttt{(($\lambda$f.$\lambda$x.f x) ($\lambda$x.x)) 0 $\rightarrow$ 0}}
        \end{prooftree}
      \end{subfigure}
    \end{mdframed}
    \caption{}
    \label{fig:rules:exmp:sol}
  \end{figure}
\end{exmp}

Variable capture is the basis for some practical difficulties when designing evaluation rules for the untyped lambda calculus.
Consider the following sub-program \texttt{($\lambda$x.y) g} with the following ongoing substitutions \texttt{\{x $\mapsto$ z, y $\mapsto$ x, $\dots$\}}, which contains \texttt{y} as a closure.
Substituting by the rules outlined in \autoref{eq:subsemfixed} yields \texttt{($\lambda$x.x) g} which is clearly invalid.
The invalid program result is a product of variable capture.
%The program before substitution and after should be \textit{$\alpha$-equivalent}, which they are not.
%The two program states becomes ambiguous by the ambiguity of multiple variables by the same name.
To solve ambiguity between variables with the same name, one can perform an \textit{$\alpha$-conversion}.
\begin{remark}
  Notice that if variables are renamed before program execution, recursive functions can still suffer from ambiguity since all parameters for that function can occur multiple times.
\end{remark}
\subsubsection{$\alpha$-conversions} \label{sec:alpha}
%An $\alpha$-conversion mapping can occur as the substitution mapping, that is \texttt{\{x $\mapsto$ $\gamma_1$, y $\mapsto$ $\gamma_2$\}}; $\alpha$-conversions are philosophical constructs more than materialized mappings.
An $\alpha$-conversion is a renaming operation which does not modify the meaning of the expression.
$\alpha$-conversions can appear similar to substitutions, for instance renaming \texttt{x} to $\gamma$ appears as \texttt{\{x $\mapsto$ $\gamma$\}}.
$\alpha$-conversions guarantee what is called \textit{$\alpha$-equivalence} which is the notion of semantic equivalence.
For instance \texttt{$\lambda$x.x} is $\alpha$-equivalent with \texttt{$\lambda\gamma.\gamma$} since both expressions are semantically equivalent.
%$\alpha$-conversions solve some critical problems such as closures and recursion when evaluating the lambda calculus.
%$\alpha$-conversions should also be non-destructive but still be context aware such that when leaving an abstraction the remaining substitution mapping is a superset of the substitution mapping when entering.
%More formally if \texttt{S, e $\rightarrow$ $\Sigma$, e'} then $\{x \,\,|\,\, (x \mapsto y) \in S\} \subseteq \{x \,\,|\,\, (x \mapsto y) \in \Sigma\}$.
%\begin{lstlisting}[language=ML,caption={Recursive addition function},label={lst:recalpha},mathescape=true]
%let f = ($\lambda$f'.$\lambda$x.
    %if (x = 0) x else f' f' ((x - 1) + (x - 1))) in
%f f 10
%\end{lstlisting}
%A strong guarantee that can be made by tuning the evaluation strategy which is particularly useful for $\alpha$-conversion algorithms is that \textit{any} returned value has had \textit{every} term that it contains visited.
%An algorithm for performing $\alpha$-conversions can be implemented that picks a new variable name from any infinite domain when an abstraction has had a value applied to it and replaces future encountered variables with the new one, such an algorithm only works if the guarantee of visiting every term is made.
%The algorithm should also introduce the applied value to the substitution set through the alpha converted name.
\noindent An $\alpha$-conversion algorithm can be implemented such that when a new variable is introduced through an abstraction, a new name for the variable is given.
More formally; Let $V_1$ be the domain of variables in the program and $V_2$ be the infinite domain for variable names that satisfies $V_1 \cap V_2 = \emptyset$, such that when a new variable \texttt{x} is discovered, replace it with some $\gamma \in V_2$ and let $V_2 = V_2 \backslash \{\gamma\}$. 
Working with the previous example \texttt{\{y $\mapsto$ x\}($\lambda$x.y) g}
%For instance, let \texttt{($\lambda\gamma_1$.$\lambda\gamma_2$.if ($\gamma_2$ = 0) $\gamma_2$ else $\gamma_1$ $\gamma_1$ (($\gamma_2$ - 1) + ($\gamma_2$ - 1)))} be the $\alpha$-converted version of \texttt{f} in \autoref{lst:recalpha}.
The function \texttt{fresh} picks a fresh variable name $\gamma$ from $V_2$ and updates $V_2$ to $V_2 \backslash \{\gamma\}$.
$\alpha$-conversions will be further explored in future refinements of \autoref{fig:scbn} in the from of renaming through the Let rule.
\\

%\subsubsection{Eager substitutions}
%Instead of performing $\alpha$-conversions to ensure variable uniqueness, one can enhance the rules in \autoref{fig:scbn}.%one can leverage the ideas proposed in~\cite{launchbury1993natural} to implement laziness, lazy substitution, solve variable ambiguity and perform efficient garbage collection.
%To further refine the call by name evaluation semantics observe that substitutions are performed eagerly, that is, they \textit{break suspension}.
%To explore solutions to avoid eager substitutions, environments may be used.
%\begin{remark}
  %When considering an arbitrarily large expression under call by name (or need), there is no guarantee that the expression will ever be evaluated, that is, a suspended expression being forced.
  %Rules specify what implementations must adhere to and not anything more.
  %Regardless it still remains as an interest to not break suspension.
  %Breaking suspension may violate the philosophy behind call by name (and need).
  %A solution could involve composing a function in compile time, which replaces all instances of a variable (by \autoref{eq:subsem} and \autoref{eq:subsemfixed}) with some parameterized value.
  %Substituting expressions eagerly might compromise practical performance guarantees, even if substitution is an asymptotically constant number of operations.
  %Consider substituting an expression which contains conditional branches, one branch might be a simple expression and the other might be a very large expression, consisting of many instances of the same variable.
  %If the program never picks the large expression, why should it ever have any impact on performance.
  %For instance, in \autoref{lst:fastslow} under call by name one would expect \texttt{fastMap} to evaluate in roughly the same time as \texttt{slowMap}.
%\begin{lstlisting}[language=ML,caption={Performance considerations in substitution},label={lst:fastslow},mathescape=true]
%fun fastMap f l =
  %match l
    %| Nil -> Nil;
    %| Cons x xs -> Cons (f x) (map f xs);
  %;
%fun slowMap f l =
  %match l
    %| Nil -> Nil;
    %| Cons x xs -> 
      %if (x == 0)
        %let v = x + x + x + x + x + x; 
        %Cons (f x) (map f xs);
      %else
        %Cons (f x) (map f xs);
      %;
  %;
%\end{lstlisting}
%\end{remark}
\subsubsection{The heap}
Heaps, like environments in typing (\autoref{eq:env}), define what ``state'' is required to evaluate some expression.
A heap contains mappings from variables to expressions, much like the environment which performs substitutions, except it acts like a store.
Heaps are a requirement for call by need semantics.
A simple modification to the rules in \autoref{fig:rules:env}, introduces a heap which states that that the semantics must bring a heap along. 
\begin{figure}[ht]
    \begin{mdframed}[style=bigbox]
        \vspace*{0.49cm}
        \begin{subfigure}[b]{0.49\textwidth}
            \begin{prooftree}
                \AxiomC{}
                \RightLabel{Abs}
                \UnaryInfC{\texttt{$\Gamma$, ($\lambda$x.e) $\rightarrow$ $\Gamma$, ($\lambda$x.e)}}
            \end{prooftree}   
            \caption{}
          \label{fig:rules:env:abs}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
            \vspace*{0.4cm}
              \begin{prooftree}
                \AxiomC{\texttt{$\Gamma$ $\cup$ \{x $\mapsto$ e\}, p $\rightarrow$ $\Theta$, l}}
                  \RightLabel{Let}
                  \UnaryInfC{\texttt{$\Gamma$, let x = e in p $\rightarrow$ $\Theta$, l}}
              \end{prooftree}   
          \caption{}
          \label{fig:rules:env:let}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
          \vspace*{0.4cm}
          \begin{prooftree}
            \AxiomC{\texttt{$\Gamma$, l $\rightarrow$ $\Theta$, ($\lambda$x.e)}}
            \AxiomC{\texttt{$\Theta$, \{x $\mapsto$ p\}e $\rightarrow$ $\Sigma$, o}}
              \RightLabel{App}
              \BinaryInfC{\texttt{$\Gamma$, l p $\rightarrow$ $\Sigma$, o}}
          \end{prooftree}   
          \caption{}
          \label{fig:rules:env:app}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
          \vspace*{0.4cm}
            \begin{prooftree}
                \AxiomC{\texttt{$\Gamma$, e $\rightarrow$ $\Theta$, p}}
                \RightLabel{Var}
                \UnaryInfC{\texttt{$\Gamma$ $\cup$ \{x $\mapsto$ e\}, x $\rightarrow$ $\Theta$ $\cup$ \{x $\mapsto$ e\}, p}}
            \end{prooftree}   
            \caption{}
          \label{fig:rules:env:var}
        \end{subfigure}
    \end{mdframed}
    \caption{Call by name lambda calculus with environments}
    \label{fig:rules:env}
\end{figure}
The rules in \autoref{fig:rules:env} are quite different from the rules in \autoref{fig:scbn}.
\begin{itemize}
  \item Var is no longer terminal, it now inspects the heap for a replacement value for some \texttt{x}.
    Notice that Var now removes the mapping from the heap $\Gamma$ such that recursively defined expressions cannot occur.
  \item Let now has a role which is distinct from App.
    Let now introduces values to the heap, but does not induce a substitution.
  \item App remains the same by eagerly substituting, but now augmented with a heap.
  \item Abs is now augmented with a heap.
\end{itemize}
The rules in \autoref{fig:rules:env} are not any more powerful that the rules in \autoref{fig:scbn}, but are a basis for lazy evaluation.

\subsubsection{Lazy evaluation}
With the revised semantics in \autoref{fig:rules:env}, lazy evaluation can now be introduced.
The basis for sharing evaluated expressions is rooted in a labelling problem~\cite{levy1988sharing}.
Before delving into a set of rules which use a labelling technique, consider that sharing can be viewed as a dependency graph of expressions.
Let \autoref{fig:dependencygraph} be a depiction of the dependency graph of \autoref{eq:xyk} under the rules in \autoref{fig:rules:env}.
\begin{align}
  &\texttt{let k = ($\lambda$z.z) ($\lambda$f.f) in }\label{eq:xyk}\\
  &\texttt{let x = k in }\tag*{}\\
  &\texttt{let y = k in }\tag*{}\\
  &\texttt{x + y}\tag*{}
\end{align}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.25cm and 0.25cm]
      \node[draw=black] (e) {\texttt{($\lambda$z.z) ($\lambda$f.f)}};

          \node[draw=black, below = of e] (k) {\texttt{k}};
          \path[->] (k) edge node[left] {} (e);
            
              %\node[draw=black, below left = of k] (gamma1) {$\gamma_1$};
              %\path[->] (gamma1) edge node[left] {} (k);

              \node[draw=black, below left = of k] (y) {\texttt{y}};
              \path[->] (y) edge node[left] {} (k);

            %\node[draw=black, below right = of k] (gamma2) {$\gamma_2$};
            %\path[->] (gamma2) edge node[left] {} (k);

              \node[draw=black, below right = of k] (x) {\texttt{x}};
              \path[->] (x) edge node[left] {} (k);

          \node[draw,fill=none,scale=1.3,rectangle,fit=(e)(x)(y)](fbb){};
  \end{tikzpicture}
  \caption{Expression dependencies}
  \label{fig:dependencygraph}
\end{figure}
A rule which encapsulates ``when evaluating a value for a variable, save the evaluated value for future use.'' is required to support sharing computed values.
\begin{figure}[ht]
  \begin{mdframed}
    \begin{prooftree}
      \AxiomC{\texttt{$\Gamma$, e $\rightarrow$ $\Theta$, p}}
      \RightLabel{Var}
      \UnaryInfC{\texttt{$\Gamma$ $\cup$ \{x $\mapsto$ e\}, x $\rightarrow$ $\Theta$ $\cup$ \{x $\mapsto$ p\}, p}}
    \end{prooftree}
  \end{mdframed}
  \caption{}
  \label{fig:eval:share}
\end{figure}
The rule in \autoref{fig:eval:share} replaces the Var rule, and introduces a subtle difference; when a variable reference occurs the value which the variable evaluates to is saved as the new reference.
Introducing shareable expressions through Let is in it's essence a labelling of an expression.
Evaluating \autoref{eq:xyk} under the new rules reveals that evaluating \texttt{x} forces \texttt{k} to be evaluated which then forces \texttt{($\lambda$z.z) ($\lambda$f.f)}, which becomes \texttt{($\lambda$f.f)} and is then saved as the new value of \texttt{k} and then as \texttt{x}, thus the dependency tree becomes \autoref{fig:dependencygraphv2}.
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.25cm and 0.25cm]
      \node[draw=black] (e) {\texttt{($\lambda$f.f)}};

          \node[draw=black, below left = of e] (k) {\texttt{k}};
          \path[->] (k) edge node[left] {} (e);
            
              %\node[draw=black, below left = of k] (gamma1) {$\gamma_1$};
              %\path[->] (gamma1) edge node[left] {} (k);

              \node[draw=black, below left = of k] (y) {\texttt{y}};
              \path[->] (y) edge node[left] {} (k);

            %\node[draw=black, below right = of k] (gamma2) {$\gamma_2$};
            %\path[->] (gamma2) edge node[left] {} (k);

              \node[draw=black, below right = of k] (x) {\texttt{x}};
              \path[->] (x) edge node[left] {} (e);

          \node[draw,fill=none,scale=1.3,rectangle,fit=(e)(x)(y)](fbb){};
  \end{tikzpicture}
  \caption{Expression dependencies after evaluating \texttt{x}}
  \label{fig:dependencygraphv2}
\end{figure}
One consideration remains, the App rule does not promote lazy evaluation.
All non-trivial parameters must be bound to a variable by the Let rule to also allow anonymous expressions to be subject to lazy evaluation.
An algorithm for binding anonymous expressions can be found in~\cite{launchbury1993natural}.

\subsubsection{Dealing with ambiguity}
The rules so far have avoided dealing with variable ambiguity.
Notice that variable capture can only occur in the Let rule, since the Let rule is the only rule of which can introduce name bindings.
Dealing with ambiguity is a matter of ensuring that variables are distinct.
Applying the technique from \autoref{sec:alpha} properly lets us evaluate programs without ambiguity.

Consider a previous case of variable capture \autoref{eq:ambg:cap}.
\begin{align}
  \texttt{\{x $\mapsto$ z, y $\mapsto$ x, $\dots$\}($\lambda$x.y) g}\label{eq:ambg:cap}
\end{align}
None of the changes so far have any impact on the falsity of the expression.
Consider that \texttt{x} and \texttt{y} must have been bound through a Let expression.
Consider also that some variable \texttt{k} cannot be subject to variable capture if \texttt{k $\notin$ Bound($\lambda$x.e)}.
Naturally if \texttt{k} is unique, that is, it is introduced through the \texttt{fresh} function from \autoref{sec:alpha} then \texttt{k} can never occur bound.
The obvious rule from these considerations must be new Let rule defined in \autoref{fig:eval:letrename}.
\begin{figure}[ht]
  \begin{mdframed}
    \begin{prooftree}
      \AxiomC{\texttt{$\Gamma$ $\cup$ \{$\gamma$ $\mapsto$ e\}, \{x $\mapsto$ $\gamma$\}p $\rightarrow$ $\Theta$, l}}
      \AxiomC{\texttt{$\gamma$ = fresh}}
        \RightLabel{Let}
        \BinaryInfC{\texttt{$\Gamma$, let x = e in p $\rightarrow$ $\Theta$, l}}
    \end{prooftree}   
  \end{mdframed}
  \caption{}
  \label{fig:eval:letrename}
\end{figure}
The correctness of \autoref{fig:eval:letrename} and the aforementioned considerations are formalised in~\cite{sestoft1997deriving}.

%For instance, consider the program in \autoref{lst:progambg} and the state after line \autoref{lst:progambg:l1} is executed in \autoref{eq:progambg2} and the remaining program in \autoref{lst:progambg2}.
%The state and remaining program is intact, but evaluating line \autoref{lst:progambg2:l1} in \autoref{lst:progambg} yields the state in \autoref{eq:progambg3} which changes the meaning of \texttt{v}.
%\begin{lstlisting}[language=ML,caption={Program which may suffer from variable ambiguity},label={lst:progambg},mathescape=true]
%let v = $\$$ (let x = 5 in ($\lambda$y.y + x)) in $\label{lst:progambg:l1}$
%let x = 0 in
%v 5
%\end{lstlisting}
%\begin{gather}
  %\texttt{\{v $\mapsto$ ($\lambda$y.y + x), x $\mapsto$ 5\}}\label{eq:progambg2}
%\end{gather}
%\begin{lstlisting}[language=ML,caption={Program after evaluating \texttt{v}},label={lst:progambg2},mathescape=true]
%let x = 0 in $\label{lst:progambg2:l1}$
%v 5
%\end{lstlisting}
%\begin{gather}
  %\texttt{\{v $\mapsto$ ($\lambda$y.y + x), x $\mapsto$ 0\}}\label{eq:progambg3}
%\end{gather}

%Naturally renaming can occur at the Let rule, since Let introduces new variables to the heap.
%Enhancing the Let rule with renaming by the semantics from \autoref{sec:alpha} yields a replacement for Let (\autoref{fig:eval:letrename})~\cite{sestoft1997deriving}. 
%Evaluating line \autoref{lst:progambg:l1} in \autoref{lst:progambg} now yields the state in \autoref{eq:pragambgfix} which introduces \texttt{x} as a fresh variable ($\gamma$) from the \texttt{fresh} function.
%\begin{gather}
  %\texttt{\{$\sigma$ $\mapsto$ ($\lambda$y.y + $\gamma$), $\gamma$ $\mapsto$ 5\}}\label{eq:pragambgfix}
%\end{gather}

\subsubsection{Introducing useful functionality}
As the set of rules stand currently one can express numbers through church encodings.
Church encodings provide a minimal and non-invasive set of combinators which allow the encoding of numbers.
Unfortunately it is not as practical as it is minimal to church encode numbers.
For instance, to represent the number \texttt{100000} one would require \texttt{100000} invocations of some successor function.
Fortunately dwelling on the representation of numbers is an easy task once one convinces themselves that ordinary numbers and arithmetic operations are friendly.

When discovering an arithmetic operations between two expressions, they must both be forced and then the pending expression must evaluated.
Clearly this rule is not encoded into the aforementioned rules, but can be modelled easily as shown in \autoref{fig:oplusrule}.
\begin{figure}[ht]
  \begin{mdframed}[style=style1]
        \begin{subfigure}[b]{1\textwidth}
        \vspace*{0.49cm}
          \begin{prooftree}
              \AxiomC{\texttt{$\Theta$, x $\rightarrow$ $\Sigma$, n}}
              \AxiomC{\texttt{$\Sigma$, y $\rightarrow$ S, t} $\,\,\,\,\,$ \texttt{$\oplus \in \{+,-,*,\backslash, =\}$}}
                \RightLabel{Bin op}
                \BinaryInfC{\texttt{$\Theta$, x $\oplus$ y $\rightarrow$ S, (n $\oplus$ t)}}
          \end{prooftree}   
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
        \vspace*{0.49cm}
            \begin{prooftree}
                \AxiomC{\texttt{n $\in \mathbb{Z^+}$}}
                \RightLabel{Num}
                \UnaryInfC{\texttt{S, n $\rightarrow$ S, n}}
            \end{prooftree}   
        \end{subfigure}
  \end{mdframed}
  \caption{}
  \label{fig:oplusrule}
\end{figure}
Notice that \autoref{fig:oplusrule} also must accompany a Num rule which introduces integers to the system.
\begin{remark}
  Notice that the Bin op rule in \autoref{fig:oplusrule} uses \texttt{x} and \texttt{y} which are in the domain of variables, since all non-trivial expressions must be bound to fresh names through Let.
\end{remark}

\begin{exmp}
Now that rules have been established which avoid variable ambiguity through renaming and support lazy evaluation, an example seems natural.
An expression which requires the aforementioned properties to resolve as expected is presented in \autoref{eq:doneexmp} and proved in \autoref{fig:finproofexmp}.
\begin{align}
  \texttt{let y = (1 + 1) in ($\lambda$x.($\lambda$y.x + y) x) y}\label{eq:doneexmp}
\end{align}
\begin{figure}[ht]
  \begin{mdframed}[style=bigbox]
    \begin{subfigure}[b]{1\textwidth}
      \vspace*{0.49cm}
      \begin{prooftree}
            \AxiomC{}
            \LeftLabel{Lam}
            \UnaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, ($\lambda$x.($\lambda$y.x + y) x) $\rightarrow$ \{$\gamma$ $\mapsto$ (1 + 1)\},($\lambda$x.($\lambda$y.x + y) x)}}
      \end{prooftree}
      \caption{}
      \label{fig:finproofexmp:app1}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
      \vspace*{0.49cm}
      \begin{prooftree}
            \AxiomC{}
            \LeftLabel{Lam}
            \UnaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, ($\lambda$y.$\gamma$ + y) $\rightarrow$ \{$\gamma$ $\mapsto$ (1 + 1)\}, ($\lambda$y.$\gamma$ + y)}}
      \end{prooftree}
      \caption{}
      \label{fig:finproofexmp:app2}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
      \vspace*{0.49cm}
      \begin{prooftree}
                \AxiomC{}
                \LeftLabel{Num}
                \UnaryInfC{\texttt{\{\}, 1 $\rightarrow$ \{\}, 1}}
                \AxiomC{}
                \LeftLabel{Num}
                \UnaryInfC{\texttt{\{\}, 1 $\rightarrow$ \{\}, 1}}
                \LeftLabel{Bin op}
                \BinaryInfC{\texttt{\{\}, 1 + 1 $\rightarrow$ \{\}, 2}}
                \LeftLabel{Var}
                \UnaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, $\gamma$ $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 2}}

                \AxiomC{}
                \RightLabel{Num}
                \UnaryInfC{\texttt{\{\}, 2 $\rightarrow$ \{\}, 2}}
                \RightLabel{Var}
                \UnaryInfC{\texttt{\{$\gamma$ $\mapsto$ 2\}, $\gamma$ $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 2}}

              \LeftLabel{Bin op}
              \BinaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, $\gamma$ + $\gamma$ $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 4}}
      \end{prooftree}
      \caption{}
      \label{fig:finproofexmp:bin}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
      \vspace*{0.49cm}
      \begin{prooftree}
            \AxiomC{\autoref{fig:finproofexmp:app1}}
              \AxiomC{\autoref{fig:finproofexmp:app2}}
              \AxiomC{\autoref{fig:finproofexmp:bin}}
              \LeftLabel{App}
            \BinaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, ($\lambda$y.$\gamma$ + y) $\gamma$ $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 4}}
          \LeftLabel{App}
          \BinaryInfC{\texttt{\{$\gamma$ $\mapsto$ (1 + 1)\}, ($\lambda$x.($\lambda$y.x + y) x) $\gamma$ $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 4}}
        \LeftLabel{Let}
        \UnaryInfC{\texttt{\{\}, let y = (1 + 1) in ($\lambda$x.($\lambda$y.x + y) x) y $\rightarrow$ \{$\gamma$ $\mapsto$ 2\}, 4}}
      \end{prooftree}
      \caption{}
      \label{fig:finproofexmp:all}
    \end{subfigure}
  \end{mdframed}
  \caption{The proof for the program in \autoref{eq:doneexmp}}
  \label{fig:finproofexmp}
\end{figure}
Notice that the left branch and right branch in \autoref{fig:finproofexmp:bin} are not identical.
The left branch saves the evaluation result such that the right branch only requires a lookup to find $\gamma$.
\end{exmp}


%The $\cup$ operator merges two substitution mappings, choosing the leftmost mapping on duplicates, such that \texttt{\{x $\mapsto$ y, z $\mapsto$ h\} $\cup$ \{x $\mapsto$ m\} $\rightarrow$ \{x $\mapsto$ y, z $\mapsto$ h\}}, such that the semantics of $\cup$ satisfy \autoref{eq:subsemfixed}.
%More generally, a simple rule for App could be defined as in \autoref{fig:simpleapp} which states ``Evaluate \texttt{($\lambda$x.e) z} with the substitution mapping \texttt{S} to \texttt{y} with the substitution mapping $\Sigma$ by first evaluating \texttt{e} with the substitution mapping \texttt{S} $\cup$ \texttt{\{x $\mapsto$ z\}} to \texttt{y} with the substitution mapping $\Sigma$''.
%\begin{figure}[ht]
%\begin{lstlisting}[language=ML,caption={Problematic program},label={lst:problemprog},mathescape=true]
%fun f a = 
  %fun g x = 
    %let a = 20;
    %a + x;
  %(g a) + a;
%f 10;
%\end{lstlisting}
%\end{figure}
%Evaluating \autoref{lst:problemprog}, using the rules proposed in \autoref{fig:rules:env}, yields an interesting case of variable ambiguity.
%At some point the evaluation machine will reach a state with the expression \texttt{(g a) + a} and substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ ($\lambda$x.let a = 20 in (a + x))\}}.
%By the laws of commutativity for addition, it should be possible pick either of the two expressions to evaluate first.
%If any one of the branches in the addition operator are side-effectful, it may become a problem, but this language only deals with pure programs.
%Evaluating the left side of \texttt{(g a) + a} first, yields \texttt{30 + a} with the substitution mapping \texttt{\{a $\mapsto$ 20, g $\mapsto$ $\dots$\}}, and finally \texttt{50}.
%Evaluating the right side of \texttt{(g a) + a} first, yields \texttt{(g a) + 10} with the substitution mapping \texttt{\{a $\mapsto$ 10, g $\mapsto$ $\dots$\}}, and finally \texttt{40}.
%Clearly the order of evaluation is of importance for the result, which is often not the desired semantics.
%If there exists multiple reduction techniques which ensure the same result, the system is called \textit{confluent}.
%The reduction rules for the lambda calculus programs are confluent~\cite{church1936some}, which implies that the current set of rules are not valid evaluation rules for the lambda calculus.
%%If reduction strategies for the lambda calculus are not confluent, programs become difficult to reason with.
%If the rules were to be confluent, any reduction order would result in \texttt{40}.
%The different results are an effect of leaking the substitution mapping to the outside of the lexical scope in which it was used.
%A simple solution could involve saving the substitution mapping once the machine enters an abstraction, then restore the same substitution mapping when it leaves.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\Theta$, f $\rightarrow$ S, ($\lambda$x.e)}}
            %\AxiomC{\texttt{S $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ $\Sigma$, y}}
              %\RightLabel{App}
              %\BinaryInfC{\texttt{$\Theta$, f z $\rightarrow$ $\Theta$, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which does not leak}
    %\label{fig:cleanapp}
%\end{figure}
%\noindent \autoref{fig:cleanapp} is a slightly modified version of \autoref{fig:rules:env:app}, with the difference of requiring the resulting substitution mapping to be the initial substitution mapping.

%\autoref{fig:cleanapp} does not handle closure correctly.
%Evaluating \autoref{lst:closprob} under the rules defined in \autoref{fig:cleanapp} yields yet another problem.
%\begin{figure}[ht]
%\begin{lstlisting}[language=ML,caption={Program with closure},label={lst:closprob},mathescape=true]
%fun g x =
  %fun f z = x + z; // closes x
  %fun run y =
    %let x = 20; $\label{closov}$
    %f y;
  %run 10;
%g 3;
%\end{lstlisting}
%\end{figure}
%When line \autoref{closov} is reached in \autoref{lst:closprob}, then \texttt{x} in the substitution mapping is overwritten by the rules in \autoref{fig:cleanapp}.
%Even if renaming is performed such that all variables become unique, recursive functions still remain a problem.
%Taking a step back and considering what the evaluation strategy which involves eager substitution implies.
%The \textit{first} substitution mapping which reaches an expression, is the substitution mapping which is relevant for that expression.
%By these semantics substitution mappings must be paired with expressions and should now be read as ``\texttt{S, e} are the pair of the environment \texttt{S} necessary to evaluate the expression \textit{e}''.
%Substitutions bound to expressions should remain immutable; whenever an expression is discovered during interpretation, the substitution at that time is the only important substitution mapping.
%The $\cup$ operator for two substitutions should now be inverted, that is, \texttt{S $\cup$ $\Sigma$} should now pick substitutions from \texttt{S} if duplicates occur.
%Furthermore, notice that binding substitutions to expressions also allows substitutions to float up throughout interpretation.
%Now the application rule must implement \autoref{eq:subsemfixed}, seen in \autoref{fig:subsapp}.
%\begin{figure}[ht]
    %\begin{mdframed}[style=bigbox]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\Theta$, f $\rightarrow$ S, ($\lambda$x.e)}}
            %\AxiomC{\texttt{\{x $\mapsto$ (S, z)\} $\cup$ S, e $\rightarrow$ ($\Sigma$ $\backslash$ \{x\}) $\cup$ S, y}}
              %\RightLabel{App}
              %\BinaryInfC{\texttt{$\Theta$, f z $\rightarrow$ $\Sigma$, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which implements \autoref{eq:subsemfixed} and paired substitutions.
    %% (the entire set of rules can be found in \autoref{fig:subrenam})
    %}
    %\label{fig:subsapp}
%\end{figure}
%\begin{remark}
  %An interesting observation is that these semantics introduce expressions with closures as a construct similar to classes in object oriented languages.
  %\texttt{e} is an abstract representation of something, while the environment \texttt{S} is the necessary values to instantiate \texttt{e}, together they can produce some output.
  %Curried functions can also be seen this way, yet it remains an interesting observation.
%\end{remark}

%\noindent Consider the previously discussed program and state in \autoref{eq:tabc}.
%\begin{align}
  %\texttt{\{x $\mapsto$ (S, z), y $\mapsto$ ($\Theta$, x), $\dots$\}($\lambda$x.y) g} \label{eq:tabc}
%\end{align}
%Performing the next two iterations such that the next states become \autoref{eq:tabd} and then finally \autoref{eq:tabe}.
%\begin{gather}
  %\texttt{\{x $\mapsto$ ($\Sigma$, g), y $\mapsto$ ($\Theta$, x), $\dots$\}y} \label{eq:tabd}\\
  %\texttt{$\Theta$ x} \label{eq:tabe}
%\end{gather}
%The program now evaluates as intended which is a consequence of binding all expressions with their environments.

%Surely \autoref{fig:subsapp} solves the problem of lazy substitutions, but alas this is not yet the case.
%To reach a set of rules which will guarantee lazy substitutions and allow call by need, an additional ingredient is needed.
%To understand the need for this ingredient, consider the following valid state \texttt{\{x $\mapsto$ z, y $\mapsto$ x\} ($\lambda$x.y) g} in the context of \autoref{fig:subsapp}.
%When visiting \texttt{($\lambda$x.y)} then the following substitution mapping is applied \texttt{\{x $\mapsto$ g\}}.
%Once \texttt{y} is visited the substitution mapping will be \texttt{\{x $\mapsto$ g, y $\mapsto$ x\}}, substituting again yields \texttt{x} which becomes \texttt{g} which does not necessarily satisfy \texttt{g $\equiv$ z}.
%One could rename all variables in some phase in compilation, but the case for recursion is not handled since a duplicate variable name can occur.
%Consider the program in \autoref{lst:amgprog} which yields an invalid state during interpretation.
%\begin{figure}[ht]
%\begin{lstlisting}[language=ML,caption={Recursive program with ambiguity},label={lst:amgprog},mathescape=true]
%fun f x = 
  %f (x + x); $\label{lst:amgprog:l2}$
%f 10;
%\end{lstlisting}
%\end{figure}
%Once Line \autoref{lst:amgprog:l2} is reached the first time, the program will have the state \texttt{\{$\dots$, x $\mapsto$ 10\}}.
%On the second iteration, the program will have the state \texttt{\{$\dots$, x $\mapsto$ (x + x)\}}, which does not make sense.

%, thus the expression is equivalent to \texttt{($\lambda$x.x) g}, which is not necessarily correct in the case that \texttt{z $\neq$ g}.

%\autoref{fig:cleanapp} does not handle recursion that well, under normal order evaluations, which becomes apparent in expressions such as \texttt{let f = ($\lambda$f.$\lambda$x.if (x == 0) 0 (x + (f (x - 1))) in f f n)}.
%Once the base case is reached (\texttt{x == 0}), then some expression has accumulated in the substitution set \texttt{\{x $\mapsto$ x + ((x - 1) + $\dots$ + ((x - 1) - 1 $\dots$))\}}, which is clearly not valid.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{S $\cup$ \{x $\mapsto$ Sz\}, e $\rightarrow$ $\Sigma$, y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ S, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which works for recursion}
    %\label{fig:recapp}
%\end{figure}
%\noindent The problem of recursion can be solved by never introducing variables, but instead always substituting them thus mapping to the value (\autoref{fig:recapp}).
%An unfortunate consideration which yields \autoref{fig:recapp} unsatisfactory, under the current semantics, is that \texttt{z} can also be bound to an expression which is not a variable, like an application for instance (parameters are never evaluated first in normal order).
%One could recursively apply substitutions to \texttt{z}, but then the \textit{suspense} would be broken.
%\begin{remark}
  %When considering an arbitrarily large expression under call by name (or need), there is no guarantee that the expression will ever be evaluated, that is, a suspended expression being evaluated.
  %Therefore substituting expressions eagerly might compromise performance guarantees.
  %Furthermore consider substituting an expression which contains conditional branches, one branch might be a single expression and the other might be a very large expression.
  %If the program never picks the large expression, it should never have any impact on performance.
%\end{remark}
%\noindent For this method to work then substitutions must be applied lazily.
%The notion of substitutions should now be changed such that substitutions are performed recursively and lazily (\autoref{eq:subsemlaz}).
%\begin{align}
  %&\{ x \mapsto y \} x = y \label{eq:subsemlaz}\\
  %&\{ x \mapsto y \} z = z  \tag*{}\\
  %&S f z = (Sf)(Sz) \tag*{}\\
  %&S (\lambda x.e) = (\lambda x.Se) \label{eq:subsemlaz2} %& (x \mapsto y) \notin S \tag*{}%\\
  %%&S (\lambda x.e) = (\lambda x.(S \backslash \{x \mapsto y\})e) & (x \mapsto y) \in S \tag*{}%\label{eq:subsemlaz2}
%\end{align}
%Note that previously the substitution mappings travelled along with the interpreter while interpreting, but now substitution mappings should travel along variables.
%Expressions should now be modelled by a pair \texttt{S,e}, where \texttt{S} is a substitution mapping which should be considered once \texttt{e} is evaluated. 
%If a substitution mapping \texttt{S} is applied and then a substitution mapping $\Sigma$ is applied, then the resulting set for an expression should become \texttt{S $\cup$ $\Sigma$}.
%The order in which substitution mappings are combined is of importance since the ``most recent'' substitution mapping should contain the lexically ``closest'' variables.
%\begin{remark}
  %The union of two substitution mappings is important since applying them sequentially could damage the asymptotic complexity of substitution.
  %Clearly the variables which are lexically ``closer'' must also be respected. % but this is not of significant importance since $\alpha$-conversion will become necessary later.
  %If $n$ substitutions \texttt{S$_1 \dots$ S$_n$} are applied efficiently ($O(1)$ lookup), then the total number of operations will be $n$, whereas if the substitutions were unioned as they were created the work would be constant.
  %An interpreter would not be of much value if a recursive function which recurses $n$ times takes $O(n^2)$ time, since $O(1 + 2  \dots + n) = O(n \frac{n + 1}{2}) = O(n^2)$.
%\end{remark}
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{\{x $\mapsto$ $\Sigma$,z\} e $\rightarrow$ $\Theta$,y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S,($\lambda$x.e) z $\rightarrow$ $\Theta$,y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{An application rule which has abstracted away substitution mappings}
    %\label{fig:recappsimple}
%\end{figure}
%%\autoref{eq:subsemlaz2} has been introduced for completeness, but can be omitted since it cannot occur in the lazy substitution model, since the only method which carries the substitution mapping into an abstraction is an application.
%Notice that any variables introduced must be so through applications.
%All the substitution work can now be moved into the introduced variable (\autoref{fig:recappsimple}).

%Functions can store intermediate values in two ways, either by partial application or closure.
%When partially applying functions, the intermediate values (and their respective variable bindings) must be saved in a safe and isolated way.
%Surely a system which uses \autoref{fig:recappsimple} is confluent, but, alas this is not the case.
%\texttt{let f = ($\lambda$x.$\lambda$y.x + y) 0 in E} shows a partially applied function \texttt{f} which should have \texttt{x} binded to the expression \texttt{0} in such a way that applying another value to \texttt{f} will finalize the value.
%Letting \texttt{E} satisfy \texttt{x $\notin$ Free(E)} while \texttt{x} occurs in \texttt{E}, that is, \texttt{x} is bound in \texttt{E}, provides an interesting problem for interpreters under call by value.
%If we let \texttt{f} be eager, that is, the binding of \texttt{x} to the expression \texttt{0} is not suspended, then \texttt{f} must associate itself with the substitution \texttt{\{x $\mapsto$ 0\}} and \textit{never} let any other substitutions override the variable \texttt{x}.
%\begin{remark}
  %Ad-hoc eager evaluation, that is, forcing suspended evaluations by use of some symbol is often allowed in functional programming languages.
  %Explicit forcing of suspended computations is a technique often used when creating data structures with good worst case performance.
%\end{remark}
%\noindent Now let \texttt{E} be \texttt{($\lambda$x.let g = f in g) 10} such that an evaluation yields \autoref{eq:red}.
%\begin{align}
  %&\texttt{S = \{x $\mapsto$ 10, f $\mapsto$ \{x $\mapsto$ 0\}($\lambda$y.x + y)\}} \tag*{}\\
  %&\texttt{S(let g = f in g)} \label{eq:red}\\
  %&\texttt{S($\lambda$g.g) S(f)} \tag*{}\\
  %&\texttt{($\lambda$g.S(g)) \{x $\mapsto$ 0\}($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{(\{x $\mapsto$ 0\} $\cup$ S)($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{S($\lambda$y.x + y)} \tag*{}\\
  %&\texttt{\{$\dots$, x $\mapsto$ 10\}($\lambda$y.x + y)} \tag*{}
%\end{align}
%\autoref{eq:red} is a textbook closure and partial application problem in disguise.
%Consider the following valid state \texttt{\{x $\mapsto$ z, y $\mapsto$ x\} ($\lambda$x.y) g} in the context of \autoref{fig:cleanapp}.
%When visiting \texttt{($\lambda$x.y)} then the following substitution mapping is applied \texttt{\{x $\mapsto$ g\}}.
%Once \texttt{y} is visited the substitution mapping will be \texttt{\{x $\mapsto$ g, y $\mapsto$ x\}}, thus the expression is equivalent to \texttt{($\lambda$x.x) g}, which is not necessarily correct in the case that \texttt{z $\neq$ g}.
%Both problems are an effect of variable ambiguity.

%We almost have all the building blocks in place now.
%Temporarily assuming that substitutions occur eagerly yields interesting results which will help finalize the model.
%Let \texttt{((($\lambda$x.$\lambda$y.$\lambda$x.x) 1) 2) 3} be an expression which is to be evaluated.
%If we follow the rules in \autoref{eq:subsemlaz} then the first substitution will yield.
%\[
  %\texttt{\{x $\mapsto$ 3\}(($\lambda$y.$\lambda$x.x) 1) 2} \rightarrow \texttt{(($\lambda$y.$\lambda$x.3) 1) 2}
%\]
%Adding a rule which allows only the innermost substitution to be captured yields the two following rules (\autoref{eq:subsemfixed2}) instead of \autoref{eq:subsemlaz2}.
%\begin{align}
  %&S (\lambda x.e) = (\lambda x.Se) \label{eq:subsemfixed2} & (x \mapsto y) \notin S \tag*{}\\
  %&S (\lambda x.e) = (\lambda x.(S \backslash \{x \mapsto y\})e) & (x \mapsto y) \in S \tag*{}%\label{eq:subsemlaz2}
%\end{align}


%\autoref{fig:recappsimple} solves the problem of not letting substituted to values have their meaning changed, by pairing expressions with their entire substitution mapping.

%\autoref{fig:recappsimple} induces rather complicated programs to reason with since substitution mapping can occur within substitution mappings.
%Attempting to extend \autoref{fig:recappsimple} further, to solve variable ambiguity, sacrifices algorithmic elegance and simplicity further.
%Taking a step back and considering what closures and partial application need to behave deterministically is rooted in solving variable ambiguity.
%Instead, if all variables are unique, ambiguity cannot occur.
%By renaming all variables to a unique name when they are introduced, the evaluation model can be simplified significantly and allows \textit{any} of the aforementioned application rules (some of which imply eager substitution).
%The operation of renaming is called an $\alpha$-conversion.
%The shortcomings of \autoref{fig:simpleapp} become more apparent once one begins to consider more exotic evaluation strategies, namely call by need.
%Looking at call by need from a philosophical perspective, indicates that reduction rules must support returning information about newly computed variable values ``up'' through the evaluation tree.
%Furthermore when returning these modified variables up through the program tree, all variables which point to the same expression must also be updated.
%\\
%The next step in correctly evaluating the lambda calculus is applying \textit{$\alpha$-conversions} which is the operation of renaming.
%\subsubsection{$\alpha$-conversions}
%The $\alpha$-conversion mapping can occur as the substitution mapping, that is \texttt{\{x $\mapsto$ $\gamma_1$, y $\mapsto$ $\gamma_2$\}}; $\alpha$-conversions are philosophical constructs more than materialized mappings.
%$\alpha$-conversions guarantee what is called \textit{$\alpha$-equivalence} which is the notion of semantic equivalence.
%For instance \texttt{$\lambda$x.x} is $\alpha$-equivalent with \texttt{$\lambda\gamma.\gamma$} since both expressions are semantically equivalent.
%%$\alpha$-conversions solve some critical problems such as closures and recursion when evaluating the lambda calculus.
%%$\alpha$-conversions should also be non-destructive but still be context aware such that when leaving an abstraction the remaining substitution mapping is a superset of the substitution mapping when entering.
%%More formally if \texttt{S, e $\rightarrow$ $\Sigma$, e'} then $\{x \,\,|\,\, (x \mapsto y) \in S\} \subseteq \{x \,\,|\,\, (x \mapsto y) \in \Sigma\}$.
%\begin{lstlisting}[language=ML,caption={Recursive addition function},label={lst:recalpha},mathescape=true]
%let f = ($\lambda$f'.$\lambda$x.
    %if (x = 0) x else f' f' ((x - 1) + (x - 1))) in
%f f 10
%\end{lstlisting}
%%A strong guarantee that can be made by tuning the evaluation strategy which is particularly useful for $\alpha$-conversion algorithms is that \textit{any} returned value has had \textit{every} term that it contains visited.
%%An algorithm for performing $\alpha$-conversions can be implemented that picks a new variable name from any infinite domain when an abstraction has had a value applied to it and replaces future encountered variables with the new one, such an algorithm only works if the guarantee of visiting every term is made.
%%The algorithm should also introduce the applied value to the substitution set through the alpha converted name.
%\noindent An $\alpha$-conversion algorithm can be implemented such that when a new variable is introduced through an abstraction, a new name for the variable is given.
%More formally; Let $V_1$ be the domain of variables in the program and $V_2$ be the infinite domain for variable names that satisfies $V_1 \cap V_2 = \emptyset$, then when a new variable \texttt{x} is discovered, replace it with some $\gamma \in V_2$ and let $V_2 = V_2 \backslash \{\gamma\}$. 
%For instance, let \texttt{($\lambda\gamma_1$.$\lambda\gamma_2$.if ($\gamma_2$ = 0) $\gamma_2$ else $\gamma_1$ $\gamma_1$ (($\gamma_2$ - 1) + ($\gamma_2$ - 1)))} be the $\alpha$-converted version of \texttt{f} in \autoref{lst:recalpha}.
%It should become clear that an $\alpha$-conversion algorithm must also follow the reduction order or else one can force terrible runtime in a call by name (or need) environment by creating purposeless terms which are never executed but are converted.
%$\alpha$-conversions must be suspended until it is needed; let $\alpha E$ denote the $\alpha$-conversion for some conversion mapping $\alpha$ on a lambda expression $E$ which lazily $\alpha$-converts $E$.
%The replacement rules for $\alpha$-conversions are the same as for substitutions.
%\begin{align}
  %&\texttt{[x $\mapsto$ y]x = y} &\label{eq:alpharep}\\
  %&\texttt{[x $\mapsto$ y]z = z} &\tag*{}\\
  %&\texttt{$\alpha$(f z) = ($\alpha$f)($\alpha$z)} &\tag*{}\\
  %&\texttt{$\alpha$($\lambda$x.e) = ($\lambda$x.$\alpha$e)} & \texttt{(x $\mapsto$ y) $\notin$ $\alpha$} \tag*{}\\
  %&\texttt{$\alpha$($\lambda$x.e) = ($\lambda$x.($\alpha\backslash$[x $\mapsto$ y])e)} & \texttt{(x $\mapsto$ y) $\in$ $\alpha$} \label{eq:alprepsub}
%\end{align}

%We almost have all the building blocks in place now.
%The problem of ambiguity between \textit{different} variables, that is, variables different places in the program with the same name and in recursion, can now be solved.
%Once again, not breaking suspension remains as an interesting value.
%In \autoref{fig:subrenam} the \texttt{fresh} function picks a new variable from $V_2$ and updates $V_2$ such that picked variable no longer exists in $V_2$.
%\begin{figure}[ht]
    %\begin{mdframed}[style=bigbox]
        %\vspace*{0.49cm}
        %\begin{subfigure}[b]{0.38\textwidth}
            %\begin{prooftree}
                %\AxiomC{}
                %\RightLabel{Abs}
                %\UnaryInfC{\texttt{S, ($\lambda$x.e) $\rightarrow$ S, ($\lambda$x.e)}}
            %\end{prooftree}   
            %\caption{}
          %\label{fig:rules:freshenv:abs}
        %\end{subfigure}
        %\begin{subfigure}[b]{0.58\textwidth}
            %\begin{prooftree}
              %\AxiomC{\texttt{Sx $\equiv$ x}}
                %\RightLabel{Var (terminal)}
                %\UnaryInfC{\texttt{S, x $\rightarrow$ S, x}}
            %\end{prooftree}   
            %\caption{}
          %\label{fig:rules:freshenv:varterm}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
        %\vspace*{0.49cm}
            %\begin{prooftree}
              %\AxiomC{\texttt{$\Theta$, y $\rightarrow$ $\Sigma$, e}}
                %\RightLabel{Var}
                %\UnaryInfC{\texttt{\{x $\mapsto$ ($\Theta$, y)\} $\cup$ S, x $\rightarrow$ \{x $\mapsto$ ($\Theta$, e)\} $\cup$ $\Sigma$, e}}
            %\end{prooftree}   
            %\caption{}
          %\label{fig:rules:freshenv:var}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
          %\vspace*{0.4cm}
              %\vspace*{0.4cm}
              %\begin{prooftree}
                %\AxiomC{\texttt{$\Theta$, f $\rightarrow$ S, ($\lambda$x.e)}}
                %\AxiomC{\texttt{\{x $\mapsto$ (S, z)\} $\cup$ S, e $\rightarrow$ ($\Sigma$ $\backslash$ \{x\}) $\cup$ S, y}}
                  %\RightLabel{App}
                  %\BinaryInfC{\texttt{$\Theta$, f z $\rightarrow$ $\Sigma$, y}}
              %\end{prooftree}   
          %\caption{An application rule which binds the current environment to expressions}
          %\label{fig:rules:freshenv:app}
        %\end{subfigure}
    %\end{mdframed}
    %\caption{A set of rules which support the desired semantics}
    %\label{fig:subrenam}
%\end{figure}
%\begin{figure}[ht]
%\end{figure}

%First consider the implications of an eager $\alpha$-conversion strategy in \autoref{eq:alpharep}.
%Applying $\alpha$-conversions in applications eagerly ensures that all references to the applied to variable is are replaced (\autoref{fig:simpleappalpha}).
%Also notice that \autoref{eq:alprepsub} ensures that only variables that reference the replaced variable are renamed.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\gamma$ = fresh \,\,\,\,\, S $\cup$ \{$\gamma$ $\mapsto$ z\}, [x $\mapsto$ $\gamma$]e $\rightarrow$ $\Sigma$, y}}
              %\RightLabel{App}
              %\UnaryInfC{\texttt{S, ($\lambda$x.e) z $\rightarrow$ $\Sigma$, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{The simple application rule with $\alpha$-conversions}
    %\label{fig:simpleappalpha}
%\end{figure}
%The semantics of eager $\alpha$-conversion implies that once an $\alpha$-conversion is converting an expression, all free variables which the $\alpha$-conversion addresses should be renamed eagerly.
%Moreover, this implies that if some $\alpha$-conversion $\alpha_1$ suspends conversion of an expression and then some other conversion $\alpha_2$ suspends a conversion on the same expression, $\alpha_1$'s conversions should take precedence.
%That is, suspended conversions cannot have their conversions overwritten by other conversions such that \texttt{[x $\mapsto$ y] $\cup$ [x $\mapsto$ z, $\gamma$ $\mapsto$ k] = [x $\mapsto$ y, $\gamma$ $\mapsto$ k]}.

%\begin{lstlisting}[language=ML,caption={$\alpha$-conversion of a program with variable name collisions},label={lst:ambg},mathescape=true]
%($\lambda$x.$\lambda$y.$\lambda$x.x + y) $\rightarrow$ ($\lambda\gamma$.$\lambda\theta$.$\lambda\sigma$.$\sigma$ + $\theta$)
%\end{lstlisting}

%In \autoref{subfig:red4} the application which contains \texttt{$\lambda$f'} and \texttt{f'} as children (the red node) must $\alpha$-convert \texttt{f'} in ``this scope''.
%If \texttt{f'} is not $\alpha$-converted a circular dependency will arise $S = \{ \gamma \mapsto \texttt{f'} \}, \alpha = [ \texttt{f'} \mapsto \gamma ]$.

%\begin{exmp}
  %\autoref{fig:red:exmp} is an example of an iteration of \autoref{lst:recalpha}.
%\begin{figure}
    %\centering
    %\begin{subfigure}[b]{0.49\textwidth}
        %\centering
        %\begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                %\node[draw=black] (app1) {\texttt{app}};
                    %\node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    %\path[-] (app1) edge node[left] {} (var1);

                    %\node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    %\path[-] (app1) edge node[left] {} (app2);
                        %\node[draw=red, below left = of app2] (f1) {\texttt{f}};
                        %\path[-] (app2) edge node[left] {} (f1);
                        %\node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        %\path[-] (app2) edge node[left] {} (f2);

                %\node[draw,scale=1.3,fill=none,rectangle,fit=(app1)(f1)(var1)](fstB){};
                %\path (fstB.north east) -- (fstB.south east) coordinate[midway] (fstP);
        %\end{tikzpicture}
        %\caption{
          %\texttt{S = \{f $\mapsto$ ($\lambda$f'.$\lambda$x.if (x = 0) x else f' f' (x - 1))\}}
        %}
    %\end{subfigure}
    %\begin{subfigure}[b]{0.49\textwidth}
        %\centering
        %\begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                %\node[draw=black] (app1) {\texttt{app}};
                    %\node[draw=black, below right = of app1] (var1) {\texttt{10}};
                    %\path[-] (app1) edge node[left] {} (var1);

                    %\node[draw=black, below left = of app1] (app2) {\texttt{app}};
                    %\path[-] (app1) edge node[left] {} (app2);
                        %\node[draw=red, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        %\path[-] (app2) edge node[left] {} (lam1);
                            %\node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            %\path[-] (lam1) edge node[left] {} (lam2);
                                %\node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                %\path[-] (lam2) edge node[left] {} (app3);
                                    %\node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    %\path[-] (app3) edge node[left] {} (app4);
                                        %\node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        %\path[-] (app4) edge node[left] {} (app5);
                                            %\node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            %\path[-] (app5) edge node[left] {} (if);
                                            %\node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            %\path[-] (app5) edge node[left] {} (ifc);
                                        %\node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        %\path[-] (app4) edge node[left] {} (ift);
                                    %\node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ((x - 1) + (x - 1))}};
                                    %\path[-] (app3) edge node[left] {} (iff);
                        %\node[draw=black, below right = of app2] (f2) {\texttt{f}};
                        %\path[-] (app2) edge node[left] {} (f2);

                %\node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](sndB){};
                %%\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            %\end{tikzpicture}
            %\caption{
                %\texttt{$\Sigma$ = S}
            %}
    %\end{subfigure}
    %%\tikz[overlay,remember picture]{\draw [->] (fstP) -- (fstP-|sndP);}
    %\begin{subfigure}[b]{0.49\textwidth}
        %\centering
        %\begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                                %\node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                    %\node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    %\path[-] (app3) edge node[left] {} (app4);
                                        %\node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        %\path[-] (app4) edge node[left] {} (app5);
                                            %\node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            %\path[-] (app5) edge node[left] {} (if);
                                            %\node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            %\path[-] (app5) edge node[left] {} (ifc);
                                        %\node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        %\path[-] (app4) edge node[left] {} (ift);
                                    %\node[draw=blue, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    %\path[-] (app3) edge node[left] {} (iff);
                %\node[draw,fill=none,scale=1.3,rectangle,fit=(app3)(if)(ifc)(iff)](fouB){};
                %%\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            %\end{tikzpicture}
            %\caption{
                %\\
                %\texttt{$\Theta_0$ = \{f' $\mapsto$ ($\Sigma$, f)\} $\cup$ $\Sigma$}\\
                %\texttt{$\Theta$ = \{x $\mapsto$ ($\Theta_0$, 10)\} $\cup$ $\Theta_0$}
            %}
    %\end{subfigure}
    %\begin{subfigure}[b]{0.49\textwidth}
        %\centering
        %\begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.15cm and 0.15cm]
                %\node[draw=blue] (app1) {\texttt{app}};
                    %\node[draw=black, below right = of app1] (var1) {\texttt{x - 1}};
                    %\path[-] (app1) edge node[left] {} (var1);

                    %\node[draw=purple, below left = of app1] (app2) {\texttt{app}};
                    %\path[-] (app1) edge node[left] {} (app2);
                        %\node[draw=black, below left = of app2] (lam1) {\texttt{$\lambda$f'}};
                        %\path[-] (app2) edge node[left] {} (lam1);
                            %\node[draw=black, below = of lam1] (lam2) {\texttt{$\lambda$x}};
                            %\path[-] (lam1) edge node[left] {} (lam2);
                                %\node[draw=black, below = of lam2] (app3) {\texttt{app}};
                                %\path[-] (lam2) edge node[left] {} (app3);
                                    %\node[draw=black, below left = of app3] (app4) {\texttt{app}};
                                    %\path[-] (app3) edge node[left] {} (app4);
                                        %\node[draw=black, below left = of app4] (app5) {\texttt{app}};
                                        %\path[-] (app4) edge node[left] {} (app5);
                                            %\node[draw=black, below left = of app5] (if) {\texttt{if}};
                                            %\path[-] (app5) edge node[left] {} (if);
                                            %\node[draw=black, below right = of app5] (ifc) {\texttt{x = 0}};
                                            %\path[-] (app5) edge node[left] {} (ifc);
                                        %\node[draw=black, below right = of app4] (ift) {\texttt{x}};
                                        %\path[-] (app4) edge node[left] {} (ift);
                                    %\node[draw=black, below right = 0.15cm and 0.6cm of app3] (iff) {\texttt{f' f' ($\dots$)}};
                                    %\path[-] (app3) edge node[left] {} (iff);
                        %\node[draw=black, below right = of app2] (f2) {\texttt{f'}};
                        %\path[-] (app2) edge node[left] {} (f2);
                %\node[draw,fill=none,scale=1.3,rectangle,fit=(app1)(if)(ifc)(var1)(iff)](fouB){};
                %%\path (sndB.north west) -- (sndB.south west) coordinate[midway] (sndP);
            %\end{tikzpicture}
            %\caption{
                %\texttt{$\Delta = \Theta$}\\
            %}
            %\label{subfig:red4}
    %\end{subfigure}
    %\caption{Example of substitutions in recursive function.}
    %\label{fig:red:exmp}
%\end{figure}
%\end{exmp}
%%\tikz[overlay,remember picture]{\draw[-latex,thick] (fstB) -- (fstB-|sndB) node[midway,below,text width=0.5cm]{};} 

%\subsubsection{Lazy evaluation}
%Call by need requires an additional effort, since it requires more than a particular evaluation order to implement~\cite{levy1988sharing}.
%%Substitutions can be rewritten when a variable \texttt{x} maps to an application such that for some substitution mapping of the form $\{ \dots, \texttt{x} \mapsto Y E \}$ then the substituted to value is reduced.
%%When two substitutions are combined one should always choose the most reduced of the two, which is trivial since it becomes a matter of picking the ''newest`` version of the substituted value.
%An important observation which is discussed in~\cite{levy1988sharing} is that of \textit{object} duplication.
%Objects are the substituted to values which are lambda calculus terms.
%The term object is used to emphasize the uniqueness of lambda calculus terms.
%%A problem when sharing evaluation in the substitution mapping is that of transitive substitutions.
%%Transitive substitutions are not expressible since that causes non-linear evaluation times on some inputs.
%%Naturally when new mappings are introduced the mapped to values can be duplicated instead such that instead of $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto \gamma_1 \}$ then $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ which violates call by need.
%A lambda calculus expression $E$ can be labelled with some name $a$ when found in the evaluation process such that when some value with the label $a$ is evaluated then all duplications of $E$ can share the computed result, effectively adding another type of mapping.
%\begin{remark}
    %Sharing can also be implemented in a way which promotes more imperative constructs, such as mutable data.
    %When some object $E$ is discovered it is declared as a mutable pointer by the interpreter (or natively if implemented in a lazy language) such that all duplications effectively point to the same object.
    %An implementation which builds upon pointers are explored in Chapter 12 in~\cite{peyton1987implementation}.
%\end{remark}

%The labelling technique can be realized quite elegantly whilst also simplifying the rule set further.
%Attempting to implement a labelling technique does not resolve the problem, the semantics in \autoref{fig:subrenam} do for instance not behave accordingly in scenarios which build upon sharing computation in deeply nested substitution mappings.
%The state in \autoref{eq:mulsub} poses a problem since the forcing of \texttt{y} from forcing \texttt{x} yields \autoref{eq:mulsubstep2}, and finally forces the computation of \texttt{y} once again.
%\begin{align}
  %&\texttt{\{x $\mapsto$ (\{y $\mapsto$ 1 + 1\}, y), z $\mapsto$ (\{y $\mapsto$ 1 + 1\}, y)\}x + (($\lambda$y.z) 0)} \label{eq:mulsub}\\
  %&\texttt{\{x $\mapsto$ ($\dots$, 2), z $\mapsto$ (\{y $\mapsto$ 1 + 1\}, y), y $\mapsto$ 2\}2 + (($\lambda$y.z) 0)} \label{eq:mulsubstep2}
%\end{align}
%Observe that forcing \texttt{z} recomputes \texttt{y}.
%Sharing the computed value of \texttt{y} is not an option either, since \texttt{y} may not be the same \texttt{y}, which is the reason for the introduction of bound substitution mappings in the first place.
%Fortunately we have yet to apply $\alpha$-conversions to the problem.
%Instead of binding the substitution mappings to expressions, $\alpha$-conversions should be bound to expressions.
%Bound $\alpha$-conversions have a very simple set of properties, such as 

%\begin{figure}
%\begin{lstlisting}[language=ML,caption={Program which benifits from lazy evaluation},label={lst:lazyevalprog},mathescape=true]
%fun double x = x + x;
%let y = double 10;
%fun add z k = z + k;
%add y y;
%\end{lstlisting}
%\end{figure}
%\autoref{lst:lazyevalprog} is a program which under call by name computes \texttt{y} two times.
%By assigning a unique name to the expression of \texttt{y}, \texttt{a} for instance, the unique expression can be tracked.
%When \texttt{y + y} is computed, the first evaluation of \texttt{y} changes \texttt{a} in the substitution mapping such that \texttt{\{$\dots$, a $\mapsto$ 20\}}.
%%Furthermore, notice that the phrase ``assigning a unique name'' is exactly what $\alpha$-conversion does.
%By these rules sharing can be implemented very elegantly by also noticing that variable references are in fact, a dependency tree.
%Let a valid substitution mapping be \texttt{\{x $\mapsto$ 10 + 2, y $\mapsto$ (x + 1), z $\mapsto$ (y + 2)\}} with the originating program in \autoref{eq:lazyexmp}.
%\begin{align}
  %\texttt{($\lambda$x.($\lambda$y.($\lambda$z.E) (y + 2)) (x + 1)) (10 + 20)} \label{eq:lazyexmp}
%\end{align}
%If the lambda calculus sub-program in \texttt{E} forces \texttt{y}, then the substitution mapping should be updated with \texttt{\{y $\mapsto$ 13\}} which implies that x is also forced \texttt{\{x $\mapsto$ 12\}}.

%Observe that the App rule in \autoref{fig:subrenam} is the only rule which introduces \textbf{new} variables to the substitution mapping.
%By using the App rule, one can introduce a variable which points to an expression.
%A reference is created to the originating expression, that is, two names which point to the same reference.
%For some function \texttt{f} which duplicates expressions and some state \texttt{\{x $\mapsto$ ($\dots$, e), f $\mapsto$ ($\dots$, ($\lambda$y.$\lambda$z.y+z))\}f x x} the only applicable rule is App.
%Using the App rule two times yields a new state \texttt{\{x $\mapsto$ ($\dots$, e), f $\mapsto$ $\dots$, y $\mapsto$ ($\dots$, x), z $\mapsto$ ($\dots$, x)\}}.
%\autoref{fig:expdep} gives clearer insight into how the expressions depend on each other.
%If any expression in \autoref{fig:expdep} is forced, then everything above will be forced.
%For instance if \texttt{y} was forced but \texttt{z} was not, then \texttt{z} would have to evaluate the reference to \texttt{x} which has been evaluated and added to the substitution mapping by \autoref{fig:rules:freshenv:var} since \texttt{x} is an ancestor of \texttt{y}.
%\begin{figure}
  %\centering
  %\begin{tikzpicture}[ scale=0.8, every node/.style={scale=0.8}, node distance = 0.25cm and 0.25cm]
        %\node[draw=black] (e) {\texttt{e}};

          %\node[draw=black, below = of e] (x) {\texttt{x}};
          %\path[->] (x) edge node[left] {} (e);
            
              %%\node[draw=black, below left = of x] (gamma1) {$\gamma_1$};
              %%\path[->] (gamma1) edge node[left] {} (x);

              %\node[draw=black, below left = of x] (y) {\texttt{y}};
              %\path[->] (y) edge node[left] {} (x);

            %%\node[draw=black, below right = of x] (gamma2) {$\gamma_2$};
            %%\path[->] (gamma2) edge node[left] {} (x);

              %\node[draw=black, below right = of x] (z) {\texttt{z}};
              %\path[->] (z) edge node[left] {} (x);

          %\node[draw,fill=none,scale=1.3,rectangle,fit=(e)(z)(y)](fbb){};
  %\end{tikzpicture}
  %\caption{Expression dependencies}
  %\label{fig:expdep}
%\end{figure}
%We have now arrived at semantics which are very similar to the semantics described in~\cite{launchbury1993natural} (except concepts such as lazy substitutions).


%Let $S = \{ \gamma_1 \mapsto E, \gamma_2 \mapsto E \}$ be the system without labels, then let \autoref{eq:labelsystem} be the labelled call by need system.
%\begin{align}
    %\label{eq:labelsystem}
    %S &= \{ \gamma_1 \mapsto a, \gamma_2 \mapsto a \}\\
    %\Omega &= \{ a \mapsto E \} \tag*{}
%\end{align}
%By \autoref{eq:labelsystem} it should becomes clear that object duplication is mitigated by ``merging'' references.

%$\alpha$-conversion loses some of it's purpose when a labelling approach is considered.
%\begin{lstlisting}[language=ML,caption={Program},label={lst:labeltest},mathescape=true]
%fun id a = a;
%fun f g a =
  %id (g a a);
%\end{lstlisting}




%%The labelled system can evaluate expressions in parallel by implementing locking (or cooperative yielding) on the label level (in \autoref{eq:labelsystem} evaluating $\gamma_1$ and $\gamma_2$ in paralell would lock $a$).
%%The only scenario where mutual evaluation of an expression is possible, is when two other expressions can access (either through abstraction or closure) the same value.
%%\\
%An evaluation written $S, \Omega, \alpha, x \rightarrow \Theta, e$ means $x$ evaluates to $e$ with the resulting label mapping $\Theta$, under some substitution mapping $S$, label mapping $\Omega$ and $\alpha$-conversion mapping $\alpha$.
%$Sx$ means that variable $x$ is replaced with whatever the substitution map maps $x$ to if $x \in S$ otherwise $Sx = x$, the same operation counts for $\alpha$.
%$\Omega l \rightarrow k$ should be read as, the expression which $l$ points to should be evaluated and be bound to $k$.
%In a call by value environment $\alpha x$ can eagerly convert since the value must be evaluated eagerly.
%In a call by name (or need) environment $\alpha x$ should remain suspended until needed like discussed in this section.
%An implementation (\autoref{lst:cbn}) of the call by name reduction algorithm \autoref{fig:cbn} can easily be extended with call by need by the aforementioned modifications.
%\begin{remark}
    %In the implementation the \texttt{Map} constructor is the usual implementation of the Map data structure.
    %Also the implementation is generalized in that it does not describe operations such as addition or whether it is a call by need or name environment.
%\end{remark}
%\begin{lstlisting}[language=ML,caption={Implementation of call by name},label={lst:cbn},mathescape=true]
%type Three a b c =
    %| Triple a b c;
%type Term =
    %| Abstraction Int Term
    %| Var Int 
    %| Application Term Term
%;

%fun reduceApp S alpha e1 e2 =
    %match e1 ->
        %| Abstraction x e' ->
            %let newvar = newvar in
            %let newS = 
                %Map newvar (apply alpha e2) in
            %let S2 = union S newS in
            %let newA =
                %Map x newvar in
            %let alpha2 = union alpha newA in
            %reduce newS newA e1
        %| e' -> match (reduce S alpha e')
            %| Triple S' alpha' e'' ->
                %let Sn = union S S' in
                %let alphan = union alpha alpha' in
                %let en = Application e'' e2
                %reduce Sn alphan en

%fun reduce S alpha e =
    %match e 
        %| Abstraction x e' -> 
            %reduce S alpha e'
        %| Var x -> match (find x S) 
            %| Just e' -> Triple S alpha e'
            %| Nothing -> Triple S alpha x
        %| Application e1 e2 -> 
            %reduceApp S alpha e1 e2
%\end{lstlisting}
%\begin{figure}[ht]
    %\begin{mdframed}[style=bigbox]
        %\vspace*{0.4cm}
        %%\begin{subfigure}[b]{0.30\textwidth}
            %%\begin{prooftree}
                %%\AxiomC{$S, \alpha, S \alpha x \rightarrow \Sigma, e$}
                %%\RightLabel{Var}
                %%\UnaryInfC{$S, \alpha, x \rightarrow \Sigma, e$}
            %%\end{prooftree}   
            %%\label{eq:prooftree:ident}
            %%\caption{}
        %%\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{}
                %\RightLabel{Abs}
                %\UnaryInfC{$S,\alpha,(\lambda x.e) \rightarrow S,\alpha,(\lambda x.e)$}
            %\end{prooftree}   
            %\label{eq:prooftree:abs}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S \cup \{ \gamma \mapsto e \},\alpha \cup \{ x \mapsto \gamma \} , e \rightarrow \Sigma, w$}
                %%\AxiomC{$S \alpha x \not\equiv \alpha x$}
                %\RightLabel{Var share (pointer)}
                %\UnaryInfC{$S \cup \{  \gamma \mapsto e \}, \alpha \cup [ x \mapsto \gamma ], x \rightarrow \Sigma, e \hspace*{1cm} e := w$}
            %\end{prooftree}   
            %\label{eq:prooftree:point}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S,\alpha, y \rightarrow \Sigma, (\lambda x . e)$}
                %\AxiomC{$\Sigma \cup \{ \gamma \mapsto \Sigma \alpha p \},\alpha \cup [ x \mapsto \gamma ],e \rightarrow \Delta, w \,\,\, \gamma = \texttt{newvar}$}
                %\RightLabel{App}
                %\BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Delta, w$}
            %\end{prooftree}   
            %\label{eq:prooftree:app}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S,\alpha,y \rightarrow \Sigma, f$}
                %\AxiomC{$f \not\equiv (\lambda x . e)$}
                %\RightLabel{App (Reduce left first)}
                %\BinaryInfC{$S, \alpha,(y \,\, p) \rightarrow \Sigma, (f \,\, p)$}
            %\end{prooftree}   
            %\label{eq:prooftree:appleft}
            %\caption{}
        %\end{subfigure}
    %\end{mdframed}
    %\caption{Reduction rules for the call by need lambda calculus}
    %\label{fig:cbn}
%\end{figure}
%\clearpage
%\begin{exmp}
%Consider the following program which duplicates an object.
%\begin{lstlisting}[language=ML,caption={Object duplication},label={lst:objdup},mathescape=true]
%let d = ($\lambda$f.$\lambda$x.f x x) in
%d ($\lambda$z.$\lambda$y.z + y) (($\lambda$i.i) 0)
%\end{lstlisting}
%The evaluation of \autoref{lst:objdup} can be seen in \autoref{eq:objdup} (the underlying call by need implementation is not of interest).
%\begin{align}
    %\label{eq:objdup}
    %&\texttt{g s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\\
    %\rightarrow &(\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y})\texttt{ s s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0} \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}]\tag*{}\\
    %\rightarrow &(\lambda \texttt{y}.\texttt{z + y})\texttt{ s }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto s \}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}]\tag*{}\\
    %\rightarrow &\texttt{c + q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    %\rightarrow &\texttt{+ s q }\{ \texttt{g} \mapsto (\lambda \texttt{z}.\lambda \texttt{y}.\texttt{z + y}), \texttt{s} \mapsto (\lambda\texttt{i}.\texttt{i}) \texttt{0}, \texttt{c} \mapsto \texttt{s}, \texttt{q} \mapsto \texttt{s}\}[\texttt{f} \mapsto \texttt{g}, \texttt{x} \mapsto \texttt{s}, \texttt{z} \mapsto \texttt{c}, \texttt{y} \mapsto \texttt{q}]\tag*{}\\
    %\rightarrow &\texttt{+ force(s) q }\{ \dots \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{+ force(}(\lambda\texttt{i}.\texttt{i})0\texttt{) q } \{ \dots \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{+ force(0) q }\{ \dots \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{+ 0 q }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{+ 0 s }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{+ 0 0 }\{ \dots, s \mapsto 0 \}[\dots]\tag*{}\\
    %\rightarrow &\texttt{0 }\{ \dots, s \mapsto 0 \}\tag*{}
%\end{align}
%\end{exmp}

\subsubsection{Garbage collection}
In functional programming languages unused variables and expressions accumulate during execution.
In the context of the rules which have been presented in this section, the heap will inevitably accumulate unused values.
It is argued in~\cite{launchbury1993natural} that garbage collection remains interesting to introduce in the semantics of the system, since it allows reasoning with space usage in an abstract way.
In imperative languages which do not make a big deal of side-effects such as \texttt{C}, unused values are managed and released manually.
Managing unused variables in a purely lazy functional programming language is not ergonomic, and implies a side-effect, thus the language is no longer pure.

A naive garbage collector could involve letting the Let rule, release references as in \autoref{fig:letrelease}. 
\begin{figure}[ht]
  \begin{mdframed}
    \begin{prooftree}
      \AxiomC{\texttt{S $\cup$ \{$\gamma$ $\mapsto$ e\}, \{x $\mapsto$ $\gamma$\}y $\rightarrow$ $\Theta$, z}}
      \AxiomC{\texttt{$\gamma$ = fresh}}
        \RightLabel{Let}
        \BinaryInfC{\texttt{S, let x = e in y $\rightarrow$ $\Theta$ $\backslash$ \{$\gamma$ $\mapsto$ e'\}, z}}
    \end{prooftree}   
  \end{mdframed}
  \caption{A Let rule which cleans up after itself}
  \label{fig:letrelease}
\end{figure}
A garbage collection rule as described in \autoref{fig:letrelease} works, but is inadequate since it does not allow removal of unused values at \textit{any} time, thus it does not let us reason with recursive programs which run in constant space.
Introducing a garbage collection rule which can be placed at any one step of the proof requires some additional work for various reasons.
Foremost, introspection of expressions becomes necessary since the rule must determine what gets to stay in the heap.
Furthermore, rules which branch such as the App rule and Bin op rule requires the tracing of expressions which are pending.
More precisely, when evaluating \texttt{l} in the App rule in \autoref{fig:rules:env:app}, the expression \texttt{p} should not be released since it must be present for the right branch (\texttt{\{x $\mapsto$ p\}e}).
All branching rules must record expressions which are needed for further branches, this is accomplished by introducing a set of expressions \texttt{N}, such that all evaluations are written $\rightarrow_{\texttt{N}}$ (\autoref{fig:branchrules}).
\begin{figure}[ht]
  \begin{mdframed}[style=style1]
    \begin{subfigure}[b]{1\textwidth}
    \vspace*{0.19cm}
          \begin{prooftree}
            \AxiomC{\texttt{$\Gamma$, f $\rightarrow_{\texttt{(N $\cup$ \{z\})}}$ $\Theta$, ($\lambda$x.e)}}
            \AxiomC{\texttt{$\Theta$, \{x $\mapsto$ z\}e $\rightarrow_{\texttt{N}}$ $\Sigma$, l}}
              \RightLabel{App}
              \BinaryInfC{\texttt{$\Gamma$, f z $\rightarrow_{\texttt{N}}$ $\Sigma$, l}}
          \end{prooftree}   
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
    \vspace*{0.49cm}
      \begin{prooftree}
          \AxiomC{\texttt{$\Theta$, x $\rightarrow_{\texttt{(N $\cup$ \{y\})}}$ $\Sigma$, n}}
          \AxiomC{\texttt{$\Sigma$, y $\rightarrow_{\texttt{N}}$ S, t} $\,\,\,\,\,$ \texttt{$\oplus \in \{+,-,*,\backslash, =\}$}}
            \RightLabel{Bin op}
            \BinaryInfC{\texttt{$\Theta$, x $\oplus$ y $\rightarrow_{\texttt{N}}$ S, (n $\oplus$ t)}}
      \end{prooftree}   
    \end{subfigure}
  \end{mdframed}
  \caption{Branching rules which record needed expressions}
  \label{fig:branchrules}
\end{figure}
In addition to \autoref{fig:branchrules}, there must also be an accompanying rule which inspects some current expression \texttt{e} and \texttt{N}, such that only the \texttt{free} variables for these expressions remain (\autoref{fig:gc}).
\begin{figure}[ht]
  \begin{mdframed}
    \begin{prooftree}
            \AxiomC{\texttt{$\Gamma$, e $\rightarrow_{\texttt{N}}$ $\Theta$, p}}
            \AxiomC{\texttt{x $\notin$ R($\Theta$, N, e)}}
              \RightLabel{GC}
              \BinaryInfC{\texttt{$\Gamma$ $\cup$ \{x $\mapsto$ z\}, e $\rightarrow_{\texttt{N}}$ $\Theta$, p}}
    \end{prooftree}   
  \end{mdframed}
  \caption{A rule which filters by used values}
  \label{fig:gc}
\end{figure}
\texttt{R} is the set of reachable variables, which inspects the heap, \texttt{N} and some current expression \texttt{e}.
One could also define the garbage collection rule more compactly if the granularity of \autoref{fig:gc} is too fine.
\begin{figure}[ht]
  \begin{mdframed}
    \begin{prooftree}
            \AxiomC{\texttt{$\Sigma$, e $\rightarrow_{\texttt{N}}$ $\Theta$, y}}
            \AxiomC{\texttt{$\Sigma$ = Prune($\Gamma$, \{e\} $\cup$ N)}}
              \RightLabel{GC}
              \BinaryInfC{\texttt{$\Gamma$ $\cup$ \{x $\mapsto$ z\}, e $\rightarrow_{\texttt{N}}$ $\Theta$, y}}
    \end{prooftree}   
  \end{mdframed}
  \caption{A rule which prunes unused values}
  \label{fig:gcmacro}
\end{figure}
\autoref{fig:gcmacro} defines a garbage collection algorithm which prunes all unreachable values, where \texttt{Prune} is the minimal set of required values to continue program evaluation.
\texttt{Prune} can be defined as in \autoref{eq:prune}
\begin{align}
  &\texttt{Prune($\Gamma$, \{\}) = \{\}} \label{eq:prune}\\
  &\texttt{Prune($\Gamma$, \{e\} $\cup$ N) = \{x $\mapsto$ y | x $\in$ free(e), x $\mapsto$ y $\in$ $\Gamma$\} $\cup$ Prune($\Gamma$, N)} \tag*{}
\end{align}

%Garbage collection (as in any method which frees memory that is no longer in use), in purely functional languages is different from imperative languages like C, since manual memory management is often abstracted away. 
%Adding mappings from variables to lambda terms without any subtractions will lead to an ever increasing substitution mapping size.
%When performing reductions one might desire to garbage collect the substitution mapping once it contains unreachable substitutions.
%Fortunately the task of collecting garbage is quite easy solvable in by a naive algorithm, since the ``scope'' of a substitution is only relevant where the substituted variables has been introduced, which may only happen in the abstraction term.
%Unfortunately not all programs ``return'', like programs which implement infinite recursion.
%\begin{remark}
  %Generally for tail call optimization (\autoref{remark:tailcall}) to be feasible, some sort of ad-hoc freeing of substitutions must be present, such that infinite programs can run in finite space.
%\end{remark}
%\noindent\cite{launchbury1993natural}~presents an elegant (and abstract) rule for garbage collection (\autoref{fig:launchgc}
%Launchbury garbage collection also introduces a new type of state \texttt{N}, which records a set of needed or live expressions.
%When rules such as App and Bin op are performed, branching occurs, that is, there exists another ``pending'' branch of computation, such that the pending branch's expression and the current substitution mapping is introduced to \texttt{N}.
%A version which regards lazy substitutions can be seen in \autoref{fig:lazysub:gc}).
%The function \texttt{R} scans the substitution mapping $\Theta$ for all variables in the expression \texttt{e}.
%\begin{figure}[ht]
    %\begin{mdframed}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\Theta$, e $\rightarrow$ S, y}}
            %\AxiomC{\texttt{x $\notin$ R($\Theta$, N, e)}}
              %\RightLabel{GC}
              %\BinaryInfC{\texttt{$\Theta$ $\cup$ \{x $\mapsto$ z\}, e $\rightarrow$ S, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{Launchbury garbage collection}
    %\label{fig:launchgc}
%\end{figure}
%\begin{figure}[ht]
    %\begin{mdframed}
          %\begin{prooftree}
            %\AxiomC{\texttt{$\Theta$, e $\rightarrow$ S, y}}
            %\AxiomC{\texttt{x $\notin$ R($\Theta$, N, e)}}
              %\RightLabel{GC}
              %\BinaryInfC{\texttt{$\Theta$ $\cup$ \{x $\mapsto$ ($\Delta$, z)\}, e $\rightarrow$ S, y}}
          %\end{prooftree}   
    %\end{mdframed}
    %\caption{Launchbury garbage collection with lazy substitution}
    %\label{fig:lazysub:gc}
%\end{figure}
%%Problems in the domain of computer science (especially algorithms), can usually be solved much more efficiently if the problem has certain properties.
%%Launchbury garbage collection argues for the possibility of parallel garbage collection, which is a property granted for free in immutable languages.
%%In imperative languages garbage collection algorithms either block the execution of the program or have complicated implementation details.
%%Fortunately in functional programming languages, data is (traditionally) immutable, which implies that references \textit{never} change.

%A garbage collection algorithm which collects every ``lost reference'' which adheres to \texttt{R} in \autoref{fig:launchgc}, could be implemented by the use of \texttt{Free} or \texttt{Bound}.
%A garbage collection algorithm for \autoref{fig:lazysub:gc} must also inspect the bound substitution mappings as in \autoref{eq:Rgc}.
%\begin{align}
  %\texttt{GC($\Theta$, N, e)} &= \tag*{}\\
  %&\texttt{T = \{(S, z) | (t $\mapsto$ (S, z)) $\in$ $\Theta$ $\land$ t $\in$ Free(e)\}}\tag*{}\\
  %&\texttt{GC'(N $\cup$ T)}\tag*{}\\
  %\texttt{GC(($\Theta$, e) $\cup$ N)} &= \texttt{Free(e)}
  %%\texttt{GC'($\Theta$, N, e) }&=\texttt{GC($\Theta$, N $\cup$ Free(e))}\label{eq:Rgc}\\\tag*{}\\
  %%\texttt{GC($\Theta$, $\emptyset$)} &= \emptyset& \tag*{}\\
  %%\texttt{GC(\{x $\mapsto$ (S, e)\} $\cup$ $\Theta$, \{x\} $\cup$ N)} &= \texttt{\{x $\mapsto$ (GC(S, Free(e)), e)\} $\cup$ GC($\Theta$, N)}& \tag*{}\\
  %%\texttt{GC(\{x $\mapsto$ (S, e)\} $\cup$ $\Theta$, N)} &= \texttt{GC($\Theta$, N)}& ,\texttt{x $\notin$ N} \tag*{}
%\end{align}
%\begin{remark}
  
%\end{remark}

%Garbage collection algorithms often run in worst case $O(n)$, in regard to the number of variables allocated by the App rule.
%The algorithm proposed in \autoref{eq:Rgc} also has similar guarantees since \texttt{Free} can be computed during compilation.
%An amortized analysis of garbage collection might seem to yield a running time of $O(1)$ by an analysis which is very similar to a stack.
%Unfortunately the garbage collector is not guaranteed to remove any references, thus running it does not guaranteed a reduction in potential.

%The potential function $\Phi$ is the size of the substitution mapping.
%Letting the potential $\Phi(D_i)$ be the equal to the size of the substitution mapping after $i$ operations, that is, insertions and removals.
%The substitution mapping has the initial potential $\Phi(D_0) = 0$.
%An insertion by rule App increases the potential by two since the App rule also introduces a renamed variable.
%\[
  %\Phi(D_i) - \Phi(D_{i - 1}) = (|S| + 2) - |S| = 2
%\]
%Such that the amortized cost becomes.
%\[
  %\textit{insertion}_i = 2 + \Phi(D_i) - \Phi(D_{i - 1}) = 4
%\]
%Substitution mappings can be efficiently be implemented by data structures such as hash maps which have probabilistic constant time insertion~\cite{cormen2009introduction}.
%The garbage collection algorithm can remove $k$ substitutions from the substitution mapping for any $0 \leq k \leq |S|$ by discovering $|S| - k$ variables by some function that adheres to \texttt{R} which runs in $O(1)$.
%\[
  %\Phi(D_i) - \Phi(D_{i - 1}) = -k
%\]
%Such that.
%\[
  %\textit{removal}_i = k + \Phi(D_i) - \Phi(D_{i - 1}) = k - k = 0
%\]
%Clearly the worst case analysis can be synthesised by $k=|S|$ such that the running time becomes $O(n)$.

%So how does one implement \texttt{R} efficiently?

\subsection{Interpreting programs}
Now that the semantics for evaluation of the lambda calculus have been presented, a machine naturally follows.
The machine which is presented is originally derived in~\cite{sestoft1997deriving}.
One can implement a machine which functions very closely to what the semantics describe.
An algorithm which is very alike the natural semantics is presented in \autoref{fig:eval:laznaive} where the \texttt{subst} function is defined as in \autoref{fig:eval:subst}.
\begin{figure}
\begin{mdframed}
\begin{align}
  \texttt{subst(f, t, $\lambda$x.e) }&\texttt{= } 
  \begin{cases}
    \texttt{$\lambda$x.e }& \texttt{if x $\equiv$ f}\\
    \texttt{$\lambda$x.subst(f, t, e) }&
  \end{cases}\tag*{}\\
  \texttt{subst(f, t, x) }&\texttt{= }
  \begin{cases}
    \texttt{t }& \texttt{if x $\equiv$ f}\\
    \texttt{x }&
  \end{cases}\tag*{}\\
  \texttt{subst(f, t, x e) }&\texttt{= subst(f, t, x) subst(f, t, e)}\tag*{}\\
  \texttt{subst(f, t, x $\oplus$ e) }&\texttt{= subst(f, t, x) $\oplus$ subst(f, t, e)}\tag*{}
\end{align}
\end{mdframed}
  \caption{A function \texttt{subst} which states ``substitute \texttt{f} in with \texttt{t} in some expression''}
  \label{fig:eval:subst}
\end{figure}
\begin{figure}
\begin{mdframed}[style=style1]
\begin{align}
  \texttt{eval($\Gamma$, $\lambda$x.e) }&\texttt{= ($\Gamma$, $\lambda$x.e)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$, f z) }&\texttt{= eval($\Theta$, subst(x, z, e))}\tag*{}\\
  \wherecase{($\Theta$, $\lambda$x.e) = eval($\Gamma$, f)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$ $\cup$ \{x $\mapsto$ e\}, x)}&\texttt{= ($\Theta$ $\cup$ \{x $\mapsto$ y\}, y)}\tag*{}\\
  \wherecase{($\Theta$, y) = eval($\Gamma$, e)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$, let x = e in p) }&\texttt{= eval($\Gamma$ $\cup$ \{$\gamma$ $\mapsto$ e\}, l)}\tag*{}\\
  \wherecase{$\gamma$ = fresh,}\tag*{}\\
  \extracase{l = subst(x, $\gamma$, p)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$, x $\oplus$ y) }&\texttt{= (S, n $\oplus$ t)}\tag*{}\\
  \wherecase{($\Sigma$, n) = eval($\Gamma$, x),}\tag*{}\\
  \extracase{(S, t) = eval($\Sigma$, y)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$, n $\in$ $\mathbb{Z^+}$) }&\texttt{= ($\Gamma$, n)}\tag*{}
\end{align}
\end{mdframed}
  \caption{An algorithm for evaluating the lazy lambda calculus}
  \label{fig:eval:laznaive}
\end{figure}
%The algorithm in \autoref{fig:eval:laznaive} is correct (which will not be proven since it will have a successor shortly) and minimal but suffers from some practical issues.
The algorithm in \autoref{fig:eval:laznaive} is minimal but suffers from some practical issues.
\subsubsection{A CPS machine}
The algorithm in \autoref{fig:eval:laznaive} cannot evaluate infinite programs, since it's recursive invocations is not in tail call position.
In the machine introduced in~\cite{sestoft1997deriving} (which will be named the \textit{stack machine}), the algorithm is implemented via a stack which is used to record state in recursive invocations that either branch or require sharing.
\cite{sestoft1997deriving} argues that stack testing in the stack machine; testing the top element of the stack to determine the next computation, is a property best eliminated.
The \textit{app1} and \textit{app2} rules from the stack machine, displayed in \autoref{fig:stackmachine:a1a2} respectively, give insight into some properties that we can use to eliminate stack testing.
\begin{figure}
\begin{mdframed}
\begin{align}
  \texttt{eval($\Gamma$, S, l p) }&\texttt{= eval($\Gamma$, p :S, l)}\tag*{}\\\tag*{}\\
  \texttt{eval($\Gamma$, p :S, $\lambda$x.e) }&\texttt{= eval($\Gamma$, S, subst(x, p, e))}\tag*{}
\end{align}
\end{mdframed}
  \caption{The \textit{app1} and \textit{app2} rules from the stack machine.\\
  \texttt{p :S} means that \texttt{p} is pushed to the stack \texttt{S}, if on the right hand side of \texttt{=}, and popped from \texttt{S}, if on the left hand side of \texttt{=}.}
  \label{fig:stackmachine:a1a2}
\end{figure}
The basis for performing stack testing is the missing information regarding what rule an expression originated from.
To eliminate stack testing in the stack machine we can translate the stack machine into a \textit{continuation-passing style} machine, CPS machine for short.
A stack will be used for the CPS machine, but the stack will have different role.
In the CPS machine the stack holds continuations of the type $\texttt{cont}: \Gamma \times S \times \lambda \rightarrow \lambda$, that is, there is no returning.
There must be catalogue of appropriate continuations for each rule that either branches or requires state and a terminal function \texttt{continue} which either continues by popping a continuation from the stack or returns the expression if the stack is empty \autoref{fig:eval:cps}.
\begin{figure}
\begin{mdframed}[style=style1]
\begin{align}
  \texttt{continue($\Gamma$, [], e) }&\texttt{= e}\tag*{}\\
  \spacecase
  \texttt{continue($\Gamma$, cont :S, e) }&\texttt{= cont($\Gamma$, S, e)}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$, S, $\lambda$x.e) }&\texttt{= continue($\Gamma$, S, $\lambda$x.e)}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$, S, l p) }&\texttt{= eval($\Gamma$, cont :S, l)}\tag*{}\\
  \wherecase{cont($\Sigma$, S', $\lambda$x.e) = }\tag*{}\\
  \extracase{ eval($\Sigma$, S', subst(x, p, e))}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$ $\cup$ \{x $\mapsto$ e\}, S, x) }&\texttt{= eval($\Gamma$, cont :S, e)}\tag*{}\\
  \wherecase{cont($\Sigma$, S', p) = }\tag*{}\\
  \extracase{ continue($\Sigma$ $\cup$ \{x $\mapsto$ p\}, S', p)}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$, S, let x = e in p) }&\texttt{= eval($\Gamma$ $\cup$ \{$\gamma$ $\mapsto$ e\}, S, l)}\tag*{}\\
  \wherecase{$\gamma$ = fresh,}\tag*{}\\
  \extracase{l = subst(x, $\gamma$, p)}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$, S, x $\oplus$ y) }&\texttt{= eval($\Gamma$, cont :S, x)}\tag*{}\\
  \wherecase{cont($\Sigma$, S', n) = }\tag*{}\\
  \extracase{ eval($\Sigma$, cont' :S', y)}\tag*{}\\
  \extracase{ \hspace*{-0.8cm}where cont'($\Theta$, S'', t) = }\tag*{}\\
  \extracase{\hspace*{0.65cm} continue($\Theta$, S'', n + t)}\tag*{}\\
  \spacecase
  \texttt{eval($\Gamma$, S, n $\in$ $\mathbb{Z^+}$) }&\texttt{= continue($\Gamma$, S, n)}\tag*{}
\end{align}
\end{mdframed}
  \caption{A CPS algorithm for a evaluating the lazy lambda calculus}
  \label{fig:eval:cps}
\end{figure}
In it's essence, the algorithm in \autoref{fig:eval:cps} evaluates terms in normal order, recording sharing (Var) and branching (App, Bin op), until it reaches a terminal expression.
When \autoref{fig:eval:cps} reaches a terminal expression, the most recently pushed continuation must naturally be the expression which is the most recent expression that is subject to the rules Var, App or Bin op.

In the true spirit of suspended computations, a suspended computation should have no impact on the performance characteristics, if not evaluated.
The \texttt{subst} function in \autoref{fig:eval:laznaive} is defined to be eager, which naturally does not follow the philosophy of lazy computation. 


\subsection{An invariant on infinite programs}\label{subsec:inf}
An important problem still remains which is that of infinite programs.
Imperative programming languages often solve this by introducing loops, whereas functional programming languages use recursion.
Recursion may be equally powerful in terms of expressiveness, but becomes a bit more tricky when considering interpreter details.
A prerequisite for an infinitely running program to exist in practice is that the program must not grow it's resource needs as it runs.

The distinction between recursive functions and loops in imperative programming languages is often what makes infinite programs expressible.
In a traditional imperative language, a function allocates a \textit{stack frame} and is explicitly parameterized, whereas a loop acts more like an anonymous closure which is always parameterized with itself (a function which is wrapped in a fixed point combinator, like the Y-combinator).
\begin{remark}
    A call stack is a stack of stack frames.
    A stack frame is a pointer to a function pointer.
    Stack frames are used to return execution to the previous function (the calling function).
    Every time a new function is called, the called-from function places a ``resume execution from here'' pointer onto the call stack.
\end{remark}
\noindent Imperative languages are also often evaluated under call by value which further simplifies implementation details.
Imperative loops (more interestingly, infinite loops) can safely release all static resources (variables bindings), which were allocated in the iteration, once an iteration has completed.
In traditional imperative languages recursive functions can only iterate a finite number of times, more specifically until the call stack is full.
\begin{figure}
\begin{lstlisting}[language=ML,caption={Program that implements two functions that fold a \texttt{List a} to a \texttt{b}},label={lst:listfoldboth},mathescape=true]
type List a = 
    | Nil
    | Cons a (List a)
;
fun add a b = a + b;

fun foldl f z l =
    match l
        | Nil -> z;
        | Cons x xs -> 
            foldl f (f x z) xs;
    ;

fun foldr f z l =
    match l
        | Nil -> z;
        | Cons x xs ->
            f x (foldr f z xs);
    ;
\end{lstlisting}
\end{figure}
%fun mapTail r f =
%//The type of mapTail
%//List a $\rightarrow$ (a $\rightarrow$ b) $\rightarrow$ (List b $\rightarrow$ List b) $\rightarrow$ List b
    %fun mapImpl r f b =
        %match r
            %| Nil -> b Nil;
            %| Cons x xs -> 
                %mapTail xs f (b (Cons (f x)));
        %;
    %match r
        %| Nil -> Nil;
        %| Cons x xs -> mapImpl xs f (Cons (f x));
    %;
%fun mapInf f l = 
    %match l
        %| Nil -> Nil;
        %| Cons x xs -> Cons (f x) (mapGrowing f xs);
    %;
%\end{lstlisting}
%\end{figure}

To really understand what happens in a lambda calculus interpreter, we must understand what happens in \autoref{lst:listfoldboth}.
\autoref{lst:listfoldboth} implements two variants of a \texttt{fold} function which accumulates a list of type \texttt{List a} to a \texttt{b}.
The two variants differ when considering evaluation strategy and \textit{tail call optimization}.
\begin{remark}\label{remark:tailcall}
    Tail call optimization is an optimization which can be performed on programs with a particular structure.
    If the last expression is a function invocation, then the rewritten program does not grow.
    For instance the expression \texttt{let f = ($\lambda$g.$\lambda$x.g x) in $\dots$ f g' 0} is eventually rewritten to \texttt{g' 0}.
    If for instance the expression awaited a result like in \texttt{let f = ($\lambda$g.$\lambda$x.x + (g x)) in $\dots$ f g' 0}, then it would be rewritten to \texttt{x + (g' 0)}, increases the size of the program by \texttt{x +}, since the \texttt{+} operator requires both expressions to be evaluated.
    It should become clear that reduction strategies always imply tail call optimization, whenever possible.
\end{remark}
The first flavor of \texttt{fold}; \texttt{foldl}, implements \texttt{fold} such that the program expression tree does not grow throughout program interpretation, under a call by value environment.
The constraint on evaluation strategy is important for \texttt{foldl}, for reasons which will become clear once other evaluation strategies are discussed.
\begin{align}
    &\texttt{ foldl 0 add (Cons 1 (Cons 2 $\dots$ (Cons n Nil)))}  \\
    =&\texttt{ l z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) z ($\lambda$xs,x.foldl f (f x z) xs)}  \tag*{} \\
    =&\texttt{ foldl f (f x z) xs $\{$ xs $\mapsto$ (Cons 2 (Cons 3 $\dots$ (Cons n Nil))), x $\mapsto$ 1, $\dots$ $\}$}  \tag*{} \\
    =&\texttt{ foldl add (add 1 0) (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    =&\texttt{ foldl add 1 (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) $\{ \dots \}$}  \tag*{} \\
    & \dots \tag*{}
\end{align}
Evaluating \texttt{foldl} on a list of size \texttt{n} with the addition function showcases how the program only grows by a constant number of terms.
\begin{remark}
    Note again that the list is always refereed to by reference; the list is not copied.
\end{remark}
%\begin{remark}
    %If the program is to remain within a constant amount of space throughout the evaluation of \texttt{foldl}, then garbage collection must support this choice.
    %Clearly a simple strategy 
%\end{remark}

%\\
%\begin{minipage}{\textwidth}
%\begin{lemma}
    %\texttt{mapTail} is tail recursive and requires an additional $O(n \cdot m)$ space when evaluated on a list of size $n$ with a function \texttt{f} produces values of size $m$.
%\end{lemma}
%\begin{proof}
    %Unpacking \texttt{Cons} and \texttt{Nil} as their underlying scott encoded functions is omitted for readability.
    %In this proof the list is of \texttt{Int} and \texttt{f} is the identity function.
    %For some $1 \leq k \leq n$ where $n$ is the size of the list, at the recursive invocation of \texttt{mapImpl}, the list is partitioned into the lower $1, \dots k - 1$ elements (\texttt{b (Cons x)}) and the higher $k, \dots n$ elements (\texttt{xs}).
    %\begin{align}
        %&\texttt{ mapTail (Cons 1 (Cons 2 $\dots$ (Cons n Nil))) ($\lambda$x.x)}  \\
        %=&\texttt{ r Nil ($\lambda$x.$\lambda$xs.mapImpl xs ($\lambda$x.x) (Cons (f x)))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 2 (Cons 3 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1)}  \eqtag{2$\dots$n} \label{eq:map:2n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 xs) (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (Cons 1 (Cons 2))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 3 (Cons 4 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2))}  \eqtag{3$\dots$n} \label{eq:map:3n} \\
        %=&\texttt{ r (b Nil) ($\lambda$x.$\lambda$xs.mapImpl xs f (b (Cons (f x))))}  \tag*{} \\
        %=&\texttt{ mapImpl xs f (($\lambda$xs.Cons 1 (Cons 2 xs)) (Cons 3))}  \tag*{} \\
        %=&\texttt{ mapImpl (Cons 4 (Cons 5 $\dots$ (Cons n Nil))) ($\lambda$x.x) (Cons 1 (Cons 2 (Cons 3)))}  \eqtag{4$\dots$n} \\
        %=&\dots\tag*{}\\
        %=&\texttt{ mapImpl (Cons k (Cons (k + 1) $\dots$ (Cons n Nil)))} \tag*{} \\
        %&\texttt{ ($\lambda$x.x) (Cons 1 ($\dots$ (Cons (k - 1))))}  \tag*{} 
    %\end{align}
    %The base case for the proof is once \texttt{mapImpl} is recursively invoked (\autoref{eq:map:2n}), size of the newly created list is $m \cdot 1$ (\texttt{f (Cons 1)}).
    %The first recursive iteration for \texttt{mapImpl} is the step between \autoref{eq:map:2n} and \autoref{eq:map:3n} such that the size becomes $m \cdot 2$ (\texttt{Cons (f 1) (Cons (f 2))}).
    %If we assume that the first $k$ recursive iterations have size $m \cdot k$, then the $k+1$'th iteration has size $m \cdot (k + 1)$ by \texttt{Cons (f 1) (Cons (f 2) $\dots$ (Cons (f (k + 1))))}.
    %Clearly whet $k = n$ then the newly created list is of size $m \cdot n$ and \texttt{xs} is empty, thus the function is.
%\end{proof}
%\end{minipage}

%When evaluating a term that results in a change to the substitution mapping the previous substitution mapping version is paired with it.
%Letting the substitution mapping be 
%$\{ \texttt{x} \mapsto (\lambda \texttt{x.g x}, \{ \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)) \}, \texttt{g} \mapsto (\lambda \texttt{x.x}, \emptyset)\}$
%the space complexity may seem to grow by $O(n^3)$ (\autoref{eq:evalruntimewrong}) but lifting the data structure to using pointers instead of copies reduces the size to $O(n)$.
%\begin{align}
    %\label{eq:evalruntimewrong}
    %& O \left( \sum_{k = 1}^n \sum_{i = 1}^{k} i \right) \\
    %= \,\,\, & O \left( \sum_{k = 1}^n \frac{k(k + 1)}{2}\right)\tag{Triangular}\\
    %= \,\,\,& O \left( \frac{1}{2} \left( \sum_{k = 1}^n k^2 + \sum_{k = 1} k \right) \right)\tag*{}\\
    %= \,\,\,& O \left( \sum_{k = 1}^n k^2 \right)\tag*{}\\
    %= \,\,\,& O \left( \frac{1}{6}n + \frac{1}{2}n^2 + \frac{1}{3}n^3 \right)\tag{Faulhaber}\\
    %= \,\,\,& O \left( n^3 \right)\tag*{}
%\end{align}
%Observe that the introduction of a context is based on free variables in closures and can in fact be avoided at the cost of compilation time by simply making all variables unique.
%Let \autoref{eq:eval:comp} be a program translated from \autoref{eq:eval:nocomp}.
%\begin{align}
    %&(\lambda x . \texttt{ let } f = \lambda y . \lambda x . x \texttt{ in } x) \label{eq:eval:nocomp}\\
    %&(\lambda x_1 . \texttt{ let } x_2 = \lambda x_3 . \lambda x_4 . x_4 \texttt{ in } x_1)\label{eq:eval:comp}
%\end{align}
%The method of storing substitutions warrants the question of evaluation strategy since eagerly replacing the mapped to expressions implies call by value (\autoref{eq:eval:eagereval}).
%If one were to keep the initially proposed semantics the result would be call by name (\autoref{eq:eval:name}).
%In the hybrid approach forcing a term rewrites the mapped to value (\autoref{eq:eval:need}).
%\begin{align}
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto 5, \texttt{y} \mapsto 5 \} \label{eq:eval:eagereval} \eqtag{Call by value}\\
    %&\{ \texttt{x} \mapsto \texttt{y} \} \cdot \{ \texttt{y} \mapsto 5 \} \rightarrow \{ \texttt{x} \mapsto \texttt{y}, \texttt{y} \mapsto 5 \} \label{eq:eval:name} \eqtag{Call by name}\\
    %&\textit{force}(\texttt{z}, \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto \texttt{f 0} \}) = \{ \texttt{f} \mapsto \lambda \texttt{x.x}, \texttt{z} \mapsto 0 \} \label{eq:eval:need} \eqtag{Call by need}
%\end{align}
%One may also have an interpreter that supports multiple strategies at once by simply specifying what type of strategy should be applied to every term.
%Evaluating a program is a matter of entering through some expression and continually reading terms and applying substitutions until said expression has been evaluated regardless of evaluation strategy. 

%The algorithm for evaluating the untyped lambda calculus is similar to Algorithm W (\autoref{fig:dmrules}) and can be seen in.
%\begin{figure}[ht]
    %\begin{mdframed}[style=style1]
        %\vspace*{0.4cm}
        %\begin{subfigure}[b]{0.30\textwidth}
            %\begin{prooftree}
                %\AxiomC{$e$}
                %\RightLabel{Terminal}
                %\UnaryInfC{$\emptyset ,e$}
            %\end{prooftree}   
            %\label{eq:prooftree:terminal}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{0.69\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S,\lambda x . e_1$}
                %\AxiomC{$S \cdot \{ x \mapsto e_2 \},e_1 $}
                %\RightLabel{App}
                %\BinaryInfC{$S,e_3$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}
        %\begin{subfigure}[b]{1\textwidth}
            %\begin{prooftree}
                %\AxiomC{$S \cdot \{ x \mapsto e_1 \},e_2$}
                %\RightLabel{Let}
                %\UnaryInfC{$S,\texttt{ let } x = e_1 \texttt{ in } e_2$}
            %\end{prooftree}   
            %\label{eq:prooftree:ref}
            %\caption{}
        %\end{subfigure}

    %\end{mdframed}
    
%\end{figure}

%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%Beta reductions with De Bruijn indices~\cite{de1972lambda} is another more straightforward method of evaluating the untyped lambda calculus.
%De Bruijn indices is a representation of lambda calculus which deals with variables based on the scope ''distance`` instead of variable names.
%\begin{lstlisting}[language=ML,caption={Add as De Bruijn},label={lst:adddebru},mathescape=true]
%let add = ($\lambda$($\lambda$ 2 1))
%\end{lstlisting}
%Consider the De Bruijn form of \autoref{lst:add} where \texttt{2} is the index of \texttt{a} and \texttt{1} is the index of \texttt{b}.
%More generally all variable occurrences are replaced with the distance to the abstraction which introduced them.
%The use of De Bruijn indices allow anonymous naming of parameters thus imply a method of solving the application problem when directly interpreting the untyped lambda calculus.
%A minor but important detail when 
%\begin{align}
    %&(\lambda x . ((\lambda f. \lambda x. f x)(\lambda z . z))x)''0`` \rightarrow\\
    %&
%\end{align}

